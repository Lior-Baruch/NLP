{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce5pQK3bFn_"
   },
   "source": [
    "# Assignment 1\n",
    "In this assignment you will be creating tools for learning and testing language models.\n",
    "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
    "\n",
    "Do make sure all results are uploaded to CSVs (as well as printed to console) for your assignment to be fully graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwG8v-Ll49KM"
   },
   "source": [
    "*As a preparation for this task, download the data files from the course git repository.\n",
    "\n",
    "The relevant files are under **lm-languages-data-new**:\n",
    "\n",
    "\n",
    "*   en.csv (or the equivalent JSON file)\n",
    "*   es.csv (or the equivalent JSON file)\n",
    "*   fr.csv (or the equivalent JSON file)\n",
    "*   in.csv (or the equivalent JSON file)\n",
    "*   it.csv (or the equivalent JSON file)\n",
    "*   nl.csv (or the equivalent JSON file)\n",
    "*   pt.csv (or the equivalent JSON file)\n",
    "*   tl.csv (or the equivalent JSON file)\n",
    "*   test.csv (or the equivalent JSON file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:40:58.983796Z",
     "end_time": "2023-04-20T10:41:00.500957Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xC-87z2GWMq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b7f40be-bf9b-4d6c-da81-25d60d710a75",
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:00.494430Z",
     "end_time": "2023-04-20T10:41:00.516626Z"
    }
   },
   "source": [
    "# download the data files from the course git repository, delete comment to run\n",
    "#!git clone https://github.com/kfirbar/nlp-course.git"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOVb4IhsqimJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYdhPfbAGkip",
    "outputId": "af6566c6-e6e6-409a-c569-8fab9bdf400e",
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:00.510095Z",
     "end_time": "2023-04-20T10:41:00.532650Z"
    }
   },
   "source": [
    "# list the files in the data directory, delete comment to run\n",
    "#!ls nlp-course/lm-languages-data-new\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "path = \"nlp-course/lm-languages-data-new\"\n",
    "data_files = [\"en.csv\", \"es.csv\", \"fr.csv\", \"in.csv\", \"it.csv\", \"nl.csv\", \"pt.csv\", \"tl.csv\"]\n",
    "test_file = \"test.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:00.529626Z",
     "end_time": "2023-04-20T10:41:00.565454Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ashyu_mT28o6"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def conveted_"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data.\n",
    "# the data in the files are in the form: tweet_id,tweet_text\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" Creates a vocabulary from the data files\n",
    "    Returns:\n",
    "        vocabulary: a list of all the characters that appear in the data files\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    # iterate over all the data files\n",
    "    for file in data_files:\n",
    "        # read the data file\n",
    "        current_data = pd.read_csv(path + \"/\" + file, encoding=\"utf-8\")\n",
    "        # iterate over all the tweets in the data file\n",
    "        for tweet in current_data[\"tweet_text\"]:\n",
    "            tweet = unicodedata.normalize('NFC', tweet)\n",
    "            # iterate over all the characters in the tweet\n",
    "            for char in tweet:\n",
    "                # add the character to the vocabulary\n",
    "                vocabulary.add(char)\n",
    "    # sort the vocabulary\n",
    "    vocabulary = sorted(list(vocabulary))\n",
    "    return vocabulary"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:00.542835Z",
     "end_time": "2023-04-20T10:41:00.565454Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['<start>', '\\n', '\\r', ' ', '!', '\"', '#', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '1', '2', '3', '4', '5', '6', '7', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x9d', 'Â¡', 'Â£', 'Â¤', 'Â¥', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', '\\xad', 'Â®', 'Â¯', 'Â°', 'Â²', 'Â´', 'Â¶', 'Â·', 'Â¸', 'Âº', 'Â»', 'Â½', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã…', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'Ã', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã™', 'Ãš', 'Ãœ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ä—', 'Ä', 'ÄŸ', 'Ä°', 'Ä±', 'Å„', 'Å', 'Å’', 'Å“', 'Å', 'ÅŸ', 'Å ', 'Å¸', 'Æ’', 'Ê”', 'Ê•', 'Ê–', 'Ê°', 'Ê³', 'Ê·', 'Ê¸', 'Ë–', 'Ë˜', 'Ëš', 'Ë›', 'Ë¡', 'Ë¢', 'Ì€', 'Ì', 'Ìƒ', 'Ìˆ', 'Ì¥', 'Ì®', 'Ì¯', 'Íœ', 'Í¡', 'Î”', 'Î˜', 'Î©', 'Ï…', 'Ï‰', 'Ğ', 'Ğ˜', 'Ğœ', 'Ğ', 'Ğ', 'ĞŸ', 'Ğ ', 'Ğ¤', 'Ğ¦', 'Ğ¯', 'Ğ°', 'Ğ±', 'Ğ²', 'Ğ³', 'Ğ´', 'Ğµ', 'Ğ·', 'Ğ¸', 'Ğº', 'Ğ»', 'Ğ¼', 'Ğ½', 'Ğ¾', 'Ğ¿', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ…', 'Ñ‹', 'Ñ', 'Ñ', 'Ñ', 'Ò’', 'Ò¯', 'ØŒ', 'Ø¢', 'Ø¦', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Ø¶', 'Ø·', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'ÛŒ', 'Û¶', 'à¤‚', 'à¤•', 'à¤—', 'à¤ª', 'à¤¬', 'à¤°', 'à¤¸', 'à¤¾', 'à¥‡', 'à¥', 'à·´', 'à¸', 'à¸‚', 'à¸‡', 'à¸ˆ', 'à¸', 'à¸”', 'à¸•', 'à¸–', 'à¸—', 'à¸™', 'à¸š', 'à¸›', 'à¸', 'à¸ ', 'à¸¡', 'à¸¢', 'à¸£', 'à¸¥', 'à¸§', 'à¸¨', 'à¸ª', 'à¸­', 'à¸°', 'à¸±', 'à¸²', 'à¸´', 'à¸µ', 'à¸¸', 'à¸¹', 'à¹€', 'à¹', 'à¹ˆ', 'à¹‰', 'à¹', 'à¹‘', 'àº¶', 'à¼', 'à¼º', 'à¼»', 'à¼¼', 'à¼½', 'á™“', 'á´—', 'á´¬', 'á´°', 'áµƒ', 'áµ‡', 'áµˆ', 'áµ‰', 'áµ', 'áµ', 'áµ’', 'áµ–', 'áµ—', 'áµ˜', 'áµ›', 'á¶œ', 'á¶ ', 'á¶¦', 'á¶°', '\\u2009', '\\u200a', '\\u200b', '\\u200d', 'â€“', 'â€”', 'â€•', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€', 'â€ ', 'â€¢', 'â€¦', 'â€°', 'â€²', 'â€¹', 'â€º', 'â€»', 'â€¼', 'â€¿', 'â‰', '\\u2066', '\\u2067', '\\u2069', 'â±', 'â·', 'â¿', 'â‚¬', 'â‚¹', 'âƒ£', 'â„ƒ', 'â„…', 'â„¢', 'â†', 'â†‘', 'â†’', 'â†“', 'â†”', 'â†•', 'â†—', 'â†˜', 'â†š', 'â†›', 'â†©', 'â†ª', 'â†¯', 'â†º', 'â‡˜', 'â‡¨', 'âˆ€', 'âˆ†', 'âˆ‡', 'âˆš', 'âˆ', 'âˆ´', 'âˆµ', 'â‰¤', 'â‰¥', 'â‰¦', 'â‰§', 'âŠ™', 'â‹…', 'â‹ª', 'â‹­', 'âŒš', 'âŒ›', 'âŒ£', 'â‹', 'â©', 'â°', 'â±', 'â³', 'â¸', 'â‘ ', 'â‘¥', 'â’', 'â’', 'â’', 'â’', 'â’‘', 'â“‚', 'â“˜', 'â“™', 'â“¢', 'â“¦', 'â”€', 'â”', 'â”ƒ', 'â”„', 'â”†', 'â”', 'â”“', 'â”—', 'â”›', 'â”³', 'â”»', 'â•‘', 'â•”', 'â•—', 'â•š', 'â•', 'â•¦', 'â•©', 'â•¬', 'â•­', 'â•®', 'â•¯', 'â•°', 'â•±', 'â•²', 'â•´', 'â–ˆ', 'â–Š', 'â–', 'â–’', 'â–”', 'â–•', 'â–™', 'â–', 'â–£', 'â–¦', 'â–ª', 'â–²', 'â–³', 'â–¶', 'â–¸', 'â–º', 'â–¼', 'â–½', 'â–¿', 'â—€', 'â—„', 'â—†', 'â—‡', 'â—ˆ', 'â—', 'â—', 'â—‘', 'â—•', 'â—¡', 'â—»', 'â—¼', 'â—½', 'â—¾', 'â˜€', 'â˜', 'â˜ƒ', 'â˜„', 'â˜…', 'â˜†', 'â˜‰', 'â˜', 'â˜‘', 'â˜“', 'â˜”', 'â˜•', 'â˜˜', 'â˜™', 'â˜š', 'â˜›', 'â˜œ', 'â˜', 'â˜', 'â˜ ', 'â˜£', 'â˜ª', 'â˜®', 'â˜¯', 'â˜°', 'â˜¹', 'â˜º', 'â˜¼', 'â˜½', 'â˜¾', 'â™€', 'â™‚', 'â™‹', 'â™', 'â™', 'â™', 'â™', 'â™“', 'â™›', 'â™¡', 'â™£', 'â™¤', 'â™¥', 'â™¦', 'â™©', 'â™ª', 'â™«', 'â™¬', 'â™¯', 'â™»', 'âš’', 'âš“', 'âš”', 'âš•', 'âš–', 'âš˜', 'âšœ', 'âš', 'âš ', 'âš¡', 'âšª', 'âš«', 'âš°', 'âš½', 'âš¾', 'â›„', 'â›…', 'â›ˆ', 'â›“', 'â›”', 'â›©', 'â›ª', 'â›³', 'â›·', 'â›½', 'âœ', 'âœ‚', 'âœƒ', 'âœ…', 'âœˆ', 'âœ‰', 'âœŠ', 'âœ‹', 'âœŒ', 'âœ', 'âœ', 'âœ“', 'âœ”', 'âœ–', 'âœ', 'âœ¡', 'âœ§', 'âœ¨', 'âœ©', 'âœ­', 'âœ°', 'âœ³', 'âœ´', 'âœµ', 'âœ¶', 'âœ·', 'âœ¿', 'â€', 'â', 'â„', 'â…', 'âˆ', 'â‹', 'âŒ', 'â', 'â“', 'â”', 'â—', 'â', 'â', 'â£', 'â¤', 'â¥', 'âŠ', 'â‹', 'âŒ', 'â', 'â', 'â', 'â”', 'â–', 'â—', 'â™', 'â›', 'âœ', 'â', 'âŸ', 'â ', 'â¡', 'â¢', 'â¤', 'â°', 'â €', 'â¤µ', 'â¦‘', 'â¦’', 'â¬…', 'â¬‡', 'â­', 'â¸„', 'â¸…', '\\u3000', 'ã€', 'ã€‚', 'ã€†', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€', 'ã€', 'ã€', 'ã€', 'ã€‘', 'ã€œ', 'ã€¡', 'ã€°', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ãŒ', 'ã', 'ã', 'ã—', 'ã›', 'ãœ', 'ãŸ', 'ã£', 'ã¥', 'ã¦', 'ã§', 'ã¨', 'ãª', 'ã«', 'ã­', 'ã®', 'ã¯', 'ã²', 'ã¿', 'ã‚€', 'ã‚‡', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚Œ', 'ã‚', 'ã‚’', 'ã‚œ', 'ã‚¤', 'ã‚§', 'ã‚¨', 'ã‚©', 'ã‚ª', 'ã‚«', 'ã‚­', 'ã‚¯', 'ã‚°', 'ã‚³', 'ã‚´', 'ã‚µ', 'ã‚¸', 'ã‚¹', 'ã‚»', 'ã‚¿', 'ãƒ', 'ãƒƒ', 'ãƒ„', 'ãƒ†', 'ãƒˆ', 'ãƒ‰', 'ãƒŠ', 'ãƒ‹', 'ãƒ', 'ãƒ', 'ãƒ‘', 'ãƒ’', 'ãƒ“', 'ãƒ”', 'ãƒ•', 'ãƒ–', 'ãƒ—', 'ãƒ', 'ãƒ ', 'ãƒ¡', 'ãƒ¥', 'ãƒ¦', 'ãƒ§', 'ãƒ©', 'ãƒ¬', 'ãƒ­', 'ãƒ®', 'ãƒ¯', 'ãƒ³', 'ãƒ»', 'ãƒ¼', 'ãƒ½', 'ã……', 'ã…ˆ', 'ã…‹', 'ã…', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£', 'ã…¤', '\\u31ef', 'ä¸–', 'ä¸­', 'ä¸»', 'äº’', 'äºº', 'ä»˜', 'åƒ', 'å„¿', 'å…', 'å…ˆ', 'å…¥', 'å†™', 'åˆ†', 'åˆ©', 'åˆ¶', 'åˆ¹', 'åŠ›', 'åŠª', 'å‹•', 'åˆ', 'å’', 'å—', 'åˆ', 'å‘Ÿ', 'å˜‰', 'å¢—', 'å¥½', 'å§¿', 'å«Œ', 'å­¦', 'å°”', 'å¸Œ', 'å½¡', 'å½±', 'å½¼', 'å¾Œ', 'æ‚ª', 'æ‰‹', 'æŠ•', 'æ‹¶', 'æŒ¨', 'æ’ƒ', 'æ’®', 'æ–‡', 'æ˜ ', 'æ™‚', 'æœˆ', 'æœ', 'æœ¬', 'æ—', 'æŸ±', 'æ¥­', 'æ©Ÿ', 'æ­Œ', 'æ­³', 'æ¯…', 'æ°—', 'æ´²', 'æ´¸', 'ç‹', 'ç”Ÿ', 'ç”¨', 'ç”»', 'ç•Œ', 'ç›¸', 'çœŸ', 'ç¬', 'çŸ¥', 'ç¨¿', 'ç©º', 'ç³Ÿ', 'çµ‚', 'çµ', 'ç¹‹', 'è€…', 'èŠ±', 'èœ', 'è¡Œ', 'è®¸', 'èµ«', 'è¸Š', 'è¾¼', 'é€š', 'é‚£', 'é–“', 'é¢¨', 'é­', 'ê ', 'ê°€', 'ê°“', 'ê°•', 'ê±¸', 'ê²€', 'ê²Œ', 'ê²©', 'ê²°', 'ê²½', 'ê³ ', 'ê³¡', 'ê³¼', 'êµ¬', 'êµ­', 'ê·œ', 'ê·¸', 'ê·¼', 'ê¸ˆ', 'ê¸°', 'ê¹€', 'ê¼¼', 'ë‚˜', 'ë‚¨', 'ë‚´', 'ë„ˆ', 'ë„', 'ë„¤', 'ë„·', 'ë…€', 'ë…„', 'ë…¸', 'ë…¼', 'ëˆ„', 'ëŠ”', 'ëŠ˜', 'ë‹ˆ', 'ë‹¤', 'ë‹¨', 'ë‹¹', 'ëŒ€', 'ë”', 'ë„', 'ë™', 'ë‘', 'ë‘‘', 'ë“€', 'ë“œ', 'ë“±', 'ë””', 'ë¼', 'ë½', 'ë‘', 'ë™', 'ëœ', 'ë¨', 'ëŸ¬', 'ëŸ°', 'ë ˆ', 'ë ›', 'ë¡œ', 'ë¡±', 'ë£Œ', 'ë£°', 'ë£¸', 'ë¦„', 'ë¦‰', 'ë¦¬', 'ë¦¼', 'ë§', 'ë§ˆ', 'ë§', 'ë§¤', 'ë§¨', 'ëª¬', 'ë¬´', 'ë¯¸', 'ë¯¼', 'ë°€', 'ë°”', 'ë°•', 'ë°©', 'ë°±', 'ë±€', 'ë²„', 'ë²…', 'ë²•', 'ë² ', 'ë²¨', 'ë²³', 'ë³´', 'ë³µ', 'ë³¸', 'ë´‰', 'ë·”', 'ë¸Œ', 'ë¸', 'ë¸”', 'ë¹„', 'ë¹…', 'ë¹¼', 'ì‚¬', 'ì‚´', 'ì‚¼', 'ìƒ', 'ìƒ', 'ìƒ¤', 'ìƒµ', 'ì„œ', 'ì„', 'ì„ ', 'ì„±', 'ì„¸', 'ì„¹', 'ì…”', 'ì…˜', 'ì…©', 'ì†Œ', 'ì†¡', 'ìˆ˜', 'ìŠˆ', 'ìŠ¤', 'ìŠ¨', 'ìŠ¬', 'ìŠ¹', 'ì‹œ', 'ì‹', 'ì‹ ', 'ì‹¤', 'ì‹¸', 'ì•„', 'ì•ˆ', 'ì••', 'ì• ', 'ì•¼', 'ì–‘', 'ì–´', 'ì—', 'ì—‘', 'ì—˜', 'ì— ', 'ì—£', 'ì—¬', 'ì—­', 'ì—°', 'ì˜', 'ì˜ˆ', 'ì˜¤', 'ì˜¨', 'ì™€', 'ì™•', 'ì™¸', 'ìš”', 'ìš©', 'ìš°', 'ìš¸', 'ì›Œ', 'ì›', 'ìœ„', 'ìœ ', 'ìœ¤', 'ì˜', 'ì´', 'ì¸', 'ì¼', 'ì„', 'ì˜', 'ì¥', 'ì¬', 'ì­', 'ì „', 'ì •', 'ì œ', 'ì ¤', 'ì¢…', 'ì£¼', 'ì¥”', 'ì¦ˆ', 'ì§€', 'ì§', 'ì§„', 'ì§‘', 'ì©œ', 'ì°Œ', 'ì°°', 'ì±„', 'ì²œ', 'ì² ', 'ì´ˆ', 'ìµœ', 'ì¶”', 'ì¶œ', 'ì¸ ', 'ì¹˜', 'ì¹´', 'ì»¤', 'ì½”', 'ì½˜', 'ì½¤', 'ì¿±', 'í¬', 'í‚¤', 'í‚¹', 'íƒ€', 'íƒ„', 'íƒ‘', 'íƒœ', 'í„°', 'í…', 'í† ', 'í†¡', 'íŠ¸', 'í‹´', 'íŒ', 'íŒŒ', 'íŒ¨', 'í€', 'í¬', 'í’€', 'í”„', 'í”Œ', 'í”¼', 'í•‘', 'í•˜', 'í•œ', 'í•´', 'í•¸', 'í—Œ', 'í—¤', 'í—¨', 'í˜', 'í˜„', 'í˜•', 'í˜¸', 'í™”', 'í™˜', 'í›ˆ', 'í', 'ï·»', 'ï¸', 'ï¸', 'ï¸µ', 'ï¹', 'ï¹ª', 'ï¼', 'ï¼‚', 'ï¼ƒ', 'ï¼ˆ', 'ï¼‰', 'ï¼Š', 'ï¼', 'ï¼“', 'ï¼–', 'ï¼—', 'ï¼˜', 'ï¼Ÿ', 'ï¼ ', 'ï¼¡', 'ï¼¢', 'ï¼£', 'ï¼¤', 'ï¼¥', 'ï¼¦', 'ï¼§', 'ï¼¨', 'ï¼©', 'ï¼«', 'ï¼¬', 'ï¼­', 'ï¼®', 'ï¼¯', 'ï¼°', 'ï¼±', 'ï¼²', 'ï¼³', 'ï¼´', 'ï¼µ', 'ï¼¶', 'ï¼·', 'ï¼¹', 'ï¼»', 'ï¼½', 'ï¼¿', 'ï½€', 'ï½', 'ï½‚', 'ï½ƒ', 'ï½„', 'ï½…', 'ï½‡', 'ï½‰', 'ï½Œ', 'ï½', 'ï½', 'ï½', 'ï½', 'ï½‘', 'ï½’', 'ï½“', 'ï½•', 'ï½–', 'ï½—', 'ï½˜', 'ï½™', 'ï½š', 'ï½œ', 'ï½¡', 'ï½¥', 'ï¾‰', 'ï¿£', 'ï¿¼', 'ï¿½', 'ğŸƒ', 'ğŸ…°', 'ğŸ…±', 'ğŸ…¾', 'ğŸ…¿', 'ğŸ†‘', 'ğŸ†’', 'ğŸ†“', 'ğŸ†”', 'ğŸ†•', 'ğŸ†–', 'ğŸ†—', 'ğŸ†˜', 'ğŸ†™', 'ğŸ†š', 'ğŸ‡¦', 'ğŸ‡§', 'ğŸ‡¨', 'ğŸ‡©', 'ğŸ‡ª', 'ğŸ‡«', 'ğŸ‡¬', 'ğŸ‡­', 'ğŸ‡®', 'ğŸ‡¯', 'ğŸ‡°', 'ğŸ‡±', 'ğŸ‡²', 'ğŸ‡³', 'ğŸ‡´', 'ğŸ‡µ', 'ğŸ‡·', 'ğŸ‡¸', 'ğŸ‡¹', 'ğŸ‡º', 'ğŸ‡»', 'ğŸ‡¼', 'ğŸ‡½', 'ğŸ‡¾', 'ğŸ‡¿', 'ğŸˆµ', 'ğŸˆ¶', 'ğŸˆ·', 'ğŸŒ€', 'ğŸŒƒ', 'ğŸŒ„', 'ğŸŒ…', 'ğŸŒ†', 'ğŸŒ‡', 'ğŸŒˆ', 'ğŸŒŠ', 'ğŸŒ‹', 'ğŸŒŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ’', 'ğŸŒ“', 'ğŸŒ—', 'ğŸŒ™', 'ğŸŒš', 'ğŸŒ›', 'ğŸŒœ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒŸ', 'ğŸŒ ', 'ğŸŒ¤', 'ğŸŒ¥', 'ğŸŒ§', 'ğŸŒ¨', 'ğŸŒª', 'ğŸŒ«', 'ğŸŒ¬', 'ğŸŒ®', 'ğŸŒ¯', 'ğŸŒ°', 'ğŸŒ±', 'ğŸŒ²', 'ğŸŒ³', 'ğŸŒ´', 'ğŸŒµ', 'ğŸŒ¶', 'ğŸŒ·', 'ğŸŒ¸', 'ğŸŒ¹', 'ğŸŒº', 'ğŸŒ»', 'ğŸŒ¼', 'ğŸŒ½', 'ğŸŒ¾', 'ğŸŒ¿', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸƒ', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹', 'ğŸŒ', 'ğŸ', 'ğŸ', 'ğŸ', 'ğŸ‘', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ•', 'ğŸ–', 'ğŸ—', 'ğŸš', 'ğŸ›', 'ğŸœ', 'ğŸ', 'ğŸ', 'ğŸŸ', 'ğŸ£', 'ğŸ¤', 'ğŸ¥', 'ğŸ¦', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ³', 'ğŸ´', 'ğŸµ', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹', 'ğŸ', 'ğŸ’', 'ğŸ“', 'ğŸ—', 'ğŸ™', 'ğŸ', 'ğŸŸ', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ¤', 'ğŸ¥', 'ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ®', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ²', 'ğŸµ', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ¾', 'ğŸ¿', 'ğŸ€', 'ğŸ', 'ğŸƒ', 'ğŸ„', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹', 'ğŸŒ', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ–', 'ğŸ˜', 'ğŸ™', 'ğŸš', 'ğŸŸ', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ«', 'ğŸ°', 'ğŸ³', 'ğŸ´', 'ğŸ¹', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸ„', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ', 'ğŸ', 'ğŸ', 'ğŸ‘', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ•', 'ğŸ–', 'ğŸ˜', 'ğŸ™', 'ğŸš', 'ğŸœ', 'ğŸ', 'ğŸ', 'ğŸŸ', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ£', 'ğŸ¥', 'ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ®', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ²', 'ğŸ³', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ‘€', 'ğŸ‘', 'ğŸ‘‚', 'ğŸ‘„', 'ğŸ‘…', 'ğŸ‘†', 'ğŸ‘‡', 'ğŸ‘ˆ', 'ğŸ‘‰', 'ğŸ‘Š', 'ğŸ‘‹', 'ğŸ‘Œ', 'ğŸ‘', 'ğŸ‘', 'ğŸ‘', 'ğŸ‘', 'ğŸ‘‘', 'ğŸ‘“', 'ğŸ‘•', 'ğŸ‘–', 'ğŸ‘—', 'ğŸ‘™', 'ğŸ‘', 'ğŸ‘Ÿ', 'ğŸ‘ ', 'ğŸ‘¡', 'ğŸ‘£', 'ğŸ‘¤', 'ğŸ‘¥', 'ğŸ‘¦', 'ğŸ‘§', 'ğŸ‘¨', 'ğŸ‘©', 'ğŸ‘ª', 'ğŸ‘«', 'ğŸ‘­', 'ğŸ‘®', 'ğŸ‘¯', 'ğŸ‘°', 'ğŸ‘±', 'ğŸ‘²', 'ğŸ‘³', 'ğŸ‘µ', 'ğŸ‘¶', 'ğŸ‘·', 'ğŸ‘¸', 'ğŸ‘¹', 'ğŸ‘º', 'ğŸ‘»', 'ğŸ‘¼', 'ğŸ‘½', 'ğŸ‘¿', 'ğŸ’€', 'ğŸ’', 'ğŸ’‚', 'ğŸ’ƒ', 'ğŸ’„', 'ğŸ’…', 'ğŸ’†', 'ğŸ’ˆ', 'ğŸ’‰', 'ğŸ’Š', 'ğŸ’‹', 'ğŸ’Œ', 'ğŸ’', 'ğŸ’', 'ğŸ’', 'ğŸ’', 'ğŸ’‘', 'ğŸ’“', 'ğŸ’”', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’˜', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ’œ', 'ğŸ’', 'ğŸ’', 'ğŸ’Ÿ', 'ğŸ’¡', 'ğŸ’¢', 'ğŸ’£', 'ğŸ’¤', 'ğŸ’¥', 'ğŸ’¦', 'ğŸ’§', 'ğŸ’¨', 'ğŸ’©', 'ğŸ’ª', 'ğŸ’«', 'ğŸ’¬', 'ğŸ’­', 'ğŸ’®', 'ğŸ’¯', 'ğŸ’°', 'ğŸ’³', 'ğŸ’µ', 'ğŸ’¶', 'ğŸ’¸', 'ğŸ’»', 'ğŸ’¼', 'ğŸ’½', 'ğŸ’¿', 'ğŸ“€', 'ğŸ“‚', 'ğŸ“…', 'ğŸ“†', 'ğŸ“ˆ', 'ğŸ“Š', 'ğŸ“‹', 'ğŸ“Œ', 'ğŸ“', 'ğŸ“', 'ğŸ““', 'ğŸ“–', 'ğŸ“š', 'ğŸ“›', 'ğŸ“', 'ğŸ“', 'ğŸ“¡', 'ğŸ“¢', 'ğŸ“£', 'ğŸ“¦', 'ğŸ“§', 'ğŸ“©', 'ğŸ“¬', 'ğŸ“¯', 'ğŸ“°', 'ğŸ“±', 'ğŸ“²', 'ğŸ“´', 'ğŸ“·', 'ğŸ“¸', 'ğŸ“¹', 'ğŸ“º', 'ğŸ“»', 'ğŸ“¼', 'ğŸ“½', 'ğŸ“¿', 'ğŸ”', 'ğŸ”‚', 'ğŸ”ƒ', 'ğŸ”„', 'ğŸ”…', 'ğŸ”‰', 'ğŸ”Š', 'ğŸ”‹', 'ğŸ”Œ', 'ğŸ”', 'ğŸ”‘', 'ğŸ”’', 'ğŸ”“', 'ğŸ””', 'ğŸ”˜', 'ğŸ”™', 'ğŸ”›', 'ğŸ”œ', 'ğŸ”', 'ğŸ”', 'ğŸ”¥', 'ğŸ”¨', 'ğŸ”©', 'ğŸ”ª', 'ğŸ”«', 'ğŸ”®', 'ğŸ”°', 'ğŸ”±', 'ğŸ”²', 'ğŸ”´', 'ğŸ”µ', 'ğŸ”¶', 'ğŸ”¸', 'ğŸ”¹', 'ğŸ”º', 'ğŸ”»', 'ğŸ”¼', 'ğŸ”½', 'ğŸ•Š', 'ğŸ•Œ', 'ğŸ•', 'ğŸ•’', 'ğŸ•˜', 'ğŸ•›', 'ğŸ•œ', 'ğŸ•Ÿ', 'ğŸ•¤', 'ğŸ•ª', 'ğŸ•¯', 'ğŸ•µ', 'ğŸ•¶', 'ğŸ•·', 'ğŸ•º', 'ğŸ–', 'ğŸ–’', 'ğŸ–•', 'ğŸ––', 'ğŸ–¤', 'ğŸ–¥', 'ğŸ–¼', 'ğŸ—‚', 'ğŸ—“', 'ğŸ—', 'ğŸ—', 'ğŸ—¡', 'ğŸ—£', 'ğŸ—¨', 'ğŸ—³', 'ğŸ—»', 'ğŸ—¼', 'ğŸ—½', 'ğŸ—¾', 'ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‡', 'ğŸ˜ˆ', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜Œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜’', 'ğŸ˜“', 'ğŸ˜”', 'ğŸ˜•', 'ğŸ˜–', 'ğŸ˜—', 'ğŸ˜˜', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ˜›', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜Ÿ', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ˜£', 'ğŸ˜¤', 'ğŸ˜¥', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜ª', 'ğŸ˜«', 'ğŸ˜¬', 'ğŸ˜­', 'ğŸ˜®', 'ğŸ˜¯', 'ğŸ˜°', 'ğŸ˜±', 'ğŸ˜²', 'ğŸ˜³', 'ğŸ˜´', 'ğŸ˜µ', 'ğŸ˜¶', 'ğŸ˜·', 'ğŸ˜¸', 'ğŸ˜¹', 'ğŸ˜º', 'ğŸ˜»', 'ğŸ˜¼', 'ğŸ˜½', 'ğŸ˜¿', 'ğŸ™€', 'ğŸ™', 'ğŸ™‚', 'ğŸ™ƒ', 'ğŸ™„', 'ğŸ™…', 'ğŸ™†', 'ğŸ™‡', 'ğŸ™ˆ', 'ğŸ™‰', 'ğŸ™Š', 'ğŸ™‹', 'ğŸ™Œ', 'ğŸ™', 'ğŸ™', 'ğŸ™', 'ğŸš€', 'ğŸš', 'ğŸš‡', 'ğŸšˆ', 'ğŸšŒ', 'ğŸš‘', 'ğŸš“', 'ğŸš”', 'ğŸš–', 'ğŸš—', 'ğŸš˜', 'ğŸš™', 'ğŸš¢', 'ğŸš£', 'ğŸš¦', 'ğŸš§', 'ğŸš¨', 'ğŸš©', 'ğŸš«', 'ğŸš¬', 'ğŸš®', 'ğŸš²', 'ğŸš´', 'ğŸšµ', 'ğŸš¶', 'ğŸš»', 'ğŸš¼', 'ğŸš¿', 'ğŸ›€', 'ğŸ›ƒ', 'ğŸ›„', 'ğŸ›©', 'ğŸ›«', 'ğŸ›¬', 'ğŸ›°', 'ğŸ›³', 'ğŸ›´', 'ğŸ¤', 'ğŸ¤‘', 'ğŸ¤’', 'ğŸ¤“', 'ğŸ¤”', 'ğŸ¤•', 'ğŸ¤–', 'ğŸ¤—', 'ğŸ¤˜', 'ğŸ¤™', 'ğŸ¤š', 'ğŸ¤›', 'ğŸ¤œ', 'ğŸ¤', 'ğŸ¤', 'ğŸ¤ ', 'ğŸ¤¡', 'ğŸ¤¢', 'ğŸ¤£', 'ğŸ¤¤', 'ğŸ¤¥', 'ğŸ¤¦', 'ğŸ¤§', 'ğŸ¤³', 'ğŸ¤´', 'ğŸ¤·', 'ğŸ¥€', 'ğŸ¥', 'ğŸ¥‚', 'ğŸ¥ƒ', 'ğŸ¥„', 'ğŸ¥…', 'ğŸ¥‡', 'ğŸ¥Š', 'ğŸ¥', 'ğŸ¥’', 'ğŸ¥”', 'ğŸ¥˜', 'ğŸ¥', 'ğŸ¦€', 'ğŸ¦', 'ğŸ¦ƒ', 'ğŸ¦„', 'ğŸ¦‡', 'ğŸ¦‰', 'ğŸ¦‹', 'ğŸ¦‘', '\\U000fe4e6', '<end>']\n",
      "vocab size:  1805\n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "vocabulary = preprocess()\n",
    "# add <start> and <end> to the vocabulary\n",
    "vocabulary = [\"<start>\"] + vocabulary + [\"<end>\"]\n",
    "print(\"vocabulary: \", vocabulary)\n",
    "print(\"vocab size: \", len(vocabulary))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:00.558410Z",
     "end_time": "2023-04-20T10:41:01.335561Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sixteenth video in our channelâ¡ï¸ how to solve lineal functionsâ€¼ï¸ https://t.co/yAXTi5Zog6\n",
      "30\n",
      "b'Sixteenth video in our channel\\xe2\\x9e\\xa1\\xef\\xb8\\x8f how to solve lineal functions\\xe2\\x80\\xbc\\xef\\xb8\\x8f https://t.co/yAXTi5Zog6'\n",
      "0 83\n",
      "1 105\n",
      "2 120\n",
      "3 116\n",
      "4 101\n",
      "5 101\n",
      "6 110\n",
      "7 116\n",
      "8 104\n",
      "9 32\n",
      "10 118\n",
      "11 105\n",
      "12 100\n",
      "13 101\n",
      "14 111\n",
      "15 32\n",
      "16 105\n",
      "17 110\n",
      "18 32\n",
      "19 111\n",
      "20 117\n",
      "21 114\n",
      "22 32\n",
      "23 99\n",
      "24 104\n",
      "25 97\n",
      "26 110\n",
      "27 110\n",
      "28 101\n",
      "29 108\n",
      "30 226\n",
      "31 158\n",
      "32 161\n",
      "33 239\n",
      "34 184\n",
      "35 143\n",
      "36 32\n",
      "37 104\n",
      "38 111\n",
      "39 119\n",
      "40 32\n",
      "41 116\n",
      "42 111\n",
      "43 32\n",
      "44 115\n",
      "45 111\n",
      "46 108\n",
      "47 118\n",
      "48 101\n",
      "49 32\n",
      "50 108\n",
      "51 105\n",
      "52 110\n",
      "53 101\n",
      "54 97\n",
      "55 108\n",
      "56 32\n",
      "57 102\n",
      "58 117\n",
      "59 110\n",
      "60 99\n",
      "61 116\n",
      "62 105\n",
      "63 111\n",
      "64 110\n",
      "65 115\n",
      "66 226\n",
      "67 128\n",
      "68 188\n",
      "69 239\n",
      "70 184\n",
      "71 143\n",
      "72 32\n",
      "73 104\n",
      "74 116\n",
      "75 116\n",
      "76 112\n",
      "77 115\n",
      "78 58\n",
      "79 47\n",
      "80 47\n",
      "81 116\n",
      "82 46\n",
      "83 99\n",
      "84 111\n",
      "85 47\n",
      "86 121\n",
      "87 65\n",
      "88 88\n",
      "89 84\n",
      "90 105\n",
      "91 53\n",
      "92 90\n",
      "93 111\n",
      "94 103\n",
      "95 54\n"
     ]
    },
    {
     "data": {
      "text/plain": "5"
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "current_data = pd.read_csv(path + \"/\" + \"en.csv\", encoding=\"utf-8\")\n",
    "print(current_data[\"tweet_text\"][69])\n",
    "print(len('Sixteenth video in our channel'))\n",
    "\n",
    "test_string = current_data[\"tweet_text\"][69]\n",
    "test_string = test_string.encode('utf-8')\n",
    "print(test_string)\n",
    "for char, index in enumerate(test_string):\n",
    "    print(char, index)\n",
    "len('lâ¡ï¸ h')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.331562Z",
     "end_time": "2023-04-20T10:41:01.428268Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb2PGj0Yc2TY"
   },
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function lm that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
    "\n",
    "{\n",
    "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
    "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
    "}\n",
    "\n",
    "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
    "\n",
    "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# helper functions for lm\n",
    "\n",
    "def build_ngram_model(current_data, n):\n",
    "    \"\"\" Builds an n-gram model from the given data\n",
    "    Args:\n",
    "        current_data: a pandas dataframe with a column named \"tweet_text\"\n",
    "        n: the n in n-gram\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    # iterate over all the tweets in the data file\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        # define n_gram and n_minus_1_gram\n",
    "        n_gram = \"<start>\" + tweet[0:n-1]\n",
    "        n_minus_1_gram = n_gram[:-1]\n",
    "        n_th_token = n_gram[-1]\n",
    "        # add the n-gram to the model\n",
    "        if n_minus_1_gram not in model:\n",
    "            model[n_minus_1_gram] = {}\n",
    "        # add the n_th token to the model\n",
    "        if n_th_token not in model[n_minus_1_gram]:\n",
    "            model[n_minus_1_gram][n_th_token] = 0\n",
    "        # count the n_th token, i.e. add 1 to its count\n",
    "        model[n_minus_1_gram][n_th_token] += 1\n",
    "        # iterate over all the n-grams in the tweet\n",
    "        for i in range(len(tweet) - n + 1):\n",
    "            n_gram = tweet[i:i+n]\n",
    "            n_minus_1_gram = n_gram[:-1]\n",
    "            n_th_token = n_gram[-1]\n",
    "            # add the n-gram to the model\n",
    "            if n_minus_1_gram not in model:\n",
    "                model[n_minus_1_gram] = {}\n",
    "            # add the n_th token to the model\n",
    "            if n_th_token not in model[n_minus_1_gram]:\n",
    "                model[n_minus_1_gram][n_th_token] = 0\n",
    "            # count the n_th token, i.e. add 1 to its count\n",
    "            model[n_minus_1_gram][n_th_token] += 1\n",
    "        n_gram = tweet[-n+1:] + \"<end>\"\n",
    "        n_minus_1_gram = tweet[-n+1:]\n",
    "        n_th_token = \"<end>\"\n",
    "        # add the n-gram to the model\n",
    "        if n_minus_1_gram not in model:\n",
    "            model[n_minus_1_gram] = {}\n",
    "        # add the n_th token to the model\n",
    "        if n_th_token not in model[n_minus_1_gram]:\n",
    "            model[n_minus_1_gram][n_th_token] = 0\n",
    "        # count the n_th token, i.e. add 1 to its count\n",
    "        model[n_minus_1_gram][n_th_token] += 1\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def add_one_smoothing(model, vocabulary):\n",
    "    \"\"\" Adds add_one smoothing to the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: count}}\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # iterate over all the tokens in the vocabulary\n",
    "        for token in vocabulary:\n",
    "            # add the token to the model\n",
    "            if token not in model[n_minus_1_gram]:\n",
    "                model[n_minus_1_gram][token] = 0\n",
    "            # count the token, i.e. add 1 to its count (add one smoothing)\n",
    "            model[n_minus_1_gram][token] += 1\n",
    "    return model\n",
    "\n",
    "def calculate_probabilities(model):\n",
    "    \"\"\" Calculates the probabilities for the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: count}}\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # get the counts of all the tokens\n",
    "        token_counts = model[n_minus_1_gram].values()\n",
    "        # calculate the total count\n",
    "        total_count = sum(token_counts)\n",
    "        # iterate over all the tokens in the model\n",
    "        for token in model[n_minus_1_gram]:\n",
    "            # calculate the probability, i.e. divide the count by the total count\n",
    "            model[n_minus_1_gram][token] /= total_count\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.371768Z",
     "end_time": "2023-04-20T10:41:01.430756Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kMC_u8eQbVvZ",
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.401175Z",
     "end_time": "2023-04-20T10:41:01.430756Z"
    }
   },
   "source": [
    "def lm(n, vocabulary, data_file_path, add_one):\n",
    "    \"\"\" Builds an n-gram model from the given data\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "        data_file_path: the data_file from which we record probabilities for our model\n",
    "        add_one: True/False (use add_one smoothing or not)\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file_path,  encoding=\"utf-8\")\n",
    "    # build the n-gram model\n",
    "    model = build_ngram_model(current_data, n)\n",
    "    if add_one:\n",
    "        # add one smoothing\n",
    "        model = add_one_smoothing(model, vocabulary)\n",
    "    # calculate the probabilities\n",
    "    model = calculate_probabilities(model)\n",
    "    return model"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# # # call the function for the first data file\n",
    "# lm_model = lm(3, vocabulary, path + \"/\" + data_files[0], True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.417433Z",
     "end_time": "2023-04-20T10:41:01.432533Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# # print the model[key][token] for a specific key\n",
    "# key = lm_model.keys().__iter__().__next__()\n",
    "# probabilities_sorted = sorted(lm_model[key].items(), key=lambda x: x[1], reverse=True)\n",
    "# # print(\"key: \", key)\n",
    "# # print(\"probabilities: \", probabilities_sorted[:10])\n",
    "# # print <end> probability\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.426374Z",
     "end_time": "2023-04-20T10:41:01.526098Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8TchtI22I3"
   },
   "source": [
    "**Part 3**\n",
    "\n",
    "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "# a function that calculates the perplexity of a model\n",
    "def calculate_perplexity(current_data, n, model):\n",
    "    \"\"\" Calculates the perplexity of a model\n",
    "    Args:\n",
    "        current_data: the data from which we record probabilities for our model\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    Returns:\n",
    "        the perplexity of the model\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0\n",
    "    n_gram_count = 0\n",
    "\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        # add the first n-gram\n",
    "        n_minus_1_gram = \"<start>\" + tweet[:n-2]\n",
    "        n_th_token = tweet[n-1]\n",
    "        if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "            log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "            n_gram_count += 1\n",
    "        # iterate over all the n-grams in the tweet\n",
    "        for i in range(len(tweet) - n + 1):\n",
    "            n_gram = tweet[i:i+n]\n",
    "            n_minus_1_gram = n_gram[:-1]\n",
    "            n_th_token = n_gram[-1]\n",
    "\n",
    "            if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "                log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "                n_gram_count += 1\n",
    "        # add the last n-gram\n",
    "        n_minus_1_gram = tweet[-n+1:]\n",
    "        n_th_token = \"<end>\"\n",
    "        if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "            log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "            n_gram_count += 1\n",
    "\n",
    "    if n_gram_count == 0:\n",
    "        return float('inf')  # Return infinite perplexity if no n-grams found\n",
    "\n",
    "    entropy = log_prob_sum / n_gram_count\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def eval(n, model, data_file):\n",
    "    \"\"\" Evaluates the perplexity of a model\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "        data_file: the data file path for which we want to calculate the perplexity\n",
    "    Returns:\n",
    "        the perplexity of the model\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file, encoding=\"utf-8\")\n",
    "    # calculate the perplexity\n",
    "    perplexity = calculate_perplexity(current_data, n, model)\n",
    "    return perplexity"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.446333Z",
     "end_time": "2023-04-20T10:41:01.545011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "# # call the function\n",
    "# perplexity = eval(3, lm_model, path + \"/\" + data_files[0])\n",
    "# print(\"perplexity: \", perplexity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.468566Z",
     "end_time": "2023-04-20T10:41:01.545011Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enGmtLE3921p"
   },
   "source": [
    "**Part 4**\n",
    "\n",
    "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values.\n",
    "\n",
    "Save the dataframe to a CSV with the name format: {student_id_1}\\_...\\_{student_id_n}\\_part4.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "def match(n, add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Then, calculate the perplexity of all possible pairs.\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        df: a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # create a dataframe\n",
    "    df = pd.DataFrame(columns=data_files, index=data_files)\n",
    "\n",
    "    # create models for every language\n",
    "    models = compute_data_files_models(data_files, n, vocabulary, path, add_one)\n",
    "\n",
    "    # calculate the perplexity of all possible pairs\n",
    "    for lang1 in data_files: # will be the model\n",
    "        # define the model\n",
    "        current_model = models[lang1]\n",
    "        for lang2 in data_files: # will be the data file\n",
    "            # define the data file\n",
    "            current_data_file = path + \"/\" + lang2\n",
    "            # evaluate the model\n",
    "            perplexity = eval(n, current_model, current_data_file)\n",
    "            # save the perplexity to the dataframe\n",
    "            df[lang1][lang2] = perplexity\n",
    "    # TODO: need to return only df\n",
    "    return df, models # return the dataframe and the models, so that we can use them later\n",
    "\n",
    "def compute_data_files_models(data_files, n, vocabulary, path , add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Args:\n",
    "        data_files: the data files to create models for\n",
    "        n: the n in n-gram\n",
    "        vocabulary: the vocabulary\n",
    "        path: the path to the data files\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for data_file in data_files:\n",
    "        models[data_file] = lm(n, vocabulary, path + \"/\" + data_file, add_one)\n",
    "    return models\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.489555Z",
     "end_time": "2023-04-20T10:41:01.545011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# # call the function\n",
    "# df_part4, models_part4 = match(3, True)\n",
    "# print(\"dataframe: \")\n",
    "# print(df_part4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.501976Z",
     "end_time": "2023-04-20T10:41:01.545011Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# # save the dataframe to a CSV\n",
    "# df_part4.to_csv(\"language_perplexity_part4.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.515707Z",
     "end_time": "2023-04-20T10:41:01.547022Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waGMwA8H_n17"
   },
   "source": [
    "**Part 5**\n",
    "\n",
    "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another.\n",
    "\n",
    "Load each result to a dataframe and save to a CSV with the name format: \n",
    "\n",
    "for cases with add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_part5.csv\n",
    "\n",
    "For cases without add_one:\n",
    "{student_id_1}\\_...\\_{student_id_n}\\_n1\\_wo\\_addone\\_part5.csv\n",
    "\n",
    "Follow the same format for n2,n3, and n4\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nk32naXyAMdl",
    "ExecuteTime": {
     "start_time": "2023-04-20T10:41:01.536849Z",
     "end_time": "2023-04-20T10:41:01.548156Z"
    }
   },
   "source": [
    "def run_match():\n",
    "    \"\"\" Runs the match function for all the n values and add_one values\n",
    "    Returns:\n",
    "        dataframes: a dictionary of the dataframes, so that we can use them later,\n",
    "                    i.e {n, add_one: dataframe} where dataframe is a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "        language_models_dict: a dictionary of the models, so that we can use them later,\n",
    "                              i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    n_values = [1, 2, 3] # TODO: change to [1, 2, 3, 4], memorry problem, change match function to use only dataframes\n",
    "    add_one_values = [True, False]\n",
    "    # create dictionaries for the dataframes, key = (n, add_one), value = dataframe\n",
    "    dataframes = {}\n",
    "    # create a dictionary for the language models, key = (n, add_one), value = language models = {language: model}\n",
    "    language_models_dict = {}\n",
    "    # iterate over all the n values\n",
    "    for n in n_values:\n",
    "        # iterate over all the add_one values\n",
    "        for add_one in add_one_values:\n",
    "            # create the dataframe and the language models, using the match function\n",
    "            current_df, current_language_models = match(n, add_one)\n",
    "            print(\"completed n = \" + str(n) + \", add_one = \" + str(add_one) + \"!\")\n",
    "            # add the dataframe to the dictionary\n",
    "            dataframes[(n, add_one)] = current_df\n",
    "            # add the language models to the dictionary\n",
    "            language_models_dict[(n, add_one)] = current_language_models\n",
    "            # save the dataframe to a CSV\n",
    "            if add_one:\n",
    "                current_df.to_csv(\"language_perplexity_n\" + str(n) + \"_part5.csv\")\n",
    "            else:\n",
    "                current_df.to_csv(\"language_perplexity_n\" + str(n) + \"_wo_addone_part5.csv\")\n",
    "    return dataframes, language_models_dict # return the dataframes and the language models, so that we can use them later\n"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed n = 1, add_one = True!\n",
      "completed n = 1, add_one = False!\n",
      "completed n = 2, add_one = True!\n"
     ]
    }
   ],
   "source": [
    "run_match_dataframes, run_match_language_models = run_match()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:40:25.037010Z",
     "end_time": "2023-04-19T12:44:01.489897Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_models_to_pickle(language_models_dict=run_match_language_models, filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Saves the language models to a pickle file\n",
    "    Args:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # save run_match_language_models to a pickle file, so that we can use it later\n",
    "    with open('run_match_language_models.pickle', 'wb') as handle:\n",
    "        pickle.dump(language_models_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_dataframes_to_pickle(dataframes_dict=run_match_dataframes, filename=\"run_match_dataframes.pickle\"):\n",
    "    \"\"\" Saves the dataframes to a pickle file\n",
    "    Args:\n",
    "        dataframes_dict: a dictionary of the dataframes, i.e {n, add_one: dataframe} where dataframe is a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "    \"\"\"\n",
    "    # save run_match_dataframes to a pickle file, so that we can use it later\n",
    "    with open('run_match_dataframes.pickle', 'wb') as handle:\n",
    "        pickle.dump(dataframes_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:44:01.525099Z",
     "end_time": "2023-04-19T12:44:01.534708Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_models_from_pickle(filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Loads the language models from a pickle file\n",
    "    Returns:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # load run_match_language_models from a pickle file\n",
    "    with open('run_match_language_models.pickle', 'rb') as handle:\n",
    "        language_models_dict = pickle.load(handle)\n",
    "    return language_models_dict\n",
    "\n",
    "def load_dataframes_from_pickle(filename=\"run_match_dataframes.pickle\"):\n",
    "    \"\"\" Loads the dataframes from a pickle file\n",
    "    Returns:\n",
    "        dataframes_dict: a dictionary of the dataframes, i.e {n, add_one: dataframe} where dataframe is a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:44:01.540679Z",
     "end_time": "2023-04-19T12:44:01.550941Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the language models to a pickle file\n",
    "save_models_to_pickle(run_match_language_models, \"run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:44:01.553131Z",
     "end_time": "2023-04-19T12:45:10.061426Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# load the language models from a pickle file\n",
    "run_match_language_models = load_models_from_pickle(\"run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:45:10.053675Z",
     "end_time": "2023-04-19T12:46:21.100761Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# test the language models dictionary\n",
    "\n",
    "test_n = 3\n",
    "test_add_one = True\n",
    "test_language = \"en.csv\"\n",
    "test_language_models = run_match_language_models[(test_n, test_add_one)][test_language]\n",
    "test_n_minus_1_gram = \"<start>a\"\n",
    "\n",
    "next_token_probability = test_language_models[test_n_minus_1_gram]\n",
    "next_token_probability = sorted(next_token_probability.items(), key=lambda x: x[1], reverse=True)\n",
    "print(next_token_probability)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:46:21.085679Z",
     "end_time": "2023-04-19T12:46:21.131530Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg4h5Cl0q2nR"
   },
   "source": [
    "**Part 6**\n",
    "\n",
    "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
    "\n",
    "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be accepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qD6IRIQLrlZF",
    "ExecuteTime": {
     "start_time": "2023-04-19T12:46:21.143058Z",
     "end_time": "2023-04-19T12:46:35.500068Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "# this function classifies the test sentences\n",
    "def classify():\n",
    "    \"\"\" Classifies the test sentences\n",
    "    Returns:\n",
    "        classification_result: a list of tuples, where each tuple contains the tweet_id, the sentence, the true language, and the predicted language\n",
    "    \"\"\"\n",
    "    # we will use the language models from part 5, with n = 3 and add_one = True\n",
    "    language_models = run_match_language_models[(3, True)]\n",
    "\n",
    "    # read the test file, tweet_id, tweet_text, label\n",
    "    test_data = pd.read_csv(path + \"/test.csv\",  encoding=\"utf-8\")\n",
    "\n",
    "    # classify the sentences\n",
    "    classification_result = []\n",
    "\n",
    "    # iterate over the rows in the test data\n",
    "    for index, row in test_data.iterrows():\n",
    "        tweet_id = row['tweet_id']\n",
    "        sentence = row['tweet_text']\n",
    "        true_language = row['label']\n",
    "        predicted_language = ''\n",
    "\n",
    "        predicted_language = single_classification(sentence, language_models)\n",
    "\n",
    "        # add the result to the classification_result list\n",
    "        classification_result.append((tweet_id, sentence, true_language, predicted_language))\n",
    "\n",
    "    return classification_result\n",
    "\n",
    "\n",
    "# this function classifies a single sentence\n",
    "def single_classification(sentence, language_models = run_match_language_models[(3, True)]):\n",
    "    \"\"\" Classifies a single sentence\n",
    "    Args:\n",
    "        sentence: the sentence to classify\n",
    "        language_models: the language models to use for classification\n",
    "                         i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    Returns:\n",
    "        predicted_language: the predicted language of the sentence\n",
    "    \"\"\"\n",
    "    predicted_language = ''\n",
    "    min_perplexity = float('inf')\n",
    "    # iterate over the language models\n",
    "    for data_file in data_files:\n",
    "        current_model = language_models[data_file]\n",
    "        # create a temporary DataFrame, with the sentence as the only row\n",
    "        temp_df = pd.DataFrame([sentence], columns=['tweet_text'])\n",
    "        # calculate the perplexity using the temporary DataFrame\n",
    "        current_perplexity = calculate_perplexity(temp_df, 3, current_model)\n",
    "\n",
    "        if current_perplexity < min_perplexity:\n",
    "            min_perplexity = current_perplexity\n",
    "            predicted_language = data_file[:-4] # remove the .csv from the end of the file name\n",
    "    return predicted_language\n",
    "\n",
    "\n",
    "\n",
    "classification_result = classify()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print real language and predicted language\n",
    "count_correct = 0\n",
    "for result in classification_result:\n",
    "    print(result[2] + \" \" + result[3])\n",
    "    if result[2] == result[3]:\n",
    "        count_correct += 1\n",
    "print(\"accuracy = \" + str(count_correct / len(classification_result)))\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:46:35.504753Z",
     "end_time": "2023-04-19T12:46:35.590313Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ECmLd3rktZ"
   },
   "source": [
    "**Part 7**\n",
    "\n",
    "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n",
    "\n",
    "Load the results to a CSV (using a DataFrame), where the row indicates the F1 results, and the columns indicate the model used. Name it {student_id_1}\\_...\\_{student_id_n}\\_part7.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# we will use the following dictionary to convert the strings to numbers\n",
    "language_to_number = {}\n",
    "# we will use the following dictionary to convert the numbers back to strings\n",
    "number_to_language = {}\n",
    "\n",
    "number = 0\n",
    "# iterate over the classification results\n",
    "for result in classification_result:\n",
    "    if result[2] not in language_to_number:\n",
    "        language_to_number[result[2]] = number\n",
    "        number_to_language[number] = result[2]\n",
    "        number += 1\n",
    "    if result[3] not in language_to_number:\n",
    "        language_to_number[result[3]] = number\n",
    "        number_to_language[number] = result[3]\n",
    "        number += 1"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:46:35.564488Z",
     "end_time": "2023-04-19T12:46:35.595884Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VOBO3YQls66r",
    "ExecuteTime": {
     "start_time": "2023-04-19T12:46:35.565497Z",
     "end_time": "2023-04-19T12:46:37.480058Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "source": [
    "import sklearn.metrics as metrics\n",
    "# The f1 score = 2 *(TP / (2TP + FP + FN))\n",
    "def calc_f1(result):\n",
    "    \"\"\" Calculates the f1 score of the classification result\n",
    "    Args:\n",
    "        result: a list of tuples, where each tuple contains the tweet_id, the sentence, the true language, and the predicted language\n",
    "    Returns:\n",
    "        f1_score: the f1 score of the classification result\n",
    "    \"\"\"\n",
    "    # create a DataFrame with the results\n",
    "    df = pd.DataFrame(result, columns=['tweet_id', 'tweet_text', 'true_language', 'predicted_language'])\n",
    "    # drop the tweet_id and tweet_text columns\n",
    "    df = df.drop(columns=['tweet_id', 'tweet_text'])\n",
    "    # convert the true_language and predicted_language columns to numbers\n",
    "    df['true_language'] = df['true_language'].apply(lambda x: language_to_number[x])\n",
    "    df['predicted_language'] = df['predicted_language'].apply(lambda x: language_to_number[x])\n",
    "    # calculate the f1 score\n",
    "    f1_score = metrics.f1_score(df['true_language'], df['predicted_language'], average='macro')\n",
    "    return f1_score\n",
    "\n",
    "\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "f1_score = calc_f1(classification_result)\n",
    "print(f1_score)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:46:37.481053Z",
     "end_time": "2023-04-19T12:46:37.535035Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br><br><br>\n",
    "**Part 8**  \n",
    "Let's use your Language model (dictionary) for generation (NLG).\n",
    "\n",
    "When it comes to sampling from a language model decoder during text generation, there are several different methods that can be used to control the randomness and diversity of the generated text. \n",
    "\n",
    "Some of the most commonly used methods include:\n",
    "\n",
    "> `Greedy sampling`\n",
    "In this method, the model simply selects the word with the highest probability as the next word at each time step. This method can produce fluent text, but it can also lead to repetitive or predictable output.\n",
    "\n",
    "> `Temperature scaling`  \n",
    "Temperature scaling involves scaling the logits output of the language model by a temperature parameter before softmax normalization. This has the effect of smoothing the distribution of probabilities and increasing the probability of lower-probability words, which can lead to more diverse and creative output.\n",
    "\n",
    "> `Top-K sampling`  \n",
    "In this method, the model restricts the sampling to the top-K most likely words at each time step, where K is a predefined hyperparameter. This can generate more diverse output than greedy sampling, while limiting the number of low-probability words that are sampled.\n",
    "\n",
    "> `Nucleus sampling` (also known as top-p sampling)  \n",
    "This method restricts the sampling to the smallest possible set of words whose cumulative probability exceeds a certain threshold, defined by a hyperparameter p. Like top-K sampling, this can generate more diverse output than greedy sampling, while avoiding sampling extremely low probability words.\n",
    "\n",
    "> `Beam search`  \n",
    "Beam search involves maintaining a fixed number k of candidate output sequences at each time step, and then selecting the k most likely sequences based on their probabilities. This can improve the fluency and coherence of the output, but may not produce as much diversity as sampling methods.\n",
    "\n",
    "The choice of sampling method depends on the specific application and desired balance between fluency, diversity, and randomness. Hyperparameters such as temperature, K, p, and beam size can also be tuned to adjust the behavior of the language model during sampling.\n",
    "\n",
    "\n",
    "You may read more about this concept in <a href='https://huggingface.co/blog/how-to-generate#:~:text=pad_token_id%3Dtokenizer.eos_token_id)-,Greedy%20Search,-Greedy%20search%20simply'>this</a> blog post.\n"
   ],
   "metadata": {
    "id": "NfBYgfjADNPL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Please added the needed code for each sampeling method:**"
   ],
   "metadata": {
    "id": "GbReeHtwNWKS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def softmax(probabilities):\n",
    "    \"\"\" Applies the softmax function to the probabilities\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities (not yet probalities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a dictionary of probabilities (normalized)\n",
    "    \"\"\"\n",
    "    np_probabilities = np.array(list(probabilities.values()))\n",
    "    np_probabilities = np.exp(np_probabilities)\n",
    "    np_probabilities = np_probabilities / np.sum(np_probabilities)\n",
    "    # convert the numpy array back to a dictionary\n",
    "    probabilities = {key: value for key, value in zip(probabilities.keys(), np_probabilities)}\n",
    "    return probabilities\n",
    "\n",
    "def make_prob_1(probabilities):\n",
    "    \"\"\" Makes the sum of the probabilities equal to 1\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities (not yet probalities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a dictionary of probabilities (normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    # by dividing each probability by the sum of all probabilities\n",
    "    sum_prob = sum(probabilities)\n",
    "    probabilities = [prob / sum_prob for prob in probabilities]\n",
    "    return probabilities\n",
    "\n",
    "def get_correct_model(all_models=run_match_language_models, prefix=\"h\", language=\"en.csv\", add_one=True):\n",
    "    \"\"\" Gets the correct language model, the correct n-1 prefix, and the probabilities of the next token\n",
    "    Args:\n",
    "        all_models: a dictionary of language models\n",
    "        prefix: the prefix\n",
    "        language: the language\n",
    "        add_one: whether to use add-one smoothing or not\n",
    "    Returns:\n",
    "        correct_model: the correct language model (dictionary), where the keys are the prefixes and the values are the probabilities of the next token\n",
    "        current_n_minus_1_prefix: the correct key for the language model\n",
    "        next_token_probabilities: the probabilities of the next token (dictionary), where the keys are the tokens and the values are the probabilities\n",
    "    \"\"\"\n",
    "    max_n = 3\n",
    "    # TODO: need to consider <start> token, maybe add parameter to function\n",
    "    start_token_length = len(\"<start>\")\n",
    "\n",
    "    # we want to use maximum n-gram we can, but not more than max_n\n",
    "    n = min(max_n, len(prefix) + 1)\n",
    "\n",
    "    # get the n-gram model\n",
    "    correct_model = all_models[(n, add_one)][language]\n",
    "\n",
    "    # get the n-1 prefix\n",
    "    current_n_minus_1_prefix = prefix[-(n - 1):]\n",
    "\n",
    "    # get the probabilities of the next token. if the prefix is not in the language model, sample a random token\n",
    "    # TODO: maybe change to STOP when key is not in the language model\n",
    "    if current_n_minus_1_prefix not in correct_model:\n",
    "        next_token_probabilities = {token: 1 / len(correct_model) for token in correct_model}\n",
    "    else:\n",
    "        next_token_probabilities = correct_model[current_n_minus_1_prefix]\n",
    "\n",
    "    return correct_model, current_n_minus_1_prefix, next_token_probabilities\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T12:46:37.530750Z",
     "end_time": "2023-04-19T12:46:37.545989Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_greedy(probabilities, k=1):\n",
    "    \"\"\" Samples the next token greedily, i.e. the token with the highest probability\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "    Returns:\n",
    "        max_token: the token with the highest probability\n",
    "    \"\"\"\n",
    "    # sample the token with the k highest probability\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    # if k is larger than the number of probabilities, set k to the number of probabilities\n",
    "    k = k if len(sorted_probabilities) >= k else len(sorted_probabilities)\n",
    "    # sample the token with the k highest probability\n",
    "    next_token = sorted_probabilities[k - 1][0]\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_temperature(probabilities, temperature=1.0, k=1):\n",
    "    \"\"\" Samples the next token using temperature sampling\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        temperature: the temperature\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # scale the probabilities by the temperature\n",
    "    probabilities = {key: value ** (1 / temperature) for key, value in probabilities.items()}\n",
    "    # softmax the probabilities\n",
    "    probabilities = softmax(probabilities)\n",
    "    # sample from the probabilities dictionary, use the np.random.choice function\n",
    "    np_probabilities = np.array(list(probabilities.values()))\n",
    "    np_tokens = np.array(list(probabilities.keys()))\n",
    "    next_token = np.random.choice(np_tokens, p=np_probabilities)\n",
    "    # return the sampled token\n",
    "    return next_token\n",
    "\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_topK(probabilities, k=1):\n",
    "    \"\"\" Samples the next token using top-k sampling, i.e. only the top k tokens are considered\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        k: the number of tokens to consider\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    # take the top k\n",
    "    top_k = sorted_probabilities[:k]\n",
    "    # split the top k into tokens and probabilities\n",
    "    top_k_probs = [prob for (token, prob) in top_k]\n",
    "    top_k_tokens = [token for (token, prob) in top_k]\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_k_probs = make_prob_1(top_k_probs)\n",
    "    # sample from the top k tokens\n",
    "    next_token = np.random.choice(top_k_tokens, p=top_k_probs)\n",
    "    return next_token\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_topP(probabilities, p=0.9):\n",
    "    \"\"\" Samples the next token using top-p sampling,\n",
    "    i.e. only the tokens with the highest probabilities are considered, until the sum of the probabilities is greater than p\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        p: the threshold\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    current_sum = 0\n",
    "    top_p_tokens = []\n",
    "    top_p_probs = []\n",
    "    current_index = 0\n",
    "    while current_sum < p:\n",
    "        top_p_tokens.append(sorted_probabilities[current_index][0])\n",
    "        top_p_probs.append(sorted_probabilities[current_index][1])\n",
    "        current_sum += sorted_probabilities[current_index][1]\n",
    "        current_index += 1\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_p_probs = make_prob_1(top_p_probs)\n",
    "    # sample from the top p tokens\n",
    "    next_token = np.random.choice(top_p_tokens, p=top_p_probs)\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def sample_beam(probabilities, num_beams = 3):\n",
    "    \"\"\" Samples the next tokens using beam search, i.e., keeps the top num_beams hypotheses at each step.\n",
    "        Helper function for beam_search\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        num_beams: the number of beams to keep, i.e. the number of hypotheses to keep at each step\n",
    "    Returns:\n",
    "        beam_tokens: a list of top num_beams tokens\n",
    "        beam_probs: a list of the corresponding probabilities of the top num_beams tokens\n",
    "    \"\"\"\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_beams = sorted_probabilities[:num_beams]\n",
    "\n",
    "    beam_tokens = [token for (token, prob) in top_beams]\n",
    "    beam_probs = [prob for (token, prob) in top_beams]\n",
    "\n",
    "    return beam_tokens, beam_probs\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "v4TrLs1kI3fW",
    "ExecuteTime": {
     "start_time": "2023-04-19T13:18:28.508946Z",
     "end_time": "2023-04-19T13:18:28.527434Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your Language Model to generate each one out of the following examples with the coresponding params.    \n",
    "Notice the 4 core issues: \n",
    "- Starting tokens\n",
    "- Length of the generation\n",
    "- Sampling methond (use all)\n",
    "- Stop Token (if this token is sampled, stop generating)"
   ],
   "metadata": {
    "id": "Giylo6-lI21t",
    "ExecuteTime": {
     "start_time": "2023-04-03T17:05:48.014439Z",
     "end_time": "2023-04-03T17:05:48.029376Z"
    }
   },
   "outputs": [],
   "execution_count": 50
  },
  {
   "cell_type": "code",
   "source": [
    "test_ = {\n",
    "    'example1' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['greedy','beam'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example2' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['temperature','topK','topP'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example3' : {\n",
    "        'start_tokens' : \"He\",\n",
    "        'sampling_method' : ['greedy','beam','temperature','topK','topP'],\n",
    "        'gen_length' : \"20\",\n",
    "        'stop_token' : \"me\",\n",
    "        'generation' : []\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "id": "9pUOkRtjN1mI",
    "ExecuteTime": {
     "start_time": "2023-04-19T13:18:29.941779Z",
     "end_time": "2023-04-19T13:18:29.955284Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def generate_string(all_models, prefix, sampling_method, gen_length, stop_token, num_beams=5):\n",
    "    \"\"\" Generates a string using the given language model\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP', 'beam'\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        num_beams: the number of beams to keep (only relevant for beam search)\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    if sampling_method == 'beam':\n",
    "        return beam_search(all_models, prefix, gen_length, stop_token, num_beams)\n",
    "    else:\n",
    "        return generate_string_not_beam(all_models, prefix, gen_length, stop_token, sampling_method)\n",
    "\n",
    "def beam_search(all_models, prefix, gen_length, stop_token, num_beams):\n",
    "    \"\"\" Generates a string using beam search\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        num_beams: the number of beams to keep (only relevant for beam search)\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    # initialize the beams\n",
    "    beams = [(prefix, 0)]  # (prefix, log_prob)\n",
    "\n",
    "    # generate the string token by token\n",
    "    for _ in range(gen_length):\n",
    "        new_beams = []\n",
    "\n",
    "        # sample the next token for each beam\n",
    "        for beam_prefix, beam_log_prob in beams:\n",
    "            # get the correct language model\n",
    "            current_lm, current_n_minus_1_prefix, next_token_probabilities = get_correct_model(all_models, beam_prefix)\n",
    "\n",
    "            # sample the top num_beams tokens and probabilities\n",
    "            beam_tokens, beam_probs = sample_beam(next_token_probabilities, num_beams)\n",
    "\n",
    "            # update the beams\n",
    "            for token, prob in zip(beam_tokens, beam_probs):\n",
    "                new_prefix, new_log_prob = update_beam(beam_prefix, beam_log_prob, token, prob, stop_token)\n",
    "                new_beams.append((new_prefix, new_log_prob))\n",
    "\n",
    "        # keep the top num_beams beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:num_beams]\n",
    "\n",
    "    # get the best beam\n",
    "    best_beam = beams[0][0]\n",
    "\n",
    "    return best_beam\n",
    "\n",
    "def generate_string_not_beam(all_models, prefix, gen_length, stop_token, sampling_method):\n",
    "    \"\"\" Samples the next tokens using the given language model\n",
    "    Args:\n",
    "        all_models: where all_models = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    current_prefix = prefix\n",
    "\n",
    "    # generate the string token by token\n",
    "    for i in range(gen_length):\n",
    "        # get the correct language model\n",
    "        current_lm, current_n_minus_1_prefix, next_token_probabilities = get_correct_model(all_models, current_prefix)\n",
    "\n",
    "        # sample the next token\n",
    "        next_token = select_next_token(next_token_probabilities, sampling_method)\n",
    "        print(\"next_token: \", next_token)\n",
    "        print(\"current_prefix: \", current_prefix)\n",
    "        print(\"len(current_prefix): \", len(current_prefix))\n",
    "        # update the current prefix\n",
    "        current_prefix += next_token\n",
    "\n",
    "        # stop if the stop token is sampled\n",
    "        if next_token == stop_token or next_token == \"<end>\":\n",
    "            break\n",
    "\n",
    "    return current_prefix\n",
    "\n",
    "def update_beam(beam_prefix, beam_log_prob, token, prob, stop_token):\n",
    "    \"\"\" Updates the beam, i.e. the current prefix and the current log probability\n",
    "    Args:\n",
    "        beam_prefix: the current beam prefix\n",
    "        beam_log_prob: the current beam log probability\n",
    "        token: the next token\n",
    "        prob: the probability of the next token\n",
    "        stop_token: the token that stops the generation\n",
    "    Returns:\n",
    "        new_prefix: the new beam prefix\n",
    "        new_log_prob: the new beam log probability\n",
    "    \"\"\"\n",
    "\n",
    "    # update the prefix and the log probability\n",
    "    new_prefix = beam_prefix + token\n",
    "    new_log_prob = beam_log_prob + np.log(prob)\n",
    "\n",
    "    # remove the last token if it is the stop token, TODO: check if this is correct, or if we should consider shorter beams\n",
    "    if token == stop_token:\n",
    "        new_prefix = new_prefix[:-1]\n",
    "\n",
    "    return new_prefix, new_log_prob\n",
    "\n",
    "def select_next_token(next_token_probabilities, sampling_method, k_greedy=1, temperature=0.5, top_k=5, p=0.9):\n",
    "    \"\"\" Selects the next token using the given sampling method\n",
    "    Args:\n",
    "        next_token_probabilities: the probabilities of the next tokens\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "    Returns:\n",
    "        next_token: the next token\n",
    "    \"\"\"\n",
    "    if sampling_method == 'greedy':\n",
    "        return sample_greedy(next_token_probabilities, k_greedy)\n",
    "    elif sampling_method == 'temperature':\n",
    "        return sample_temperature(next_token_probabilities, temperature)\n",
    "    elif sampling_method == 'topK':\n",
    "        return sample_topK(next_token_probabilities, top_k)\n",
    "    elif sampling_method == 'topP':\n",
    "        return sample_topP(next_token_probabilities, p)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown sampling method: {sampling_method}')\n",
    "\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T13:23:08.650913Z",
     "end_time": "2023-04-19T13:23:08.674128Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your LM to generate a string based on the parametes of each examples, and store the generation sequance at the generation list."
   ],
   "metadata": {
    "id": "YTbF-9zKVchQ"
   },
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "all_models = run_match_language_models\n",
    "language = \"en.csv\"\n",
    "add_one = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T13:23:09.602741Z",
     "end_time": "2023-04-19T13:23:09.619051Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "current_model = all_models[(3, True)][language]\n",
    "print(current_model[\"ğŸ’ªğŸ½\"])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T13:33:22.873193Z",
     "end_time": "2023-04-19T13:33:22.896066Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# clear the generations, useful if you want to run the code multiple times\n",
    "for example in test_:\n",
    "    test_[example]['generation'] = []\n",
    "\n",
    "# generate the strings for each example\n",
    "for example in test_:\n",
    "    print(f\"Example {example}:\")\n",
    "    # generate the string for each sampling method\n",
    "    for i in range(len(test_[example]['sampling_method'])):\n",
    "        print(\"sampling method:\", test_[example]['sampling_method'][i])\n",
    "        # get the parameters\n",
    "        sampling_method = test_[example]['sampling_method'][i]\n",
    "        gen_length = int(test_[example]['gen_length'])\n",
    "        stop_token = test_[example]['stop_token']\n",
    "        prefix = test_[example]['start_tokens']\n",
    "\n",
    "        # generate the string\n",
    "        generated_string = generate_string(all_models, prefix=prefix, sampling_method=sampling_method, gen_length=gen_length, stop_token=stop_token)\n",
    "        # store the string\n",
    "        test_[example]['generation'].append(generated_string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T13:24:32.446861Z",
     "end_time": "2023-04-19T13:24:32.734977Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# print the generation results for each example\n",
    "\n",
    "# iterate over the examples\n",
    "for example in test_:\n",
    "    print(f\"Example {example}:\")\n",
    "    # iterate over the sampling methods\n",
    "    for i in range(len(test_[example]['generation'])): # we don't want to print the beam search\n",
    "        print(\"Generation using\", test_[example]['sampling_method'][i], \":\", test_[example]['generation'][i])\n",
    "        print(\"generation length:\", len(test_[example]['generation'][i]))\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-19T13:29:03.453292Z",
     "end_time": "2023-04-19T13:29:03.482989Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### do not change ###\n",
    "print('-------- NLG --------')\n",
    "\n",
    "for k,v in test_.items():\n",
    "  l = ''.join([f'\\t{sm} >> {v[\"start_tokens\"]}{g}\\n' for sm,g in zip(v['sampling_method'],v['generation'])])\n",
    "  print(f'{k}:')\n",
    "  print(l)"
   ],
   "metadata": {
    "id": "bvla30-lVw8n",
    "ExecuteTime": {
     "start_time": "2023-04-18T18:49:17.090722Z",
     "end_time": "2023-04-18T18:49:17.149541Z"
    },
    "pycharm": {
     "is_executing": true
    }
   },
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "is_executing": true
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEtckSWNANqW"
   },
   "source": [
    "<br><br><br>\n",
    "# **Good luck!**"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-18T18:49:17.107027Z",
     "end_time": "2023-04-18T18:49:17.154031Z"
    },
    "pycharm": {
     "is_executing": true
    }
   }
  }
 ]
}
