{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce5pQK3bFn_"
   },
   "source": [
    "# Assignment 1\n",
    "In this assignment you will be creating tools for learning and testing language models.\n",
    "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
    "\n",
    "Do make sure all results are uploaded to CSVs (as well as printed to console) for your assignment to be fully graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwG8v-Ll49KM"
   },
   "source": [
    "*As a preparation for this task, download the data files from the course git repository.\n",
    "\n",
    "The relevant files are under **lm-languages-data-new**:\n",
    "\n",
    "\n",
    "*   en.csv (or the equivalent JSON file)\n",
    "*   es.csv (or the equivalent JSON file)\n",
    "*   fr.csv (or the equivalent JSON file)\n",
    "*   in.csv (or the equivalent JSON file)\n",
    "*   it.csv (or the equivalent JSON file)\n",
    "*   nl.csv (or the equivalent JSON file)\n",
    "*   pt.csv (or the equivalent JSON file)\n",
    "*   tl.csv (or the equivalent JSON file)\n",
    "*   test.csv (or the equivalent JSON file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "outputs": [],
   "source": [
    "# !pip install numpy pandas emoji\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:13.137440Z",
     "end_time": "2023-04-25T12:48:13.321142Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xC-87z2GWMq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b7f40be-bf9b-4d6c-da81-25d60d710a75",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:13.156309Z",
     "end_time": "2023-04-25T12:48:13.430256Z"
    }
   },
   "source": [
    "#!git clone https://github.com/kfirbar/nlp-course.git"
   ],
   "execution_count": 147,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOVb4IhsqimJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYdhPfbAGkip",
    "outputId": "af6566c6-e6e6-409a-c569-8fab9bdf400e",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:13.169656Z",
     "end_time": "2023-04-25T12:48:13.461679Z"
    }
   },
   "source": [
    "\n",
    "#!ls nlp-course/lm-languages-data-new\n"
   ],
   "execution_count": 148,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "outputs": [],
   "source": [
    "path = \"nlp-course/lm-languages-data-new\"\n",
    "data_files = [\"en.csv\", \"es.csv\", \"fr.csv\", \"in.csv\", \"it.csv\", \"nl.csv\", \"pt.csv\", \"tl.csv\"]\n",
    "test_file = \"test.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:13.183676Z",
     "end_time": "2023-04-25T12:48:13.486154Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ashyu_mT28o6"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def tweet_to_token_tuples(tweet, start_token=\"<start>\", end_token=\"<end>\"):\n",
    "    \"\"\" Converts a tweet to a list of tokens, where each token is an emoji, a character, or a special start/end token.\n",
    "        some emojis are represented by more than one character (e.g. â¡ï¸)\n",
    "    Args:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "    Returns:\n",
    "        token_list: a list of tokens (emojis or characters) (in the order they appear in the tweet)\n",
    "    \"\"\"\n",
    "    token_tuple = []\n",
    "\n",
    "    # remove the start and end tokens from the tweet (if they exist) and return the tweet and the start and end tokens\n",
    "    tweet, start_token, end_token = get_start_end_tokens_and_remove_from_tweet(tweet, start_token, end_token)\n",
    "\n",
    "    # get the start and end index of all the emojis in the tweet\n",
    "    emojis_info = emoji.emoji_list(tweet) # a list of dictionaries, each dictionary contains the start and end index of an emoji in the tweet\n",
    "    emojis_info = {info[\"match_start\"]: info for info in emojis_info}\n",
    "\n",
    "    # add the start token to the token list\n",
    "    if start_token is not None:\n",
    "        token_tuple.append(start_token)\n",
    "\n",
    "    # iterate over all the characters in the tweet\n",
    "    char_index = 0\n",
    "    while char_index < len(tweet):\n",
    "        # if the current character is the start of an emoji, add the emoji to the token list and move the char_index to the end of the emoji\n",
    "        if char_index in emojis_info:\n",
    "            token_tuple.append(emojis_info[char_index][\"emoji\"])\n",
    "            char_index = emojis_info[char_index][\"match_end\"]\n",
    "        else:\n",
    "            token_tuple.append(tweet[char_index])\n",
    "            char_index += 1\n",
    "\n",
    "    # add the end token to the token list\n",
    "    if end_token is not None:\n",
    "        token_tuple.append(end_token)\n",
    "\n",
    "    # convert the token list to a tuple (so it will be hashable)\n",
    "    token_tuple = tuple(token_tuple)\n",
    "    return token_tuple\n",
    "\n",
    "def get_start_end_tokens_and_remove_from_tweet(tweet, start_token, end_token):\n",
    "    \"\"\" Removes the start and end tokens from the tweet (if they exist) and returns the tweet and the start and end tokens\n",
    "    Args:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    Returns:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    \"\"\"\n",
    "    if tweet.startswith(start_token):\n",
    "        tweet = tweet[len(start_token):]\n",
    "        #print(tweet)\n",
    "    else:\n",
    "        start_token = None\n",
    "\n",
    "    if tweet.endswith(end_token):\n",
    "        tweet = tweet[:-len(end_token)]\n",
    "        #print(tweet)\n",
    "    else:\n",
    "        end_token = None\n",
    "\n",
    "    return tweet, start_token, end_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:13.202139Z",
     "end_time": "2023-04-25T12:48:13.495667Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start>', '1', ':', ' ', 'â¡ï¸', '.', ' ', '2', ':', ' ', 'ğŸ¤£', 'â¤ï¸', '.', ' ', '3', ':', ' ', 'ğŸ¤£', 'â¤ï¸', 'â¤ï¸', '.', '<end>')\n"
     ]
    }
   ],
   "source": [
    "test_string = \"<start>1: â¡ï¸. 2: ğŸ¤£â¤ï¸. 3: ğŸ¤£â¤ï¸â¤ï¸.<end>\"\n",
    "print(tweet_to_token_tuples(test_string))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:13.210888Z",
     "end_time": "2023-04-25T12:48:13.495667Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xCfzsITW8Yaj",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:13.226904Z",
     "end_time": "2023-04-25T12:48:13.518687Z"
    }
   },
   "source": [
    "# a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data.\n",
    "# the data in the files are in the form: tweet_id,tweet_text\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" Creates a vocabulary from the data files\n",
    "    Returns:\n",
    "        vocabulary: a list of all the characters that appear in the data files\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    # iterate over all the data files\n",
    "    for file in data_files:\n",
    "        # read the data file\n",
    "        current_data = pd.read_csv(path + \"/\" + file, encoding=\"utf-8\")\n",
    "        # iterate over all the tweets in the data file\n",
    "        for tweet in current_data[\"tweet_text\"]:\n",
    "            # convert the tweet to a list of tokens\n",
    "            tweet = tweet_to_token_tuples(tweet)\n",
    "            # iterate over all the tokens in the tweet\n",
    "            for token in tweet:\n",
    "                # add the token to the vocabulary\n",
    "                vocabulary.add(token)\n",
    "    # sort the vocabulary\n",
    "    vocabulary = sorted(list(vocabulary))\n",
    "    return vocabulary"
   ],
   "execution_count": 152,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['<start>', '<end>', '\\n', '\\r', ' ', '!', '\"', '#', '#ï¸âƒ£', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '0âƒ£', '0ï¸âƒ£', '1', '1âƒ£', '1ï¸âƒ£', '2', '2âƒ£', '3', '3âƒ£', '3ï¸âƒ£', '4', '4âƒ£', '4ï¸âƒ£', '5', '6', '6ï¸âƒ£', '7', '7ï¸âƒ£', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x9d', 'Â¡', 'Â£', 'Â¤', 'Â¥', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', '\\xad', 'Â®', 'Â®ï¸', 'Â¯', 'Â°', 'Â²', 'Â´', 'Â¶', 'Â·', 'Â¸', 'Âº', 'Â»', 'Â½', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã…', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'Ã', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã™', 'Ãš', 'Ãœ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ä—', 'Ä', 'ÄŸ', 'Ä°', 'Ä±', 'Å„', 'Å', 'Å’', 'Å“', 'Å', 'ÅŸ', 'Å ', 'Å¸', 'Æ’', 'Ê”', 'Ê•', 'Ê–', 'Ê°', 'Ê³', 'Ê·', 'Ê¸', 'Ë–', 'Ë˜', 'Ëš', 'Ë›', 'Ë¡', 'Ë¢', 'Ì€', 'Ì', 'Ìƒ', 'Ìˆ', 'Ì¥', 'Ì®', 'Ì¯', 'Íœ', 'Í¡', 'Î”', 'Î˜', 'Î©', 'Ï…', 'Ï‰', 'Ğ', 'Ğ˜', 'Ğœ', 'Ğ', 'Ğ', 'ĞŸ', 'Ğ ', 'Ğ¤', 'Ğ¦', 'Ğ¯', 'Ğ°', 'Ğ±', 'Ğ²', 'Ğ³', 'Ğ´', 'Ğµ', 'Ğ·', 'Ğ¸', 'Ğº', 'Ğ»', 'Ğ¼', 'Ğ½', 'Ğ¾', 'Ğ¿', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ…', 'Ñ‹', 'Ñ', 'Ñ', 'Ñ', 'Ò’', 'Ò¯', 'ØŒ', 'Ø¢', 'Ø¦', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Ø¶', 'Ø·', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'ÛŒ', 'Û¶', 'à¤‚', 'à¤•', 'à¤—', 'à¤ª', 'à¤¬', 'à¤°', 'à¤¸', 'à¤¾', 'à¥‡', 'à¥', 'à·´', 'à¸', 'à¸‚', 'à¸‡', 'à¸ˆ', 'à¸', 'à¸”', 'à¸•', 'à¸–', 'à¸—', 'à¸™', 'à¸š', 'à¸›', 'à¸', 'à¸ ', 'à¸¡', 'à¸¢', 'à¸£', 'à¸¥', 'à¸§', 'à¸¨', 'à¸ª', 'à¸­', 'à¸°', 'à¸±', 'à¸²', 'à¸´', 'à¸µ', 'à¸¸', 'à¸¹', 'à¹€', 'à¹', 'à¹ˆ', 'à¹‰', 'à¹', 'à¹‘', 'àº¶', 'à¼', 'à¼º', 'à¼»', 'à¼¼', 'à¼½', 'á™“', 'á´—', 'á´¬', 'á´°', 'áµƒ', 'áµ‡', 'áµˆ', 'áµ‰', 'áµ', 'áµ', 'áµ’', 'áµ–', 'áµ—', 'áµ˜', 'áµ›', 'á¶œ', 'á¶ ', 'á¶¦', 'á¶°', '\\u2009', '\\u200a', '\\u200b', 'â€“', 'â€”', 'â€•', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€', 'â€ ', 'â€¢', 'â€¦', 'â€°', 'â€²', 'â€¹', 'â€º', 'â€»', 'â€¼', 'â€¼ï¸', 'â€¿', 'â‰', 'â‰ï¸', '\\u2066', '\\u2067', '\\u2069', 'â±', 'â·', 'â¿', 'â‚¬', 'â‚¹', 'â„ƒ', 'â„…', 'â„¢', 'â†', 'â†‘', 'â†’', 'â†“', 'â†”ï¸', 'â†•', 'â†—ï¸', 'â†˜ï¸', 'â†š', 'â†›', 'â†©', 'â†ª', 'â†¯', 'â†º', 'â‡˜', 'â‡¨', 'âˆ€', 'âˆ†', 'âˆ‡', 'âˆš', 'âˆ', 'âˆ´', 'âˆµ', 'â‰¤', 'â‰¥', 'â‰¦', 'â‰§', 'âŠ™', 'â‹…', 'â‹ª', 'â‹­', 'âŒš', 'âŒ›', 'âŒ£', 'â‹', 'â©', 'â°', 'â±', 'â³', 'â¸', 'â‘ ', 'â‘¥', 'â’', 'â’', 'â’', 'â’', 'â’‘', 'â“‚', 'â“‚ï¸', 'â“˜', 'â“™', 'â“¢', 'â“¦', 'â”€', 'â”', 'â”ƒ', 'â”„', 'â”†', 'â”', 'â”“', 'â”—', 'â”›', 'â”³', 'â”»', 'â•‘', 'â•”', 'â•—', 'â•š', 'â•', 'â•¦', 'â•©', 'â•¬', 'â•­', 'â•®', 'â•¯', 'â•°', 'â•±', 'â•²', 'â•´', 'â–ˆ', 'â–Š', 'â–', 'â–’', 'â–”', 'â–•', 'â–™', 'â–', 'â–£', 'â–¦', 'â–ª', 'â–ªï¸', 'â–²', 'â–³', 'â–¶', 'â–¶ï¸', 'â–¸', 'â–º', 'â–¼', 'â–½', 'â–¿', 'â—€', 'â—€ï¸', 'â—„', 'â—†', 'â—‡', 'â—ˆ', 'â—', 'â—', 'â—‘', 'â—•', 'â—¡', 'â—»ï¸', 'â—¼ï¸', 'â—½', 'â—¾', 'â˜€', 'â˜€ï¸', 'â˜', 'â˜ï¸', 'â˜ƒ', 'â˜„ï¸', 'â˜…', 'â˜†', 'â˜‰', 'â˜', 'â˜ï¸', 'â˜‘', 'â˜‘ï¸', 'â˜“', 'â˜”', 'â˜•', 'â˜˜', 'â˜˜ï¸', 'â˜™', 'â˜š', 'â˜›', 'â˜œ', 'â˜', 'â˜ï¸', 'â˜ğŸ½', 'â˜ğŸ¾', 'â˜', 'â˜ ï¸', 'â˜£', 'â˜ª', 'â˜®ï¸', 'â˜¯', 'â˜°', 'â˜¹', 'â˜¹ï¸', 'â˜º', 'â˜ºï¸', 'â˜¼', 'â˜½', 'â˜¾', 'â™€', 'â™‚', 'â™‹', 'â™', 'â™', 'â™', 'â™', 'â™“', 'â™›', 'â™¡', 'â™£', 'â™£ï¸', 'â™¤', 'â™¥', 'â™¥ï¸', 'â™¦', 'â™¦ï¸', 'â™©', 'â™ª', 'â™«', 'â™¬', 'â™¯', 'â™»', 'âš’', 'âš“', 'âš”', 'âš”ï¸', 'âš–ï¸', 'âš˜', 'âšœ', 'âšœï¸', 'âš', 'âš ', 'âš ï¸', 'âš¡', 'âšª', 'âš«', 'âš°', 'âš½', 'âš¾', 'â›„', 'â›…', 'â›ˆ', 'â›“', 'â›”', 'â›©', 'â›ª', 'â›³', 'â›·', 'â›½', 'âœ', 'âœ‚', 'âœ‚ï¸', 'âœƒ', 'âœ…', 'âœˆ', 'âœˆï¸', 'âœ‰', 'âœ‰ï¸', 'âœŠ', 'âœŠğŸ»', 'âœŠğŸ¼', 'âœŠğŸ½', 'âœŠğŸ¾', 'âœŠğŸ¿', 'âœ‹', 'âœ‹ğŸ»', 'âœ‹ğŸ¼', 'âœŒ', 'âœŒï¸', 'âœŒğŸ»', 'âœŒğŸ¼', 'âœŒğŸ½', 'âœŒğŸ¾', 'âœ', 'âœğŸ¼', 'âœğŸ½', 'âœ', 'âœï¸', 'âœ“', 'âœ”', 'âœ”ï¸', 'âœ–', 'âœï¸', 'âœ¡', 'âœ¡ï¸', 'âœ§', 'âœ¨', 'âœ©', 'âœ­', 'âœ°', 'âœ³', 'âœ³ï¸', 'âœ´', 'âœµ', 'âœ¶', 'âœ·', 'âœ¿', 'â€', 'â', 'â„', 'â„ï¸', 'â…', 'âˆ', 'â‹', 'âŒ', 'â', 'â“', 'â”', 'â—', 'â', 'â', 'â£', 'â£ï¸', 'â¤', 'â¤ï¸', 'â¥', 'âŠ', 'â‹', 'âŒ', 'â', 'â', 'â', 'â”', 'â–', 'â—', 'â™', 'â›', 'âœ', 'â', 'âŸ', 'â ', 'â¡', 'â¡ï¸', 'â¢', 'â¤', 'â°', 'â €', 'â¤µ', 'â¤µï¸', 'â¦‘', 'â¦’', 'â¬…', 'â¬…ï¸', 'â¬‡', 'â¬‡ï¸', 'â­', 'â¸„', 'â¸…', '\\u3000', 'ã€', 'ã€‚', 'ã€†', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€', 'ã€', 'ã€', 'ã€', 'ã€‘', 'ã€œ', 'ã€¡', 'ã€°', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ãŒ', 'ã', 'ã', 'ã—', 'ã›', 'ãœ', 'ãŸ', 'ã£', 'ã¥', 'ã¦', 'ã§', 'ã¨', 'ãª', 'ã«', 'ã­', 'ã®', 'ã¯', 'ã²', 'ã¿', 'ã‚€', 'ã‚‡', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚Œ', 'ã‚', 'ã‚’', 'ã‚œ', 'ã‚¤', 'ã‚§', 'ã‚¨', 'ã‚©', 'ã‚ª', 'ã‚«', 'ã‚­', 'ã‚¯', 'ã‚°', 'ã‚³', 'ã‚´', 'ã‚µ', 'ã‚¸', 'ã‚¹', 'ã‚»', 'ã‚¿', 'ãƒ', 'ãƒƒ', 'ãƒ„', 'ãƒ†', 'ãƒˆ', 'ãƒ‰', 'ãƒŠ', 'ãƒ‹', 'ãƒ', 'ãƒ', 'ãƒ‘', 'ãƒ’', 'ãƒ“', 'ãƒ”', 'ãƒ•', 'ãƒ–', 'ãƒ—', 'ãƒ', 'ãƒ ', 'ãƒ¡', 'ãƒ¥', 'ãƒ¦', 'ãƒ§', 'ãƒ©', 'ãƒ¬', 'ãƒ­', 'ãƒ®', 'ãƒ¯', 'ãƒ³', 'ãƒ»', 'ãƒ¼', 'ãƒ½', 'ã……', 'ã…ˆ', 'ã…‹', 'ã…', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£', 'ã…¤', '\\u31ef', 'ä¸–', 'ä¸­', 'ä¸»', 'äº’', 'äºº', 'ä»˜', 'åƒ', 'å„¿', 'å…', 'å…ˆ', 'å…¥', 'å†™', 'åˆ†', 'åˆ©', 'åˆ¶', 'åˆ¹', 'åŠ›', 'åŠª', 'å‹•', 'åˆ', 'å’', 'å—', 'åˆ', 'å‘Ÿ', 'å˜‰', 'å¢—', 'å¥½', 'å§¿', 'å«Œ', 'å­¦', 'å°”', 'å¸Œ', 'å½¡', 'å½±', 'å½¼', 'å¾Œ', 'æ‚ª', 'æ‰‹', 'æŠ•', 'æ‹¶', 'æŒ¨', 'æ’ƒ', 'æ’®', 'æ–‡', 'æ˜ ', 'æ™‚', 'æœˆ', 'æœ', 'æœ¬', 'æ—', 'æŸ±', 'æ¥­', 'æ©Ÿ', 'æ­Œ', 'æ­³', 'æ¯…', 'æ°—', 'æ´²', 'æ´¸', 'ç‹', 'ç”Ÿ', 'ç”¨', 'ç”»', 'ç•Œ', 'ç›¸', 'çœŸ', 'ç¬', 'çŸ¥', 'ç¨¿', 'ç©º', 'ç³Ÿ', 'çµ‚', 'çµ', 'ç¹‹', 'è€…', 'èŠ±', 'èœ', 'è¡Œ', 'è®¸', 'èµ«', 'è¸Š', 'è¾¼', 'é€š', 'é‚£', 'é–“', 'é¢¨', 'é­', 'ê ', 'ê°€', 'ê°“', 'ê°•', 'ê±¸', 'ê²€', 'ê²Œ', 'ê²©', 'ê²°', 'ê²½', 'ê³ ', 'ê³¡', 'ê³¼', 'êµ¬', 'êµ­', 'ê·œ', 'ê·¸', 'ê·¼', 'ê¸ˆ', 'ê¸°', 'ê¹€', 'ê¼¼', 'ë‚˜', 'ë‚¨', 'ë‚´', 'ë„ˆ', 'ë„', 'ë„¤', 'ë„·', 'ë…€', 'ë…„', 'ë…¸', 'ë…¼', 'ëˆ„', 'ëŠ”', 'ëŠ˜', 'ë‹ˆ', 'ë‹¤', 'ë‹¨', 'ë‹¹', 'ëŒ€', 'ë”', 'ë„', 'ë™', 'ë‘', 'ë‘‘', 'ë“€', 'ë“œ', 'ë“±', 'ë””', 'ë¼', 'ë½', 'ë‘', 'ë™', 'ëœ', 'ë¨', 'ëŸ¬', 'ëŸ°', 'ë ˆ', 'ë ›', 'ë¡œ', 'ë¡±', 'ë£Œ', 'ë£°', 'ë£¸', 'ë¦„', 'ë¦‰', 'ë¦¬', 'ë¦¼', 'ë§', 'ë§ˆ', 'ë§', 'ë§¤', 'ë§¨', 'ëª¬', 'ë¬´', 'ë¯¸', 'ë¯¼', 'ë°€', 'ë°”', 'ë°•', 'ë°©', 'ë°±', 'ë±€', 'ë²„', 'ë²…', 'ë²•', 'ë² ', 'ë²¨', 'ë²³', 'ë³´', 'ë³µ', 'ë³¸', 'ë´‰', 'ë·”', 'ë¸Œ', 'ë¸', 'ë¸”', 'ë¹„', 'ë¹…', 'ë¹¼', 'ì‚¬', 'ì‚´', 'ì‚¼', 'ìƒ', 'ìƒ', 'ìƒ¤', 'ìƒµ', 'ì„œ', 'ì„', 'ì„ ', 'ì„±', 'ì„¸', 'ì„¹', 'ì…”', 'ì…˜', 'ì…©', 'ì†Œ', 'ì†¡', 'ìˆ˜', 'ìŠˆ', 'ìŠ¤', 'ìŠ¨', 'ìŠ¬', 'ìŠ¹', 'ì‹œ', 'ì‹', 'ì‹ ', 'ì‹¤', 'ì‹¸', 'ì•„', 'ì•ˆ', 'ì••', 'ì• ', 'ì•¼', 'ì–‘', 'ì–´', 'ì—', 'ì—‘', 'ì—˜', 'ì— ', 'ì—£', 'ì—¬', 'ì—­', 'ì—°', 'ì˜', 'ì˜ˆ', 'ì˜¤', 'ì˜¨', 'ì™€', 'ì™•', 'ì™¸', 'ìš”', 'ìš©', 'ìš°', 'ìš¸', 'ì›Œ', 'ì›', 'ìœ„', 'ìœ ', 'ìœ¤', 'ì˜', 'ì´', 'ì¸', 'ì¼', 'ì„', 'ì˜', 'ì¥', 'ì¬', 'ì­', 'ì „', 'ì •', 'ì œ', 'ì ¤', 'ì¢…', 'ì£¼', 'ì¥”', 'ì¦ˆ', 'ì§€', 'ì§', 'ì§„', 'ì§‘', 'ì©œ', 'ì°Œ', 'ì°°', 'ì±„', 'ì²œ', 'ì² ', 'ì´ˆ', 'ìµœ', 'ì¶”', 'ì¶œ', 'ì¸ ', 'ì¹˜', 'ì¹´', 'ì»¤', 'ì½”', 'ì½˜', 'ì½¤', 'ì¿±', 'í¬', 'í‚¤', 'í‚¹', 'íƒ€', 'íƒ„', 'íƒ‘', 'íƒœ', 'í„°', 'í…', 'í† ', 'í†¡', 'íŠ¸', 'í‹´', 'íŒ', 'íŒŒ', 'íŒ¨', 'í€', 'í¬', 'í’€', 'í”„', 'í”Œ', 'í”¼', 'í•‘', 'í•˜', 'í•œ', 'í•´', 'í•¸', 'í—Œ', 'í—¤', 'í—¨', 'í˜', 'í˜„', 'í˜•', 'í˜¸', 'í™”', 'í™˜', 'í›ˆ', 'í', 'ï·»', 'ï¸', 'ï¸', 'ï¸µ', 'ï¹', 'ï¹ª', 'ï¼', 'ï¼‚', 'ï¼ƒ', 'ï¼ˆ', 'ï¼‰', 'ï¼Š', 'ï¼', 'ï¼“', 'ï¼–', 'ï¼—', 'ï¼˜', 'ï¼Ÿ', 'ï¼ ', 'ï¼¡', 'ï¼¢', 'ï¼£', 'ï¼¤', 'ï¼¥', 'ï¼¦', 'ï¼§', 'ï¼¨', 'ï¼©', 'ï¼«', 'ï¼¬', 'ï¼­', 'ï¼®', 'ï¼¯', 'ï¼°', 'ï¼±', 'ï¼²', 'ï¼³', 'ï¼´', 'ï¼µ', 'ï¼¶', 'ï¼·', 'ï¼¹', 'ï¼»', 'ï¼½', 'ï¼¿', 'ï½€', 'ï½', 'ï½‚', 'ï½ƒ', 'ï½„', 'ï½…', 'ï½‡', 'ï½‰', 'ï½Œ', 'ï½', 'ï½', 'ï½', 'ï½', 'ï½‘', 'ï½’', 'ï½“', 'ï½•', 'ï½–', 'ï½—', 'ï½˜', 'ï½™', 'ï½š', 'ï½œ', 'ï½¡', 'ï½¥', 'ï¾‰', 'ï¿£', 'ï¿¼', 'ï¿½', 'ğŸƒ', 'ğŸ…°', 'ğŸ…±', 'ğŸ…±ï¸', 'ğŸ…¾', 'ğŸ…¾ï¸', 'ğŸ…¿', 'ğŸ†‘', 'ğŸ†’', 'ğŸ†“', 'ğŸ†”', 'ğŸ†•', 'ğŸ†–', 'ğŸ†—', 'ğŸ†˜', 'ğŸ†™', 'ğŸ†š', 'ğŸ‡¦ğŸ‡©', 'ğŸ‡¦ğŸ‡±', 'ğŸ‡¦ğŸ‡·', 'ğŸ‡¦ğŸ‡¹', 'ğŸ‡§ğŸ‡ª', 'ğŸ‡§ğŸ‡´', 'ğŸ‡§ğŸ‡·', 'ğŸ‡§ğŸ‡¸', 'ğŸ‡¨ğŸ‡¦', 'ğŸ‡¨ğŸ‡©', 'ğŸ‡¨ğŸ‡®', 'ğŸ‡¨ğŸ‡±', 'ğŸ‡¨ğŸ‡³', 'ğŸ‡¨ğŸ‡´', 'ğŸ‡¨ğŸ‡º', 'ğŸ‡©ğŸ‡ª', 'ğŸ‡©ğŸ‡´', 'ğŸ‡©ğŸ‡¿', 'ğŸ‡ªğŸ‡­', 'ğŸ‡ªğŸ‡¸', 'ğŸ‡ªğŸ‡º', 'ğŸ‡«', 'ğŸ‡«ğŸ‡·', 'ğŸ‡¬ğŸ‡§', 'ğŸ‡¬ğŸ‡·', 'ğŸ‡­ğŸ‡°', 'ğŸ‡®ğŸ‡ª', 'ğŸ‡®ğŸ‡¹', 'ğŸ‡¯ğŸ‡µ', 'ğŸ‡°ğŸ‡µ', 'ğŸ‡°ğŸ‡·', 'ğŸ‡°ğŸ‡¼', 'ğŸ‡±', 'ğŸ‡±ğŸ‡¨', 'ğŸ‡±ğŸ‡º', 'ğŸ‡²ğŸ‡¦', 'ğŸ‡²ğŸ‡©', 'ğŸ‡²ğŸ‡´', 'ğŸ‡²ğŸ‡½', 'ğŸ‡³ğŸ‡¬', 'ğŸ‡³ğŸ‡±', 'ğŸ‡³ğŸ‡¿', 'ğŸ‡µğŸ‡­', 'ğŸ‡µğŸ‡°', 'ğŸ‡µğŸ‡·', 'ğŸ‡µğŸ‡¸', 'ğŸ‡µğŸ‡¹', 'ğŸ‡·ğŸ‡º', 'ğŸ‡¸ğŸ‡³', 'ğŸ‡¹ğŸ‡³', 'ğŸ‡ºğŸ‡¸', 'ğŸ‡ºğŸ‡¾', 'ğŸ‡»ğŸ‡ª', 'ğŸ‡»ğŸ‡³', 'ğŸ‡¼', 'ğŸˆµ', 'ğŸˆ¶', 'ğŸˆ·', 'ğŸŒ€', 'ğŸŒƒ', 'ğŸŒ„', 'ğŸŒ…', 'ğŸŒ†', 'ğŸŒ‡', 'ğŸŒˆ', 'ğŸŒŠ', 'ğŸŒ‹', 'ğŸŒŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ’', 'ğŸŒ“', 'ğŸŒ—', 'ğŸŒ™', 'ğŸŒš', 'ğŸŒ›', 'ğŸŒœ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒŸ', 'ğŸŒ ', 'ğŸŒ¤', 'ğŸŒ¥', 'ğŸŒ§', 'ğŸŒ¨', 'ğŸŒªï¸', 'ğŸŒ«', 'ğŸŒ¬', 'ğŸŒ®', 'ğŸŒ¯', 'ğŸŒ°', 'ğŸŒ±', 'ğŸŒ²', 'ğŸŒ³', 'ğŸŒ´', 'ğŸŒµ', 'ğŸŒ¶', 'ğŸŒ¶ï¸', 'ğŸŒ·', 'ğŸŒ¸', 'ğŸŒ¹', 'ğŸŒº', 'ğŸŒ»', 'ğŸŒ¼', 'ğŸŒ½', 'ğŸŒ¾', 'ğŸŒ¿', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸƒ', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹', 'ğŸŒ', 'ğŸ', 'ğŸ', 'ğŸ', 'ğŸ‘', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ•', 'ğŸ–', 'ğŸ—', 'ğŸš', 'ğŸ›', 'ğŸœ', 'ğŸ', 'ğŸ', 'ğŸŸ', 'ğŸ£', 'ğŸ¤', 'ğŸ¥', 'ğŸ¦', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ³', 'ğŸ´', 'ğŸµ', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹', 'ğŸ', 'ğŸ’', 'ğŸ“', 'ğŸ—', 'ğŸ™', 'ğŸ™ï¸', 'ğŸ', 'ğŸŸ', 'ğŸŸï¸', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ¤', 'ğŸ¥', 'ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ®', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ²', 'ğŸµ', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ¾', 'ğŸ¿', 'ğŸ€', 'ğŸ', 'ğŸƒ', 'ğŸƒğŸ¼', 'ğŸ„', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸŠğŸ»', 'ğŸŠğŸ»\\u200dâ™€ï¸', 'ğŸ‹ï¸', 'ğŸŒ', 'ğŸŒğŸ¾', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ–', 'ğŸ˜', 'ğŸ™', 'ğŸš', 'ğŸŸ', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ«', 'ğŸ°', 'ğŸ³\\u200dğŸŒˆ', 'ğŸ³ï¸\\u200dğŸŒˆ', 'ğŸ´\\u200dâ˜ ï¸', 'ğŸ¹', 'ğŸ»', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸ„', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ', 'ğŸ', 'ğŸ', 'ğŸ‘', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ•', 'ğŸ–', 'ğŸ˜', 'ğŸ™', 'ğŸš', 'ğŸœ', 'ğŸ', 'ğŸ', 'ğŸŸ', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ£', 'ğŸ¥', 'ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ®', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ²', 'ğŸ³', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ‘€', 'ğŸ‘', 'ğŸ‘‚', 'ğŸ‘„', 'ğŸ‘…', 'ğŸ‘†', 'ğŸ‘†ğŸ»', 'ğŸ‘‡', 'ğŸ‘‡ğŸ»', 'ğŸ‘‡ğŸ¼', 'ğŸ‘‡ğŸ½', 'ğŸ‘‡ğŸ¾', 'ğŸ‘ˆ', 'ğŸ‘ˆğŸ¼', 'ğŸ‘ˆğŸ¾', 'ğŸ‘‰', 'ğŸ‘‰ğŸ»', 'ğŸ‘‰ğŸ¼', 'ğŸ‘‰ğŸ½', 'ğŸ‘Š', 'ğŸ‘ŠğŸ»', 'ğŸ‘ŠğŸ¼', 'ğŸ‘ŠğŸ½', 'ğŸ‘ŠğŸ¾', 'ğŸ‘ŠğŸ¿', 'ğŸ‘‹', 'ğŸ‘‹ğŸ»', 'ğŸ‘‹ğŸ¼', 'ğŸ‘‹ğŸ½', 'ğŸ‘Œ', 'ğŸ‘ŒğŸ»', 'ğŸ‘ŒğŸ¼', 'ğŸ‘ŒğŸ½', 'ğŸ‘ŒğŸ¾', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¾', 'ğŸ‘ğŸ¿', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¾', 'ğŸ‘', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¾', 'ğŸ‘‘', 'ğŸ‘“', 'ğŸ‘•', 'ğŸ‘–', 'ğŸ‘—', 'ğŸ‘™', 'ğŸ‘', 'ğŸ‘Ÿ', 'ğŸ‘ ', 'ğŸ‘¡', 'ğŸ‘£', 'ğŸ‘¤', 'ğŸ‘¥', 'ğŸ‘¦', 'ğŸ‘¦ğŸ»', 'ğŸ‘§', 'ğŸ‘§ğŸ»', 'ğŸ‘§ğŸ¼', 'ğŸ‘¨', 'ğŸ‘¨\\u200dâ¤ï¸\\u200dğŸ‘¨', 'ğŸ‘¨\\u200dğŸŒ¾', 'ğŸ‘¨\\u200dğŸ¤', 'ğŸ‘¨\\u200dğŸ‘©\\u200dğŸ‘§\\u200dğŸ‘¦', 'ğŸ‘¨\\u200dğŸ’»', 'ğŸ‘¨ğŸ»', 'ğŸ‘¨ğŸ»\\u200dâš•ï¸', 'ğŸ‘¨ğŸ»\\u200dğŸ’»', 'ğŸ‘©', 'ğŸ‘©\\u200dâ¤ï¸\\u200dğŸ‘©', 'ğŸ‘©\\u200dğŸŒ¾', 'ğŸ‘©\\u200dğŸ‘§\\u200dğŸ‘§', 'ğŸ‘©ğŸ»\\u200dğŸ¤', 'ğŸ‘©ğŸ¼\\u200dğŸ¨', 'ğŸ‘©ğŸ¼\\u200dğŸ’»', 'ğŸ‘ª', 'ğŸ‘«', 'ğŸ‘­', 'ğŸ‘®', 'ğŸ‘®ğŸ»', 'ğŸ‘¯', 'ğŸ‘°', 'ğŸ‘°ğŸ½', 'ğŸ‘±', 'ğŸ‘±\\u200dâ™€ï¸', 'ğŸ‘±ğŸ»\\u200dâ™€ï¸', 'ğŸ‘²', 'ğŸ‘³', 'ğŸ‘³ğŸ¼', 'ğŸ‘µ', 'ğŸ‘µğŸ¼', 'ğŸ‘¶', 'ğŸ‘¶ğŸ»', 'ğŸ‘¶ğŸ¼', 'ğŸ‘¶ğŸ½', 'ğŸ‘·', 'ğŸ‘¸', 'ğŸ‘¸ğŸ»', 'ğŸ‘¸ğŸ¼', 'ğŸ‘¸ğŸ¾', 'ğŸ‘¹', 'ğŸ‘º', 'ğŸ‘»', 'ğŸ‘¼', 'ğŸ‘¼ğŸ»', 'ğŸ‘¼ğŸ¼', 'ğŸ‘½', 'ğŸ‘¿', 'ğŸ’€', 'ğŸ’', 'ğŸ’ğŸ»', 'ğŸ’ğŸ»\\u200dâ™‚ï¸', 'ğŸ’ğŸ¼', 'ğŸ’ğŸ½', 'ğŸ’ğŸ¾', 'ğŸ’ğŸ¿', 'ğŸ’‚', 'ğŸ’‚ğŸ»', 'ğŸ’‚ğŸ¿', 'ğŸ’ƒ', 'ğŸ’ƒğŸ»', 'ğŸ’ƒğŸ¼', 'ğŸ’ƒğŸ½', 'ğŸ’ƒğŸ¾', 'ğŸ’„', 'ğŸ’…', 'ğŸ’…ğŸ»', 'ğŸ’…ğŸ¼', 'ğŸ’…ğŸ½', 'ğŸ’†', 'ğŸ’†ğŸ»', 'ğŸ’†ğŸ½', 'ğŸ’†ğŸ¾', 'ğŸ’ˆ', 'ğŸ’‰', 'ğŸ’Š', 'ğŸ’‹', 'ğŸ’Œ', 'ğŸ’', 'ğŸ’', 'ğŸ’', 'ğŸ’', 'ğŸ’‘', 'ğŸ’“', 'ğŸ’”', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’˜', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ’œ', 'ğŸ’', 'ğŸ’', 'ğŸ’Ÿ', 'ğŸ’¡', 'ğŸ’¢', 'ğŸ’£', 'ğŸ’¤', 'ğŸ’¥', 'ğŸ’¦', 'ğŸ’§', 'ğŸ’¨', 'ğŸ’©', 'ğŸ’ª', 'ğŸ’ªğŸ»', 'ğŸ’ªğŸ¼', 'ğŸ’ªğŸ½', 'ğŸ’ªğŸ¾', 'ğŸ’«', 'ğŸ’¬', 'ğŸ’­', 'ğŸ’®', 'ğŸ’¯', 'ğŸ’°', 'ğŸ’³', 'ğŸ’µ', 'ğŸ’¶', 'ğŸ’¸', 'ğŸ’»', 'ğŸ’¼', 'ğŸ’½', 'ğŸ’¿', 'ğŸ“€', 'ğŸ“‚', 'ğŸ“…', 'ğŸ“†', 'ğŸ“ˆ', 'ğŸ“Š', 'ğŸ“‹', 'ğŸ“Œ', 'ğŸ“', 'ğŸ“', 'ğŸ““', 'ğŸ“–', 'ğŸ“š', 'ğŸ“›', 'ğŸ“', 'ğŸ“', 'ğŸ“¡', 'ğŸ“¢', 'ğŸ“£', 'ğŸ“¦', 'ğŸ“§', 'ğŸ“©', 'ğŸ“¬', 'ğŸ“¯', 'ğŸ“°', 'ğŸ“±', 'ğŸ“²', 'ğŸ“´', 'ğŸ“·', 'ğŸ“¸', 'ğŸ“¹', 'ğŸ“º', 'ğŸ“»', 'ğŸ“¼', 'ğŸ“½', 'ğŸ“½ï¸', 'ğŸ“¿', 'ğŸ”', 'ğŸ”‚', 'ğŸ”ƒ', 'ğŸ”„', 'ğŸ”…', 'ğŸ”‰', 'ğŸ”Š', 'ğŸ”‹', 'ğŸ”Œ', 'ğŸ”', 'ğŸ”‘', 'ğŸ”’', 'ğŸ”“', 'ğŸ””', 'ğŸ”˜', 'ğŸ”™', 'ğŸ”›', 'ğŸ”œ', 'ğŸ”', 'ğŸ”', 'ğŸ”¥', 'ğŸ”¨', 'ğŸ”©', 'ğŸ”ª', 'ğŸ”«', 'ğŸ”®', 'ğŸ”°', 'ğŸ”±', 'ğŸ”²', 'ğŸ”´', 'ğŸ”µ', 'ğŸ”¶', 'ğŸ”¸', 'ğŸ”¹', 'ğŸ”º', 'ğŸ”»', 'ğŸ”¼', 'ğŸ”½', 'ğŸ•Š', 'ğŸ•Œ', 'ğŸ•', 'ğŸ•’', 'ğŸ•˜', 'ğŸ•›', 'ğŸ•œ', 'ğŸ•Ÿ', 'ğŸ•¤', 'ğŸ•ª', 'ğŸ•¯', 'ğŸ•µ', 'ğŸ•¶', 'ğŸ•·ï¸', 'ğŸ•º', 'ğŸ•ºğŸ»', 'ğŸ•ºğŸ½', 'ğŸ–', 'ğŸ–ï¸', 'ğŸ–ğŸ¼', 'ğŸ–ğŸ½', 'ğŸ–’', 'ğŸ–•', 'ğŸ–•ğŸ»', 'ğŸ–•ğŸ¼', 'ğŸ–•ğŸ½', 'ğŸ–•ğŸ¾', 'ğŸ–•ğŸ¿', 'ğŸ––', 'ğŸ––ğŸ¾', 'ğŸ–¤', 'ğŸ–¥ï¸', 'ğŸ–¼', 'ğŸ—‚', 'ğŸ—“', 'ğŸ—“ï¸', 'ğŸ—', 'ğŸ—', 'ğŸ—¡', 'ğŸ—£', 'ğŸ—£ï¸', 'ğŸ—¨ï¸', 'ğŸ—³', 'ğŸ—»', 'ğŸ—¼', 'ğŸ—½', 'ğŸ—¾', 'ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‡', 'ğŸ˜ˆ', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜Œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜’', 'ğŸ˜“', 'ğŸ˜”', 'ğŸ˜•', 'ğŸ˜–', 'ğŸ˜—', 'ğŸ˜˜', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ˜›', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜Ÿ', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ˜£', 'ğŸ˜¤', 'ğŸ˜¥', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜ª', 'ğŸ˜«', 'ğŸ˜¬', 'ğŸ˜­', 'ğŸ˜®', 'ğŸ˜¯', 'ğŸ˜°', 'ğŸ˜±', 'ğŸ˜²', 'ğŸ˜³', 'ğŸ˜´', 'ğŸ˜µ', 'ğŸ˜¶', 'ğŸ˜·', 'ğŸ˜¸', 'ğŸ˜¹', 'ğŸ˜º', 'ğŸ˜»', 'ğŸ˜¼', 'ğŸ˜½', 'ğŸ˜¿', 'ğŸ™€', 'ğŸ™', 'ğŸ™‚', 'ğŸ™ƒ', 'ğŸ™„', 'ğŸ™…', 'ğŸ™…ğŸ»', 'ğŸ™…ğŸ¼', 'ğŸ™…ğŸ½\\u200dâ™‚ï¸', 'ğŸ™†', 'ğŸ™†\\u200dâ™‚ï¸', 'ğŸ™†ğŸ½', 'ğŸ™‡', 'ğŸ™‡\\u200dâ™€ï¸', 'ğŸ™‡ğŸ»', 'ğŸ™‡ğŸ½', 'ğŸ™ˆ', 'ğŸ™‰', 'ğŸ™Š', 'ğŸ™‹', 'ğŸ™‹ğŸ»', 'ğŸ™‹ğŸ»\\u200dâ™‚ï¸', 'ğŸ™‹ğŸ¼', 'ğŸ™‹ğŸ½', 'ğŸ™Œ', 'ğŸ™ŒğŸ»', 'ğŸ™ŒğŸ¼', 'ğŸ™ŒğŸ½', 'ğŸ™ŒğŸ¾', 'ğŸ™', 'ğŸ™ğŸ»', 'ğŸ™', 'ğŸ™', 'ğŸ™ğŸ»', 'ğŸ™ğŸ¼', 'ğŸ™ğŸ½', 'ğŸ™ğŸ¾', 'ğŸ™ğŸ¿', 'ğŸš€', 'ğŸš', 'ğŸš‡', 'ğŸšˆ', 'ğŸšŒ', 'ğŸš‘', 'ğŸš“', 'ğŸš”', 'ğŸš–', 'ğŸš—', 'ğŸš˜', 'ğŸš™', 'ğŸš¢', 'ğŸš£', 'ğŸš¦', 'ğŸš§', 'ğŸš¨', 'ğŸš©', 'ğŸš«', 'ğŸš¬', 'ğŸš®', 'ğŸš²', 'ğŸš´', 'ğŸš´\\u200dâ™€ï¸', 'ğŸš´ğŸ»', 'ğŸš´ğŸ»\\u200dâ™€ï¸', 'ğŸšµ', 'ğŸšµ\\u200dâ™€ï¸', 'ğŸš¶', 'ğŸš¶\\u200dâ™‚ï¸', 'ğŸš¶ğŸ¾', 'ğŸš»', 'ğŸš¼', 'ğŸš¿', 'ğŸ›€', 'ğŸ›ƒ', 'ğŸ›„', 'ğŸ›©ï¸', 'ğŸ›«', 'ğŸ›¬', 'ğŸ›°', 'ğŸ›³', 'ğŸ›´', 'ğŸ¤', 'ğŸ¤‘', 'ğŸ¤’', 'ğŸ¤“', 'ğŸ¤”', 'ğŸ¤•', 'ğŸ¤–', 'ğŸ¤—', 'ğŸ¤˜', 'ğŸ¤˜ğŸ»', 'ğŸ¤˜ğŸ¼', 'ğŸ¤˜ğŸ½', 'ğŸ¤˜ğŸ¾', 'ğŸ¤™', 'ğŸ¤™ğŸ»', 'ğŸ¤™ğŸ¼', 'ğŸ¤™ğŸ¾', 'ğŸ¤šğŸ»', 'ğŸ¤šğŸ½', 'ğŸ¤›ğŸ½', 'ğŸ¤œğŸ¿', 'ğŸ¤', 'ğŸ¤', 'ğŸ¤ğŸ»', 'ğŸ¤ğŸ¼', 'ğŸ¤ğŸ½', 'ğŸ¤ ', 'ğŸ¤¡', 'ğŸ¤¢', 'ğŸ¤£', 'ğŸ¤¤', 'ğŸ¤¥', 'ğŸ¤¦\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ»\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ»\\u200dâ™‚ï¸', 'ğŸ¤¦ğŸ¼\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ½\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ½\\u200dâ™‚ï¸', 'ğŸ¤¦ğŸ¾\\u200dâ™‚ï¸', 'ğŸ¤§', 'ğŸ¤³ğŸ½', 'ğŸ¤´ğŸ»', 'ğŸ¤·', 'ğŸ¤·\\u200dâ™€ï¸', 'ğŸ¤·\\u200dâ™‚ï¸', 'ğŸ¤·ğŸ»\\u200dâ™€ï¸', 'ğŸ¤·ğŸ»\\u200dâ™‚ï¸', 'ğŸ¤·ğŸ¼\\u200dâ™€ï¸', 'ğŸ¤·ğŸ½\\u200dâ™€ï¸', 'ğŸ¤·ğŸ½\\u200dâ™‚ï¸', 'ğŸ¤·ğŸ¾\\u200dâ™€ï¸', 'ğŸ¤·ğŸ¾\\u200dâ™‚ï¸', 'ğŸ¥€', 'ğŸ¥', 'ğŸ¥‚', 'ğŸ¥ƒ', 'ğŸ¥„', 'ğŸ¥…', 'ğŸ¥‡', 'ğŸ¥Š', 'ğŸ¥', 'ğŸ¥’', 'ğŸ¥”', 'ğŸ¥˜', 'ğŸ¥', 'ğŸ¦€', 'ğŸ¦', 'ğŸ¦ƒ', 'ğŸ¦„', 'ğŸ¦‡', 'ğŸ¦‰', 'ğŸ¦‹', 'ğŸ¦‘', '\\U000fe4e6']\n",
      "vocab size:  2057\n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "vocabulary = preprocess()\n",
    "# add <start> and <end> to the vocabulary\n",
    "vocabulary = [\"<start>\"] + [\"<end>\"] + vocabulary\n",
    "\n",
    "print(\"vocabulary: \", vocabulary)\n",
    "print(\"vocab size: \", len(vocabulary))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:13.247925Z",
     "end_time": "2023-04-25T12:48:19.050873Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb2PGj0Yc2TY"
   },
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function `lm` that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
    "\n",
    "{\n",
    "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
    "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
    "}\n",
    "\n",
    "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
    "\n",
    "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "outputs": [],
   "source": [
    "# helper functions for lm\n",
    "\n",
    "def build_ngram_model(current_data, n):\n",
    "    \"\"\" Builds an n-gram model from the data\n",
    "    Args:\n",
    "        current_data: a pandas dataframe with a column named \"tweet_text\"\n",
    "        n: the n in n-gram\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "\n",
    "    # iterate over all the tweets in the data file\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        # convert the tweet to a tuple of tokens\n",
    "        tweet = tweet_to_token_tuples(tweet)\n",
    "\n",
    "        # define n_gram, n_minus_1_gram and n_th_token\n",
    "        n_gram = (\"<start>\", ) + tweet[0:n-1]\n",
    "        n_minus_1_gram = n_gram[:-1]\n",
    "        n_th_token = n_gram[-1]\n",
    "\n",
    "        # add the n-gram to the model\n",
    "        model = add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token)\n",
    "\n",
    "        # iterate over all the n-grams in the tweet\n",
    "        for i in range(len(tweet) - n + 1):\n",
    "            # define n_gram, n_minus_1_gram and n_th_token\n",
    "            n_gram = tweet[i:i+n]\n",
    "            n_minus_1_gram = n_gram[:-1]\n",
    "            n_th_token = n_gram[-1]\n",
    "\n",
    "            # add the n-gram to the model\n",
    "            model = add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token)\n",
    "\n",
    "        n_minus_1_gram = tweet[-n+1:]\n",
    "        n_th_token = \"<end>\"\n",
    "        # add the n-gram to the model\n",
    "        model = add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token)\n",
    "\n",
    "    return model\n",
    "\n",
    "def add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token):\n",
    "    \"\"\" Adds a n-gram to the model and counts the n_th token\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "        n_minus_1_gram: a tuple of n-1 tokens\n",
    "        n_th_token: the n_th token\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "    # add the n-gram to the model\n",
    "    if n_minus_1_gram not in model:\n",
    "        model[n_minus_1_gram] = {}\n",
    "    # add the n_th token to the model\n",
    "    if n_th_token not in model[n_minus_1_gram]:\n",
    "        model[n_minus_1_gram][n_th_token] = 0\n",
    "    # count the n_th token, i.e. add 1 to its count\n",
    "    model[n_minus_1_gram][n_th_token] += 1\n",
    "    return model\n",
    "\n",
    "\n",
    "def add_one_smoothing(model, vocabulary):\n",
    "    \"\"\" Adds add_one smoothing to the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # add spacial key <not in model>\n",
    "        model[n_minus_1_gram][\"<notInModel>\"] = 0\n",
    "        # iterate over all the tokens in the vocabulary\n",
    "        for token in vocabulary:\n",
    "            # add the token to the model\n",
    "            if token not in model[n_minus_1_gram]:\n",
    "                #model[n_minus_1_gram][token] = 0\n",
    "                # TODO: maybe try to do 1 key for all the tokens not in the model\n",
    "                model[n_minus_1_gram][\"<notInModel>\"] += 1\n",
    "            else:\n",
    "                # count the token, i.e. add 1 to its count (add one smoothing)\n",
    "                model[n_minus_1_gram][token] += 1\n",
    "    return model\n",
    "\n",
    "def calculate_probabilities(model):\n",
    "    \"\"\" Calculates the probabilities for the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # get the counts of all the tokens\n",
    "        token_counts = model[n_minus_1_gram].values()\n",
    "        # calculate the total count\n",
    "        total_count = sum(token_counts)\n",
    "        # iterate over all the tokens in the model\n",
    "        for token in model[n_minus_1_gram]:\n",
    "            # calculate the probability, i.e. divide the count by the total count\n",
    "            model[n_minus_1_gram][token] /= total_count\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:19.064047Z",
     "end_time": "2023-04-25T12:48:19.188776Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kMC_u8eQbVvZ",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:19.078703Z",
     "end_time": "2023-04-25T12:48:19.218281Z"
    }
   },
   "source": [
    "def lm(n, vocabulary, data_file_path, add_one):\n",
    "    \"\"\" Builds an n-gram model from the given data\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "        data_file_path: the data_file from which we record probabilities for our model\n",
    "        add_one: True/False (use add_one smoothing or not)\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file_path,  encoding=\"utf-8\")\n",
    "    # build the n-gram model\n",
    "    model = build_ngram_model(current_data, n)\n",
    "    if add_one:\n",
    "        # add one smoothing\n",
    "        model = add_one_smoothing(model, vocabulary)\n",
    "    # calculate the probabilities\n",
    "    model = calculate_probabilities(model)\n",
    "    return model"
   ],
   "execution_count": 155,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "outputs": [],
   "source": [
    "# # call the function for the first data file\n",
    "lm_model = lm(2, vocabulary, path + \"/\" + data_files[0], True)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:19.095321Z",
     "end_time": "2023-04-25T12:48:20.649643Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "outputs": [
    {
     "data": {
      "text/plain": "['!',\n '\"',\n '#',\n '$',\n \"'\",\n '(',\n '*',\n '-',\n '.',\n '/',\n '0',\n '1',\n '2',\n '3',\n '4',\n '5',\n '6',\n '7',\n '8',\n '9',\n ':',\n '<',\n '<notInModel>',\n '>',\n '@',\n 'A',\n 'B',\n 'C',\n 'D',\n 'E',\n 'F',\n 'G',\n 'H',\n 'I',\n 'J',\n 'K',\n 'L',\n 'M',\n 'N',\n 'O',\n 'P',\n 'Q',\n 'R',\n 'S',\n 'T',\n 'U',\n 'V',\n 'W',\n 'X',\n 'Y',\n 'Z',\n '[',\n 'a',\n 'b',\n 'c',\n 'd',\n 'e',\n 'f',\n 'g',\n 'h',\n 'i',\n 'j',\n 'k',\n 'l',\n 'm',\n 'n',\n 'o',\n 'p',\n 'q',\n 'r',\n 's',\n 't',\n 'v',\n 'w',\n 'x',\n 'y',\n '{',\n '\\x91',\n 'Â¿',\n 'â€˜',\n 'â€œ',\n 'â€¼',\n 'â†’',\n 'âœ¨',\n 'â¤',\n 'â¤ï¸',\n 'â™',\n 'â¦‘',\n 'ã€',\n 'ï¼ ',\n 'ğŸ‡ºğŸ‡¸',\n 'ğŸ´',\n 'ğŸ¶',\n 'ğŸ‘Œ',\n 'ğŸ’',\n 'ğŸ’Ÿ',\n 'ğŸ’«',\n 'ğŸ’­',\n 'ğŸ“ˆ',\n 'ğŸ“·',\n 'ğŸ”´',\n 'ğŸ˜‚',\n 'ğŸ˜­',\n 'ğŸ™„',\n 'ğŸ™Œ']"
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ket lm_model keys\n",
    "sorted(lm_model[(\"<start>\",)])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:20.649643Z",
     "end_time": "2023-04-25T12:48:20.659798Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8TchtI22I3"
   },
   "source": [
    "**Part 3**\n",
    "\n",
    "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "# a function that calculates the perplexity of a model\n",
    "def calculate_perplexity(current_data, n, model):\n",
    "    \"\"\" Calculates the perplexity of a model\n",
    "    Args:\n",
    "        current_data: the data from which we record probabilities for our model\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    Returns:\n",
    "        the perplexity of the model\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0\n",
    "    n_gram_count = 0\n",
    "\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        # convert the tweet to a list of tokens\n",
    "        tweet = tweet_to_token_tuples(tweet)\n",
    "\n",
    "        # add the first n-gram\n",
    "        n_minus_1_gram = (\"<start>\", ) + tweet[:n-2]\n",
    "        n_th_token = tweet[n-1]\n",
    "        if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "            log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "            n_gram_count += 1\n",
    "\n",
    "        # iterate over all the n-grams in the tweet\n",
    "        for i in range(len(tweet) - n + 1):\n",
    "            n_gram = tweet[i:i+n]\n",
    "            n_minus_1_gram = n_gram[:-1]\n",
    "            n_th_token = n_gram[-1]\n",
    "\n",
    "            if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "                log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "                n_gram_count += 1\n",
    "\n",
    "        # add the last n-gram\n",
    "        n_minus_1_gram = tweet[-n+1:]\n",
    "        n_th_token = \"<end>\"\n",
    "        if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "            log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "            n_gram_count += 1\n",
    "\n",
    "    if n_gram_count == 0:\n",
    "        return float('inf')  # Return infinite perplexity if no n-grams found\n",
    "\n",
    "    entropy = log_prob_sum / n_gram_count\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:20.659798Z",
     "end_time": "2023-04-25T12:48:20.671722Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F0kkMn328-lJ",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:20.675733Z",
     "end_time": "2023-04-25T12:48:20.778204Z"
    }
   },
   "source": [
    "def eval(n, model, data_file):\n",
    "    \"\"\" Evaluates the perplexity of a model\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "        data_file: the data file path for which we want to calculate the perplexity\n",
    "    Returns:\n",
    "        the perplexity of the model\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file, encoding=\"utf-8\")\n",
    "    # calculate the perplexity\n",
    "    perplexity = calculate_perplexity(current_data, n, model)\n",
    "    return perplexity"
   ],
   "execution_count": 159,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity:  22.179340868703267\n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "perplexity = eval(2, lm_model, path + \"/\" + data_files[0])\n",
    "print(\"perplexity: \", perplexity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:20.690027Z",
     "end_time": "2023-04-25T12:48:21.997216Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enGmtLE3921p"
   },
   "source": [
    "**Part 4**\n",
    "\n",
    "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values.\n",
    "\n",
    "Save the dataframe to a CSV with the name format: {student_id_1}\\_...\\_{student_id_n}\\_part4.csv"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "caAxLE9s_fvn",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:22.005673Z",
     "end_time": "2023-04-25T12:48:22.018815Z"
    }
   },
   "source": [
    "def match(n, add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Then, calculate the perplexity of all possible pairs.\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        df: a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # create a dataframe\n",
    "    df = pd.DataFrame(columns=data_files, index=data_files)\n",
    "\n",
    "    # create models for every language\n",
    "    models = compute_data_files_models(data_files, n, vocabulary, path, add_one)\n",
    "\n",
    "    # calculate the perplexity of all possible pairs\n",
    "    for lang1 in data_files: # will be the model\n",
    "        # define the model\n",
    "        current_model = models[lang1]\n",
    "        for lang2 in data_files: # will be the data file\n",
    "            # define the data file\n",
    "            current_data_file = path + \"/\" + lang2\n",
    "            # evaluate the model\n",
    "            perplexity = eval(n, current_model, current_data_file)\n",
    "            # save the perplexity to the dataframe\n",
    "            df[lang1][lang2] = perplexity\n",
    "    return df, models # return the dataframe and the models, so that we can use them later\n",
    "\n",
    "def compute_data_files_models(data_files, n, vocabulary, path , add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Args:\n",
    "        data_files: the data files to create models for\n",
    "        n: the n in n-gram\n",
    "        vocabulary: the vocabulary\n",
    "        path: the path to the data files\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for data_file in data_files:\n",
    "        models[data_file] = lm(n, vocabulary, path + \"/\" + data_file, add_one)\n",
    "    return models\n"
   ],
   "execution_count": 161,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "outputs": [],
   "source": [
    "# # call the function\n",
    "# df_part4, models_part4 = match(2, True)\n",
    "# print(\"dataframe: \")\n",
    "# print(df_part4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:22.016074Z",
     "end_time": "2023-04-25T12:48:22.078551Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "outputs": [],
   "source": [
    "# # save the dataframe to a CSV\n",
    "# df_part4.to_csv(\"language_perplexity_part4.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:22.030039Z",
     "end_time": "2023-04-25T12:48:22.078551Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waGMwA8H_n17"
   },
   "source": [
    "**Part 5**\n",
    "\n",
    "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another.\n",
    "\n",
    "Load each result to a dataframe and save to a CSV with the name format: \n",
    "\n",
    "for cases with add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_part5.csv\n",
    "\n",
    "For cases without add_one:\n",
    "{student_id_1}\\_...\\_{student_id_n}\\_n1\\_wo\\_addone\\_part5.csv\n",
    "\n",
    "Follow the same format for n2,n3, and n4\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nk32naXyAMdl",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:22.045292Z",
     "end_time": "2023-04-25T12:48:22.078551Z"
    }
   },
   "source": [
    "def run_match():\n",
    "    \"\"\" Runs the match function for all the n values and add_one values\n",
    "    Returns:\n",
    "        dataframes: a dictionary of the dataframes, so that we can use them later,\n",
    "                    i.e {n, add_one: dataframe} where dataframe is a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "        language_models_dict: a dictionary of the models, so that we can use them later,\n",
    "                              i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    n_values = [1, 2, 3, 4] # TODO: change to [1, 2, 3, 4], memory problem, change match function to use only dataframes\n",
    "    add_one_values = [True, False]\n",
    "    # create dictionaries for the dataframes, key = (n, add_one), value = dataframe\n",
    "    dataframes = {}\n",
    "    # create a dictionary for the language models, key = (n, add_one), value = language models = {language: model}\n",
    "    language_models_dict = {}\n",
    "    # iterate over all the n values\n",
    "    for n in n_values:\n",
    "        # iterate over all the add_one values\n",
    "        for add_one in add_one_values:\n",
    "            # create the dataframe and the language models, using the match function\n",
    "            current_df, current_language_models = match(n, add_one)\n",
    "            print(\"completed n = \" + str(n) + \", add_one = \" + str(add_one) + \"!\")\n",
    "            # add the dataframe to the dictionary\n",
    "            dataframes[(n, add_one)] = current_df\n",
    "            # add the language models to the dictionary\n",
    "            language_models_dict[(n, add_one)] = current_language_models\n",
    "            # save the dataframe to a CSV\n",
    "            if add_one:\n",
    "                current_df.to_csv(\"language_perplexity_n\" + str(n) + \"_part5.csv\")\n",
    "            else:\n",
    "                current_df.to_csv(\"language_perplexity_n\" + str(n) + \"_wo_addone_part5.csv\")\n",
    "    return dataframes, language_models_dict # return the dataframes and the language models, so that we can use them later\n"
   ],
   "execution_count": 164,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "run_match_dataframes, run_match_language_models = run_match()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:26:20.172305Z",
     "end_time": "2023-04-25T12:39:09.581361Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframes: \n",
      "{(1, True):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  38.651038  40.255172  39.768361  40.818029  39.606203  38.826503  \\\n",
      "es.csv  37.595611   36.37604  37.781812  38.028523  37.356185  38.548417   \n",
      "fr.csv  38.999054   38.16384  37.594818  43.927003  38.247928   40.02221   \n",
      "in.csv  39.739496  41.822219  42.609707  37.795484  41.580978  39.705989   \n",
      "it.csv  37.767691  37.733831  38.158867  40.628272  37.912302  38.965778   \n",
      "nl.csv  38.223314  39.836763  39.315063  40.311804  39.511595  37.763723   \n",
      "pt.csv  38.178067  35.662961  37.460598  39.344541  37.109923  39.232554   \n",
      "tl.csv  42.879986  45.278113  47.205344  40.906098  44.466137  44.414077   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  40.777992  40.359558  \n",
      "es.csv  36.097163  40.950019  \n",
      "fr.csv  38.515622  44.431782  \n",
      "in.csv  41.273174  37.467349  \n",
      "it.csv  38.157666  40.126253  \n",
      "nl.csv  40.027033  41.276579  \n",
      "pt.csv   37.66028  42.275292  \n",
      "tl.csv  45.236791  41.217006  , (1, False):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  35.917434  40.188323  39.707872  40.742841  39.530161  38.758176  \\\n",
      "es.csv   37.66664  33.629184  37.947834  37.990303  37.322022  38.545393   \n",
      "fr.csv  38.986682  38.135539   35.01136  44.176083  38.175724  40.000094   \n",
      "in.csv  39.673316  41.745694  42.545856  34.746732   41.50229  39.638134   \n",
      "it.csv   37.70422  37.682695  38.096324  40.576388   34.94557  38.911613   \n",
      "nl.csv  38.160482   39.75885  39.248182  40.236632  39.432553  34.989503   \n",
      "pt.csv   38.24458  35.611028  37.520429  39.390268  37.064219  39.215699   \n",
      "tl.csv   42.81696   45.20219  47.148086  40.836949  44.394823  44.351927   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  40.692283  40.264285  \n",
      "es.csv  36.025618  40.948413  \n",
      "fr.csv  38.438945  44.469281  \n",
      "in.csv  41.179732   37.37979  \n",
      "it.csv  38.071262   40.04248  \n",
      "nl.csv  39.928373  41.178517  \n",
      "pt.csv  34.152722  42.369358  \n",
      "tl.csv  45.143136  37.594761  , (2, True):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  22.179341  33.284028  29.481373  30.720834  32.934482   28.20067  \\\n",
      "es.csv  25.949902  20.034726  24.094715   28.14212  24.365684  29.791193   \n",
      "fr.csv  26.531176  27.952429   20.96296  30.741311  28.460292  30.323116   \n",
      "in.csv   30.86368   35.74069  34.253967  22.493705  35.075304  31.609465   \n",
      "it.csv  27.670422  25.043401   27.12782  29.035276  20.549514  31.392106   \n",
      "nl.csv  28.595533  34.398741  30.765063  30.882991  34.644177  21.703024   \n",
      "pt.csv  28.087916  24.651538  26.854594  30.371782  25.743886  31.999403   \n",
      "tl.csv  30.469447  35.887688   36.43063   27.85726  34.677674  33.318352   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  34.368443  28.600285  \n",
      "es.csv  24.017643  28.019658  \n",
      "fr.csv  29.058724  30.519703  \n",
      "in.csv  37.165386  26.849716  \n",
      "it.csv  26.141149  27.847121  \n",
      "nl.csv  35.952841  32.208462  \n",
      "pt.csv  21.155358  29.798034  \n",
      "tl.csv  37.140787   22.79452  , (2, False):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv   18.75693  27.539228  24.878303  25.445844    27.4473  23.833932  \\\n",
      "es.csv  21.855713  16.795064  20.635272  23.570386  20.203406   24.88387   \n",
      "fr.csv  22.429841  23.019354  17.695885  25.261443   23.82032  24.951356   \n",
      "in.csv  26.178683  29.308283  28.556437  18.673946  28.847628  26.727413   \n",
      "it.csv  23.556727  20.919624  23.078292  24.023618  17.170311  26.344302   \n",
      "nl.csv  24.391358   28.57262  26.020645  25.749314  28.917983  18.399292   \n",
      "pt.csv  23.780375  20.779006  23.017574  25.318392  21.461032  26.664362   \n",
      "tl.csv  25.426681  28.948527  29.738407  22.861179  28.123278  27.510504   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  27.556253  23.733775  \n",
      "es.csv  19.710956  22.764784  \n",
      "fr.csv  23.565468  25.162554  \n",
      "in.csv  29.836825  22.447539  \n",
      "it.csv  21.245004  23.093038  \n",
      "nl.csv    29.1329  26.738064  \n",
      "pt.csv  17.082495  23.999985  \n",
      "tl.csv   29.11257  18.574983  , (3, True):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  29.370924  53.520793  45.274179  51.152813  53.192999  43.041086  \\\n",
      "es.csv  40.684825  26.878653  34.470696  49.413345  33.804317  49.795412   \n",
      "fr.csv  35.976562  38.672089  27.025657  50.234067  40.666267  43.237969   \n",
      "in.csv   57.26742  61.378786  62.800177  34.671439  62.846158  59.542765   \n",
      "it.csv  46.161699  36.911489  41.977991   52.59374  27.589143  56.223339   \n",
      "nl.csv  41.274458  53.097778  47.319487   52.63183  57.218979  30.092254   \n",
      "pt.csv  45.681136  33.145743  41.822943   54.52391  37.121749  55.627854   \n",
      "tl.csv  53.334808  64.534849  66.401733  44.724269  58.902936  58.562978   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  57.990461  42.934051  \n",
      "es.csv  32.699796  46.792109  \n",
      "fr.csv  42.950405  47.331318  \n",
      "in.csv  67.605722  44.923401  \n",
      "it.csv   42.05964  48.834074  \n",
      "nl.csv  61.823282  52.797894  \n",
      "pt.csv   28.67775  51.014423  \n",
      "tl.csv  65.861758  32.600197  , (3, False):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv   9.044354  15.282312  14.272734   14.73625  15.137754  13.498138  \\\n",
      "es.csv  14.449873   8.674021  13.049967  15.629115  12.254028  15.771715   \n",
      "fr.csv  12.934168  13.211857    8.64769  15.358583  13.559946  13.706211   \n",
      "in.csv  18.273194  17.397721  18.455231  10.164792  17.695734  18.145291   \n",
      "it.csv  15.390192  12.688286   14.38265   15.64579   8.596889  17.065461   \n",
      "nl.csv  14.279845  16.054064  15.384319  15.972773  17.251473   9.185398   \n",
      "pt.csv  15.591489  11.710184  15.005122  16.325042  12.802958  17.193836   \n",
      "tl.csv  16.008774  16.522049  17.339498  13.609511  15.355238  16.315699   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  14.987073  12.484028  \n",
      "es.csv  11.084769  14.122375  \n",
      "fr.csv  13.752872  14.049846  \n",
      "in.csv  17.620391  14.333229  \n",
      "it.csv   13.02045  14.471503  \n",
      "nl.csv  17.213256  15.364592  \n",
      "pt.csv    8.14822  14.625475  \n",
      "tl.csv  15.258595   8.654417  , (4, True):             en.csv      es.csv      fr.csv      in.csv      it.csv   \n",
      "en.csv   70.131747  126.025236  109.282297  136.067835  125.897335  \\\n",
      "es.csv  117.479838    64.10376   85.848424  143.645191   86.113345   \n",
      "fr.csv   89.917415   81.863863   61.765083  135.576022   94.935615   \n",
      "in.csv  129.465922   123.95443  138.978854   89.216161  125.678706   \n",
      "it.csv   118.76821   88.834016  106.873668  143.329448   65.896985   \n",
      "nl.csv   85.382832   93.487355   88.435228  119.120125  100.878812   \n",
      "pt.csv  116.855767   68.719152   98.884791  143.939666   90.319714   \n",
      "tl.csv  118.890945  144.833203   145.56181  108.305615  123.983348   \n",
      "\n",
      "            nl.csv      pt.csv      tl.csv  \n",
      "en.csv  110.929992    136.8669  113.321774  \n",
      "es.csv  125.764518   73.913264  142.770331  \n",
      "fr.csv   100.92154   99.795055  132.463881  \n",
      "in.csv  138.670206  133.849286  119.326722  \n",
      "it.csv  142.329098  105.565491  144.134764  \n",
      "nl.csv   71.141662  113.356397  121.789059  \n",
      "pt.csv  139.038645   67.794953  141.957029  \n",
      "tl.csv  134.453229  144.577869   78.707172  , (4, False):           en.csv    es.csv    fr.csv    in.csv    it.csv    nl.csv    pt.csv   \n",
      "en.csv  4.483196    8.2004  8.007279   7.70137  7.641266  7.769921   7.60685  \\\n",
      "es.csv   9.80938  4.738417   8.75328  9.994932  8.149262  9.143294  7.029518   \n",
      "fr.csv  8.488768  7.961529  4.477069  8.989619  8.043753  8.194154  8.252752   \n",
      "in.csv  9.608732  9.198318  9.509478  5.024408  9.007372  9.589626   8.78601   \n",
      "it.csv  9.041425  8.029467  8.968213  9.499557  4.595224  9.176466  7.952916   \n",
      "nl.csv  8.153166  7.838007  8.101399  8.730806  7.833639  4.532329  7.928893   \n",
      "pt.csv  9.205177  7.025105  8.972975  9.509722   7.94442  9.237863  4.363594   \n",
      "tl.csv  8.203324   8.05161  8.152144   8.04439  7.625808  8.416823  7.489535   \n",
      "\n",
      "          tl.csv  \n",
      "en.csv  6.700533  \n",
      "es.csv  9.011313  \n",
      "fr.csv   8.50649  \n",
      "in.csv  8.914977  \n",
      "it.csv  8.806239  \n",
      "nl.csv  8.265987  \n",
      "pt.csv  8.512109  \n",
      "tl.csv  4.295471  }\n"
     ]
    }
   ],
   "source": [
    "print(\"dataframes: \")\n",
    "print(run_match_dataframes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:22.054859Z",
     "end_time": "2023-04-25T12:48:22.103167Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_models_to_pickle(language_models_dict=run_match_language_models, filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Saves the language models to a pickle file\n",
    "    Args:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # save run_match_language_models to a pickle file, so that we can use it later\n",
    "    with open('run_match_language_models.pickle', 'wb') as handle:\n",
    "        pickle.dump(language_models_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_dataframes_to_pickle(dataframes_dict=run_match_dataframes, filename=\"run_match_dataframes.pickle\"):\n",
    "    \"\"\" Saves the dataframes to a pickle file\n",
    "    Args:\n",
    "        dataframes_dict: a dictionary of the dataframes, i.e {n, add_one: dataframe} where dataframe is a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "    \"\"\"\n",
    "    # save run_match_dataframes to a pickle file, so that we can use it later\n",
    "    with open('run_match_dataframes.pickle', 'wb') as handle:\n",
    "        pickle.dump(dataframes_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:40:22.932167Z",
     "end_time": "2023-04-25T12:40:23.477338Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_models_from_pickle(filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Loads the language models from a pickle file\n",
    "    Returns:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # load run_match_language_models from a pickle file\n",
    "    with open('run_match_language_models.pickle', 'rb') as handle:\n",
    "        language_models_dict = pickle.load(handle)\n",
    "    return language_models_dict\n",
    "\n",
    "def load_dataframes_from_pickle(filename=\"run_match_dataframes.pickle\"):\n",
    "    \"\"\" Loads the dataframes from a pickle file\n",
    "    Returns:\n",
    "        dataframes_dict: a dictionary of the dataframes, i.e {n, add_one: dataframe} where dataframe is a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:36.936514Z",
     "end_time": "2023-04-25T12:48:36.956818Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save the language models to a pickle file (remove/put # in front of the next line to save/load the language models)\n",
    "save_models_to_pickle(run_match_language_models, \"run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:40:27.500211Z",
     "end_time": "2023-04-25T12:40:30.000059Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "outputs": [],
   "source": [
    "# load the language models from a pickle file\n",
    "run_match_language_models = load_models_from_pickle(\"run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:48:39.215295Z",
     "end_time": "2023-04-25T12:48:41.077340Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 keys: \n",
      "[('<start>', 'R', 'T'), ('R', 'T', ' '), ('T', ' ', '@'), (' ', '@', 'O'), ('@', 'O', 'N'), ('O', 'N', 'H'), ('N', 'H', 'E'), ('H', 'E', 'R'), ('E', 'R', 'P'), ('R', 'P', 'E')]\n"
     ]
    }
   ],
   "source": [
    "# test the language models dictionary\n",
    "\n",
    "test_n = 4\n",
    "test_add_one = True\n",
    "test_language = \"en.csv\"\n",
    "test_language_models = run_match_language_models[(test_n, test_add_one)][test_language]\n",
    "\n",
    "print(\"10 keys: \")\n",
    "print(list(test_language_models.keys())[:10])\n",
    "\n",
    "test_key = test_language_models.keys()[0]"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:50:57.286359Z",
     "end_time": "2023-04-25T12:50:57.325298Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg4h5Cl0q2nR"
   },
   "source": [
    "**Part 6**\n",
    "\n",
    "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
    "\n",
    "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be accepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qD6IRIQLrlZF",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:53:22.313255Z",
     "end_time": "2023-04-25T12:53:22.576178Z"
    }
   },
   "source": [
    "def classify(n=3, add_one=True):\n",
    "    \"\"\" Classifies the test sentences\n",
    "    Returns:\n",
    "        classification_result: a list of tuples, where each tuple contains the tweet_id, the sentence, the true language, and the predicted language\n",
    "    \"\"\"\n",
    "    # we will use the language models from part 5, with n = 3 and add_one = True\n",
    "    language_models = run_match_language_models[(n, add_one)]\n",
    "\n",
    "    # read the test file, tweet_id, tweet_text, label\n",
    "    test_data = pd.read_csv(path + \"/test.csv\",  encoding=\"utf-8\")\n",
    "\n",
    "    # classify the sentences\n",
    "    classification_result = []\n",
    "\n",
    "    # iterate over the rows in the test data\n",
    "    for index, row in test_data.iterrows():\n",
    "        tweet_id = row['tweet_id']\n",
    "        sentence = row['tweet_text']\n",
    "        true_language = row['label']\n",
    "        predicted_language = ''\n",
    "\n",
    "        predicted_language = single_classification(sentence, language_models, n)\n",
    "\n",
    "        # add the result to the classification_result list\n",
    "        classification_result.append((tweet_id, sentence, true_language, predicted_language))\n",
    "\n",
    "    return classification_result\n",
    "\n",
    "\n",
    "def single_classification(sentence, language_models = run_match_language_models[(3, True)], n=3):\n",
    "    \"\"\" Classifies a single sentence\n",
    "    Args:\n",
    "        sentence: the sentence to classify\n",
    "        language_models: the language models to use for classification\n",
    "                         i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    Returns:\n",
    "        predicted_language: the predicted language of the sentence\n",
    "    \"\"\"\n",
    "    predicted_language = ''\n",
    "    min_perplexity = float('inf')\n",
    "    # iterate over the language models\n",
    "    for data_file in data_files:\n",
    "        current_model = language_models[data_file]\n",
    "        # create a temporary DataFrame, with the sentence as the only row\n",
    "        temp_df = pd.DataFrame([sentence], columns=['tweet_text'])\n",
    "        # calculate the perplexity using the temporary DataFrame\n",
    "        current_perplexity = calculate_perplexity(temp_df, n, current_model)\n",
    "\n",
    "        if current_perplexity < min_perplexity:\n",
    "            min_perplexity = current_perplexity\n",
    "            predicted_language = data_file[:-4] # remove the .csv from the end of the file name\n",
    "    return predicted_language\n",
    "\n"
   ],
   "execution_count": 175,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "outputs": [],
   "source": [
    "classification_result_n1 = classify(n=1, add_one=False)\n",
    "classification_result_n2 = classify(n=2, add_one=False)\n",
    "classification_result_n3 = classify(n=3, add_one=False)\n",
    "classification_result_n4 = classify(n=4, add_one=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:53:22.336796Z",
     "end_time": "2023-04-25T12:54:41.050166Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "outputs": [],
   "source": [
    "def print_classification_result(classification_result):\n",
    "    for result in classification_result_n1:\n",
    "        predicted_language = result[3]\n",
    "        true_language = result[2]\n",
    "        print(\"predicted_language: \" + predicted_language + \", true_language: \" + true_language)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.051175Z",
     "end_time": "2023-04-25T12:54:41.066926Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "outputs": [],
   "source": [
    "# print(\"classification result for n = 1, add_one = True\")\n",
    "# print_classification_result(classification_result_n1)\n",
    "# print(\"classification result for n = 2, add_one = True\")\n",
    "# print_classification_result(classification_result_n2)\n",
    "# print(\"classification result for n = 3, add_one = True\")\n",
    "# print_classification_result(classification_result_n3)\n",
    "# print(\"classification result for n = 4, add_one = True\")\n",
    "# print_classification_result(classification_result_n4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.066926Z",
     "end_time": "2023-04-25T12:54:41.100627Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "outputs": [],
   "source": [
    "def get_accuracy(classification_result):\n",
    "    count_correct = 0\n",
    "    for result in classification_result:\n",
    "        predicted_language = result[3]\n",
    "        true_language = result[2]\n",
    "        if predicted_language == true_language:\n",
    "            count_correct += 1\n",
    "    return count_correct / len(classification_result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.083032Z",
     "end_time": "2023-04-25T12:54:41.100627Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, add_one = True, accuracy = 0.5833229153644206\n",
      "n = 2, add_one = True, accuracy = 0.8516064508063508\n",
      "n = 3, add_one = True, accuracy = 0.8756094511813977\n",
      "n = 4, add_one = True, accuracy = 0.7858482310288786\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy of the classification\n",
    "print(\"n = 1, add_one = True, accuracy = \" + str(get_accuracy(classification_result_n1)))\n",
    "print(\"n = 2, add_one = True, accuracy = \" + str(get_accuracy(classification_result_n2)))\n",
    "print(\"n = 3, add_one = True, accuracy = \" + str(get_accuracy(classification_result_n3)))\n",
    "print(\"n = 4, add_one = True, accuracy = \" + str(get_accuracy(classification_result_n4)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.092618Z",
     "end_time": "2023-04-25T12:54:41.109095Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ECmLd3rktZ"
   },
   "source": [
    "**Part 7**\n",
    "\n",
    "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n",
    "\n",
    "Load the results to a CSV (using a DataFrame), with a model_name and f1_score Name it {student_id_1}\\_...\\_{student_id_n}\\_part7.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "  model_name  f1_score\n",
    "0    Model A      0.85\n",
    "1    Model B      0.92\n",
    "2    Model C      0.87\n",
    "3    Model D      0.90\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "KF3ImVdPgAGC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "outputs": [],
   "source": [
    "def map_language_number(classification_result):\n",
    "    #we will use the following dictionary to convert the strings to numbers\n",
    "    language_to_number = {}\n",
    "    # we will use the following dictionary to convert the numbers back to strings\n",
    "    number_to_language = {}\n",
    "\n",
    "    number = 0\n",
    "    # iterate over the classification results\n",
    "    for result in classification_result:\n",
    "        if result[2] not in language_to_number:\n",
    "            language_to_number[result[2]] = number\n",
    "            number_to_language[number] = result[2]\n",
    "            number += 1\n",
    "        if result[3] not in language_to_number:\n",
    "            language_to_number[result[3]] = number\n",
    "            number_to_language[number] = result[3]\n",
    "            number += 1\n",
    "    return language_to_number, number_to_language"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.111705Z",
     "end_time": "2023-04-25T12:54:41.117451Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VOBO3YQls66r",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.125773Z",
     "end_time": "2023-04-25T12:54:41.179170Z"
    }
   },
   "source": [
    "import sklearn.metrics as metrics\n",
    "# The f1 score = 2 *(TP / (2TP + FP + FN))\n",
    "def calc_f1(result):\n",
    "    \"\"\" Calculates the f1 score of the classification result\n",
    "    Args:\n",
    "        result: a list of tuples, where each tuple contains the tweet_id, the sentence, the true language, and the predicted language\n",
    "    Returns:\n",
    "        f1_score: the f1 score of the classification result\n",
    "    \"\"\"\n",
    "    # create mappings from language to number and number to language\n",
    "    language_to_number, number_to_language = map_language_number(result)\n",
    "\n",
    "    # create a DataFrame with the results\n",
    "    df = pd.DataFrame(result, columns=['tweet_id', 'tweet_text', 'true_language', 'predicted_language'])\n",
    "    # drop the tweet_id and tweet_text columns\n",
    "    df = df.drop(columns=['tweet_id', 'tweet_text'])\n",
    "    # convert the true_language and predicted_language columns to numbers\n",
    "    df['true_language'] = df['true_language'].apply(lambda x: language_to_number[x])\n",
    "    df['predicted_language'] = df['predicted_language'].apply(lambda x: language_to_number[x])\n",
    "    # calculate the f1 score\n",
    "    f1_score = metrics.f1_score(df['true_language'], df['predicted_language'], average='macro')\n",
    "    return f1_score"
   ],
   "execution_count": 182,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, add_one = True, f1_score = 0.5765291191677335\n",
      "n = 2, add_one = True, f1_score = 0.8517528626462993\n",
      "n = 3, add_one = True, f1_score = 0.7787734992164582\n",
      "n = 4, add_one = True, f1_score = 0.6988932355717234\n"
     ]
    }
   ],
   "source": [
    "# calculate the f1 score for each n\n",
    "f1_score_n1 = calc_f1(classification_result_n1)\n",
    "print(\"n = 1, add_one = True, f1_score = \" + str(f1_score_n1))\n",
    "\n",
    "f1_score_n2 = calc_f1(classification_result_n2)\n",
    "print(\"n = 2, add_one = True, f1_score = \" + str(f1_score_n2))\n",
    "\n",
    "f1_score_n3 = calc_f1(classification_result_n3)\n",
    "print(\"n = 3, add_one = True, f1_score = \" + str(f1_score_n3))\n",
    "\n",
    "f1_score_n4 = calc_f1(classification_result_n4)\n",
    "print(\"n = 4, add_one = True, f1_score = \" + str(f1_score_n4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.141142Z",
     "end_time": "2023-04-25T12:54:41.231850Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br><br><br>\n",
    "**Part 8**  \n",
    "Let's use your Language model (dictionary) for generation (NLG).\n",
    "\n",
    "When it comes to sampling from a language model decoder during text generation, there are several different methods that can be used to control the randomness and diversity of the generated text. \n",
    "\n",
    "Some of the most commonly used methods include:\n",
    "\n",
    "> `Greedy sampling`\n",
    "In this method, the model simply selects the word with the highest probability as the next word at each time step. This method can produce fluent text, but it can also lead to repetitive or predictable output.\n",
    "\n",
    "> `Temperature scaling`  \n",
    "Temperature scaling involves scaling the logits output of the language model by a temperature parameter before softmax normalization. This has the effect of smoothing the distribution of probabilities and increasing the probability of lower-probability words, which can lead to more diverse and creative output.\n",
    "\n",
    "> `Top-K sampling`  \n",
    "In this method, the model restricts the sampling to the top-K most likely words at each time step, where K is a predefined hyperparameter. This can generate more diverse output than greedy sampling, while limiting the number of low-probability words that are sampled.\n",
    "\n",
    "> `Nucleus sampling` (also known as top-p sampling)  \n",
    "This method restricts the sampling to the smallest possible set of words whose cumulative probability exceeds a certain threshold, defined by a hyperparameter p. Like top-K sampling, this can generate more diverse output than greedy sampling, while avoiding sampling extremely low probability words.\n",
    "\n",
    "> `Beam search`  \n",
    "Beam search involves maintaining a fixed number k of candidate output sequences at each time step, and then selecting the k most likely sequences based on their probabilities. This can improve the fluency and coherence of the output, but may not produce as much diversity as sampling methods.\n",
    "\n",
    "The choice of sampling method depends on the specific application and desired balance between fluency, diversity, and randomness. Hyperparameters such as temperature, K, p, and beam size can also be tuned to adjust the behavior of the language model during sampling.\n",
    "\n",
    "\n",
    "You may read more about this concept in <a href='https://huggingface.co/blog/how-to-generate#:~:text=pad_token_id%3Dtokenizer.eos_token_id)-,Greedy%20Search,-Greedy%20search%20simply'>this</a> blog post.\n"
   ],
   "metadata": {
    "id": "NfBYgfjADNPL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Please added the needed code for each sampeling method:**"
   ],
   "metadata": {
    "id": "GbReeHtwNWKS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def softmax(probabilities):\n",
    "    \"\"\" Applies the softmax function to the probabilities\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities (not yet probalities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a dictionary of probabilities (normalized)\n",
    "    \"\"\"\n",
    "    np_probabilities = np.array(list(probabilities.values()))\n",
    "    np_probabilities = np.exp(np_probabilities)\n",
    "    np_probabilities = np_probabilities / np.sum(np_probabilities)\n",
    "    # convert the numpy array back to a dictionary\n",
    "    probabilities = {key: value for key, value in zip(probabilities.keys(), np_probabilities)}\n",
    "    return probabilities\n",
    "\n",
    "def make_prob_1(probabilities):\n",
    "    \"\"\" Makes the sum of the probabilities equal to 1\n",
    "    Args:\n",
    "        probabilities: a list of probabilities (not yet probalities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a list of probabilities (normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    # by dividing each probability by the sum of all probabilities\n",
    "    sum_prob = sum(probabilities)\n",
    "    probabilities = [prob / sum_prob for prob in probabilities]\n",
    "    return probabilities\n",
    "\n",
    "def get_correct_model(all_models=run_match_language_models, prefix=\"h\", language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Gets the correct language model, the correct n-1 prefix, and the probabilities of the next token\n",
    "    Args:\n",
    "        all_models: a dictionary of language models\n",
    "        prefix: the prefix\n",
    "        language: the language\n",
    "        add_one: whether to use add-one smoothing or not\n",
    "    Returns:\n",
    "        correct_model: the correct language model (dictionary), where the keys are the prefixes and the values are the probabilities of the next token\n",
    "        current_n_minus_1_prefix: the correct key for the language model\n",
    "        next_token_probabilities: the probabilities of the next token (dictionary), where the keys are the tokens and the values are the probabilities\n",
    "    \"\"\"\n",
    "    prefix_tokens = tweet_to_token_tuples(prefix)\n",
    "\n",
    "    # we want to use maximum n-gram we can, but not more than max_n\n",
    "    n = min(max_n, len(prefix_tokens) + 1)\n",
    "\n",
    "    # get the n-gram model\n",
    "    correct_model = all_models[(n, add_one)][language]\n",
    "\n",
    "    # get the n-1 tokens of the prefix, i.e. the key for the language model\n",
    "    tuple_key_prefix = tuple(prefix_tokens[-(n - 1):])\n",
    "\n",
    "    # if the prefix is not in the language model, sample a random key from the language model\n",
    "    if tuple_key_prefix not in correct_model:\n",
    "        tuple_key_prefix = random.choice(list(correct_model.keys())) # TODO: ask what to do in this situation, maybe smoothing not correct?\n",
    "\n",
    "\n",
    "    # get the probabilities of the next token\n",
    "    next_token_probabilities = correct_model[tuple_key_prefix]\n",
    "\n",
    "    return correct_model, tuple_key_prefix, next_token_probabilities\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.227856Z",
     "end_time": "2023-04-25T12:54:41.238286Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_greedy(probabilities, k=1):\n",
    "    \"\"\" Samples the next token greedily, i.e. the token with the highest probability\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "    Returns:\n",
    "        max_token: the token with the highest probability\n",
    "    \"\"\"\n",
    "    # sample the token with the k highest probability\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    # if k is larger than the number of probabilities, set k to the number of probabilities\n",
    "    k = k if len(sorted_probabilities) >= k else len(sorted_probabilities)\n",
    "    # sample the token with the k highest probability\n",
    "    next_token = sorted_probabilities[k - 1][0]\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_temperature(probabilities, temperature=1.0, k=1):\n",
    "    \"\"\" Samples the next token using temperature sampling\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        temperature: the temperature\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # scale the probabilities by the temperature\n",
    "    probabilities = {key: value ** (1 / temperature) for key, value in probabilities.items()}\n",
    "    # softmax the probabilities\n",
    "    probabilities = softmax(probabilities)\n",
    "    # sample from the probabilities dictionary, use the np.random.choice function\n",
    "    np_probabilities = np.array(list(probabilities.values()))\n",
    "    np_tokens = np.array(list(probabilities.keys()))\n",
    "    # convert the tokens (tuple) to a list of strings\n",
    "    np_tokens = [\"\".join(token) for token in np_tokens]\n",
    "    # sample the next token\n",
    "    next_token = np.random.choice(np_tokens, p=np_probabilities)\n",
    "    # return the sampled token\n",
    "    return next_token\n",
    "\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_topK(probabilities, k=1):\n",
    "    \"\"\" Samples the next token using top-k sampling, i.e. only the top k tokens are considered\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        k: the number of tokens to consider\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    # take the top k\n",
    "    top_k = sorted_probabilities[:k]\n",
    "    # split the top k into tokens and probabilities\n",
    "    top_k_probs = [prob for (token, prob) in top_k]\n",
    "    top_k_tokens = [token for (token, prob) in top_k]\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_k_probs = make_prob_1(top_k_probs)\n",
    "    # sample from the top k tokens\n",
    "    next_token = np.random.choice(top_k_tokens, p=top_k_probs)\n",
    "    return next_token\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_topP(probabilities, p=0.9):\n",
    "    \"\"\" Samples the next token using top-p sampling,\n",
    "    i.e. only the tokens with the highest probabilities are considered, until the sum of the probabilities is greater than p\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        p: the threshold\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    current_sum = 0\n",
    "    top_p_tokens = []\n",
    "    top_p_probs = []\n",
    "    current_index = 0\n",
    "    while current_sum < p:\n",
    "        top_p_tokens.append(sorted_probabilities[current_index][0])\n",
    "        top_p_probs.append(sorted_probabilities[current_index][1])\n",
    "        current_sum += sorted_probabilities[current_index][1]\n",
    "        current_index += 1\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_p_probs = make_prob_1(top_p_probs)\n",
    "    # convert the tokens (tuple) to a list of strings\n",
    "    top_p_tokens = [\"\".join(token) for token in top_p_tokens]\n",
    "    # sample from the top p tokens\n",
    "    next_token = np.random.choice(top_p_tokens, p=top_p_probs)\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def sample_beam(probabilities, num_beams = 3):\n",
    "    \"\"\" Samples the next tokens using beam search, i.e., keeps the top num_beams hypotheses at each step.\n",
    "        Helper function for beam_search\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        num_beams: the number of beams to keep, i.e. the number of hypotheses to keep at each step\n",
    "    Returns:\n",
    "        beam_tokens: a list of top num_beams tokens\n",
    "        beam_probs: a list of the corresponding probabilities of the top num_beams tokens\n",
    "    \"\"\"\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_beams = sorted_probabilities[:num_beams]\n",
    "\n",
    "    beam_tokens = [token for (token, prob) in top_beams]\n",
    "    beam_probs = [prob for (token, prob) in top_beams]\n",
    "\n",
    "    return beam_tokens, beam_probs\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "v4TrLs1kI3fW",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.248342Z",
     "end_time": "2023-04-25T12:54:41.310374Z"
    }
   },
   "execution_count": 185,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sampled token using temperature sampling:  1\n"
     ]
    }
   ],
   "source": [
    "test_string = \"<start>1: â¡ï¸. 2: ğŸ¤£â¤ï¸. 3: ğŸ¤£â¤ï¸â¤ï¸.<end>\"\n",
    "probabilities = {'<start>': 0.0, '1': 0.0, 'â¡ï¸': 0.7, 'ï¸': 0.0, '2': 0.05, 'ğŸ¤£': 0.05, 'â¤': 0.05, 'ï¸': 0.05, '3': 0.05, 'ğŸ¤£': 0.05}\n",
    "# sample the next token using temperature sampling\n",
    "next_token = sample_temperature(probabilities, temperature=0.5)\n",
    "print(\"sampled token using temperature sampling: \", next_token)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.257886Z",
     "end_time": "2023-04-25T12:54:41.318753Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your Language Model to generate each one out of the following examples with the coresponding params.    \n",
    "Notice the 4 core issues: \n",
    "- Starting tokens\n",
    "- Length of the generation\n",
    "- Sampling methond (use all)\n",
    "- Stop Token (if this token is sampled, stop generating)"
   ],
   "metadata": {
    "id": "Giylo6-lI21t"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_ = {\n",
    "    'example1' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['greedy','beam'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example2' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['temperature','topK','topP'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example3' : {\n",
    "        'start_tokens' : \"He\",\n",
    "        'sampling_method' : ['greedy','beam','temperature','topK','topP'],\n",
    "        'gen_length' : \"20\",\n",
    "        'stop_token' : \"me\",\n",
    "        'generation' : []\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "id": "9pUOkRtjN1mI",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.273049Z",
     "end_time": "2023-04-25T12:54:41.318753Z"
    }
   },
   "execution_count": 187,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your LM to generate a string based on the parametes of each examples, and store the generation sequance at the generation list."
   ],
   "metadata": {
    "id": "YTbF-9zKVchQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_string(all_models, prefix, sampling_method, gen_length, stop_token, num_beams=5):\n",
    "    \"\"\" Generates a string using the given language model\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP', 'beam'\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        num_beams: the number of beams to keep (only relevant for beam search)\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    if sampling_method == 'beam':\n",
    "        return beam_search(all_models, prefix, gen_length, stop_token, num_beams)\n",
    "    else:\n",
    "        return generate_string_not_beam(all_models, prefix, gen_length, stop_token, sampling_method)\n",
    "\n",
    "def beam_search(all_models, prefix, gen_length, stop_token, num_beams):\n",
    "    \"\"\" Generates a string using beam search\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        num_beams: the number of beams to keep (only relevant for beam search)\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    # initialize the beams\n",
    "    beams = [(prefix, 0)]  # (prefix, log_prob)\n",
    "\n",
    "    # generate the string token by token\n",
    "    for _ in range(gen_length):\n",
    "        new_beams = []\n",
    "\n",
    "        # sample the next token for each beam\n",
    "        for beam_prefix, beam_log_prob in beams:\n",
    "            # get the correct language model\n",
    "            current_lm, current_tuple_key_prefix, next_token_probabilities = get_correct_model(all_models, beam_prefix)\n",
    "\n",
    "            # sample the top num_beams tokens and probabilities\n",
    "            beam_tokens, beam_probs = sample_beam(next_token_probabilities, num_beams)\n",
    "\n",
    "            # update the beams\n",
    "            for token, prob in zip(beam_tokens, beam_probs):\n",
    "                if not beam_prefix.endswith(stop_token) or beam_prefix.endswith(\"<end>\"):\n",
    "                    new_prefix, new_log_prob = update_beam(beam_prefix, beam_log_prob, token, prob, stop_token)\n",
    "                    new_beams.append((new_prefix, new_log_prob))\n",
    "                else:\n",
    "                    new_beams.append((beam_prefix, beam_log_prob))\n",
    "\n",
    "\n",
    "        # keep the top num_beams beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:num_beams]\n",
    "\n",
    "    # get the best beam\n",
    "    best_beam = beams[0][0]\n",
    "\n",
    "    return best_beam\n",
    "\n",
    "def generate_string_not_beam(all_models, prefix, gen_length, stop_token, sampling_method):\n",
    "    \"\"\" Samples the next tokens using the given language model\n",
    "    Args:\n",
    "        all_models: where all_models = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    current_prefix = prefix\n",
    "\n",
    "    # generate the string token by token\n",
    "    for _ in range(gen_length):\n",
    "        # get the correct language model\n",
    "        current_lm, current_tuple_key_prefix, next_token_probabilities = get_correct_model(all_models, current_prefix)\n",
    "\n",
    "        # sample the next token\n",
    "        next_token = select_next_token(next_token_probabilities, sampling_method)\n",
    "\n",
    "        # update the current prefix\n",
    "        current_prefix += next_token\n",
    "\n",
    "        # stop if the stop token was sampled\n",
    "        if current_prefix.endswith(stop_token) or next_token == \"<end>\":\n",
    "            break\n",
    "\n",
    "\n",
    "    return current_prefix\n",
    "\n",
    "def update_beam(beam_prefix, beam_log_prob, token, prob, stop_token):\n",
    "    \"\"\" Updates the beam, i.e. the current prefix and the current log probability\n",
    "    Args:\n",
    "        beam_prefix: the current beam prefix\n",
    "        beam_log_prob: the current beam log probability\n",
    "        token: the next token\n",
    "        prob: the probability of the next token\n",
    "        stop_token: the token that stops the generation\n",
    "    Returns:\n",
    "        new_prefix: the new beam prefix\n",
    "        new_log_prob: the new beam log probability\n",
    "    \"\"\"\n",
    "\n",
    "    # update the prefix and the log probability\n",
    "    new_prefix = beam_prefix + token\n",
    "    new_log_prob = beam_log_prob + np.log(prob)\n",
    "\n",
    "\n",
    "\n",
    "    return new_prefix, new_log_prob\n",
    "\n",
    "def select_next_token(next_token_probabilities, sampling_method, k_greedy=1, temperature=0.5, top_k=5, p=0.9):\n",
    "    \"\"\" Selects the next token using the given sampling method\n",
    "    Args:\n",
    "        next_token_probabilities: the probabilities of the next tokens\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "    Returns:\n",
    "        next_token: the next token\n",
    "    \"\"\"\n",
    "    if sampling_method == 'greedy':\n",
    "        return sample_greedy(next_token_probabilities, k_greedy)\n",
    "    elif sampling_method == 'temperature':\n",
    "        return sample_temperature(next_token_probabilities, temperature)\n",
    "    elif sampling_method == 'topK':\n",
    "        return sample_topK(next_token_probabilities, top_k)\n",
    "    elif sampling_method == 'topP':\n",
    "        return sample_topP(next_token_probabilities, p)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown sampling method: {sampling_method}')\n",
    "\n"
   ],
   "metadata": {
    "id": "3zf-omUXQezz",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.297970Z",
     "end_time": "2023-04-25T12:54:41.318753Z"
    }
   },
   "execution_count": 188,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "all_models = run_match_language_models\n",
    "language = \"en.csv\"\n",
    "add_one = True"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.317704Z",
     "end_time": "2023-04-25T12:54:41.330455Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "# clear the generations, useful if you want to run the code multiple times\n",
    "for example in test_:\n",
    "    test_[example]['generation'] = []\n",
    "\n",
    "# generate the strings for each example\n",
    "for example in test_:\n",
    "    # generate the string for each sampling method\n",
    "    for i in range(len(test_[example]['sampling_method'])):\n",
    "        # get the parameters\n",
    "        sampling_method = test_[example]['sampling_method'][i]\n",
    "        gen_length = int(test_[example]['gen_length'])\n",
    "        stop_token = test_[example]['stop_token']\n",
    "        prefix = test_[example]['start_tokens']\n",
    "\n",
    "        # generate the string\n",
    "        generated_string = generate_string(all_models, prefix=prefix, sampling_method=sampling_method, gen_length=gen_length, stop_token=stop_token)\n",
    "        # cut the start_token from the generated string\n",
    "        generated_string = generated_string[len(prefix):]\n",
    "        # store the string\n",
    "        test_[example]['generation'].append(generated_string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.334476Z",
     "end_time": "2023-04-25T12:54:41.391872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example example1:\n",
      "Start tokens: H\n",
      "Generation using greedy : ouse the s\n",
      "generation length: 10\n",
      "number of tokens: 10\n",
      "Generation using beam : ealthcare \n",
      "generation length: 10\n",
      "number of tokens: 10\n",
      "\n",
      "Example example2:\n",
      "Start tokens: H\n",
      "Generation using temperature : cj4<end>\n",
      "generation length: 8\n",
      "number of tokens: 4\n",
      "Generation using topK : eleasy and\n",
      "generation length: 10\n",
      "number of tokens: 10\n",
      "Generation using topP : APOKE Nico\n",
      "generation length: 10\n",
      "number of tokens: 10\n",
      "\n",
      "Example example3:\n",
      "Start tokens: He\n",
      "Generation using greedy : alth the so much 24t\n",
      "generation length: 20\n",
      "number of tokens: 20\n",
      "Generation using beam : althcare in that the\n",
      "generation length: 20\n",
      "number of tokens: 20\n",
      "Generation using temperature : ckmoot4: #word. FIVE\n",
      "generation length: 20\n",
      "number of tokens: 20\n",
      "Generation using topK :  is we can't https:/\n",
      "generation length: 20\n",
      "number of tokens: 20\n",
      "Generation using topP : ar https://t.co/Dk8b\n",
      "generation length: 20\n",
      "number of tokens: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the generations and the number of tokens\n",
    "\n",
    "# iterate over the examples\n",
    "for example in test_:\n",
    "    print(f\"Example {example}:\")\n",
    "    print(\"Start tokens:\", test_[example]['start_tokens'])\n",
    "    # iterate over the sampling methods\n",
    "    for i in range(len(test_[example]['generation'])):\n",
    "        print(\"Generation using\", test_[example]['sampling_method'][i], \":\", test_[example]['generation'][i])\n",
    "        print(\"generation length:\", len(test_[example]['generation'][i]))\n",
    "        print(\"number of tokens:\", len(tweet_to_token_tuples(test_[example]['generation'][i])))\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.363642Z",
     "end_time": "2023-04-25T12:54:41.391872Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### do not change ###\n",
    "print('-------- NLG --------')\n",
    "\n",
    "for k,v in test_.items():\n",
    "  l = ''.join([f'\\t{sm} >> {v[\"start_tokens\"]}{g}\\n' for sm,g in zip(v['sampling_method'],v['generation'])])\n",
    "  print(f'{k}:')\n",
    "  print(l)"
   ],
   "metadata": {
    "id": "bvla30-lVw8n",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.383356Z",
     "end_time": "2023-04-25T12:54:41.395470Z"
    }
   },
   "execution_count": 192,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- NLG --------\n",
      "example1:\n",
      "\tgreedy >> House the s\n",
      "\tbeam >> Healthcare \n",
      "\n",
      "example2:\n",
      "\ttemperature >> Hcj4<end>\n",
      "\ttopK >> Heleasy and\n",
      "\ttopP >> HAPOKE Nico\n",
      "\n",
      "example3:\n",
      "\tgreedy >> Health the so much 24t\n",
      "\tbeam >> Healthcare in that the\n",
      "\ttemperature >> Heckmoot4: #word. FIVE\n",
      "\ttopK >> He is we can't https:/\n",
      "\ttopP >> Hear https://t.co/Dk8b\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEtckSWNANqW"
   },
   "source": [
    "<br><br><br>\n",
    "# **Good luck!**"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "jmFOMp2FWj-4",
    "ExecuteTime": {
     "start_time": "2023-04-25T12:54:41.395470Z",
     "end_time": "2023-04-25T12:54:41.411816Z"
    }
   },
   "execution_count": 192,
   "outputs": []
  }
 ]
}
