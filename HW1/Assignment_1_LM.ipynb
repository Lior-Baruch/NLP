{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce5pQK3bFn_"
   },
   "source": [
    "# Assignment 1\n",
    "In this assignment you will be creating tools for learning and testing language models.\n",
    "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
    "\n",
    "Do make sure all results are uploaded to CSVs (as well as printed to console) for your assignment to be fully graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwG8v-Ll49KM"
   },
   "source": [
    "*As a preparation for this task, download the data files from the course git repository.\n",
    "\n",
    "The relevant files are under **lm-languages-data-new**:\n",
    "\n",
    "\n",
    "*   en.csv (or the equivalent JSON file)\n",
    "*   es.csv (or the equivalent JSON file)\n",
    "*   fr.csv (or the equivalent JSON file)\n",
    "*   in.csv (or the equivalent JSON file)\n",
    "*   it.csv (or the equivalent JSON file)\n",
    "*   nl.csv (or the equivalent JSON file)\n",
    "*   pt.csv (or the equivalent JSON file)\n",
    "*   tl.csv (or the equivalent JSON file)\n",
    "*   test.csv (or the equivalent JSON file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [],
   "source": [
    "# !pip install numpy pandas emoji\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:47:39.399154Z",
     "end_time": "2023-04-27T13:47:39.494853Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xC-87z2GWMq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b7f40be-bf9b-4d6c-da81-25d60d710a75",
    "ExecuteTime": {
     "start_time": "2023-04-27T13:47:39.412650Z",
     "end_time": "2023-04-27T13:47:39.551984Z"
    }
   },
   "source": [
    "#!git clone https://github.com/kfirbar/nlp-course.git"
   ],
   "execution_count": 268,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOVb4IhsqimJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYdhPfbAGkip",
    "outputId": "af6566c6-e6e6-409a-c569-8fab9bdf400e",
    "ExecuteTime": {
     "start_time": "2023-04-27T13:47:39.427512Z",
     "end_time": "2023-04-27T13:47:39.555985Z"
    }
   },
   "source": [
    "\n",
    "#!ls nlp-course/lm-languages-data-new\n"
   ],
   "execution_count": 269,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [],
   "source": [
    "path = \"nlp-course/lm-languages-data-new\"\n",
    "data_files = [\"en.csv\", \"es.csv\", \"fr.csv\", \"in.csv\", \"it.csv\", \"nl.csv\", \"pt.csv\", \"tl.csv\"]\n",
    "test_file = \"test.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:47:39.446185Z",
     "end_time": "2023-04-27T13:47:39.556983Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ashyu_mT28o6"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def tweet_to_token_tuples(tweet, start_token=\"<start>\", end_token=\"<end>\"):\n",
    "    \"\"\" Converts a tweet to a list of tokens, where each token is an emoji, a character, or a special start/end token.\n",
    "        some emojis are represented by more than one character (e.g. â¡ï¸)\n",
    "    Args:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "    Returns:\n",
    "        token_list: a list of tokens (emojis or characters) (in the order they appear in the tweet)\n",
    "    \"\"\"\n",
    "    token_tuple = []\n",
    "\n",
    "    # remove the start and end tokens from the tweet (if they exist) and return the tweet and the start and end tokens\n",
    "    tweet, start_token, end_token = get_start_end_tokens_and_remove_from_tweet(tweet, start_token, end_token)\n",
    "\n",
    "    # get the start and end index of all the emojis in the tweet\n",
    "    emojis_info = emoji.emoji_list(tweet) # a list of dictionaries, each dictionary contains the start and end index of an emoji in the tweet\n",
    "    emojis_info = {info[\"match_start\"]: info for info in emojis_info}\n",
    "\n",
    "    # add the start token to the token list\n",
    "    if start_token is not None:\n",
    "        token_tuple.append(start_token)\n",
    "\n",
    "    # iterate over all the characters in the tweet\n",
    "    char_index = 0\n",
    "    while char_index < len(tweet):\n",
    "        # if the current character is the start of an emoji, add the emoji to the token list and move the char_index to the end of the emoji\n",
    "        if char_index in emojis_info:\n",
    "            token_tuple.append(emojis_info[char_index][\"emoji\"])\n",
    "            char_index = emojis_info[char_index][\"match_end\"]\n",
    "        else:\n",
    "            token_tuple.append(tweet[char_index])\n",
    "            char_index += 1\n",
    "\n",
    "    # add the end token to the token list\n",
    "    if end_token is not None:\n",
    "        token_tuple.append(end_token)\n",
    "\n",
    "    # convert the token list to a tuple (so it will be hashable)\n",
    "    token_tuple = tuple(token_tuple)\n",
    "    return token_tuple\n",
    "\n",
    "def get_start_end_tokens_and_remove_from_tweet(tweet, start_token, end_token):\n",
    "    \"\"\" Removes the start and end tokens from the tweet (if they exist) and returns the tweet and the start and end tokens\n",
    "    Args:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    Returns:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    \"\"\"\n",
    "    if tweet.startswith(start_token):\n",
    "        tweet = tweet[len(start_token):]\n",
    "        #print(tweet)\n",
    "    else:\n",
    "        start_token = None\n",
    "\n",
    "    if tweet.endswith(end_token):\n",
    "        tweet = tweet[:-len(end_token)]\n",
    "        #print(tweet)\n",
    "    else:\n",
    "        end_token = None\n",
    "\n",
    "    return tweet, start_token, end_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:47:39.464144Z",
     "end_time": "2023-04-27T13:47:39.556983Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start>', '1', ':', ' ', 'â¡ï¸', '.', ' ', '2', ':', ' ', 'ğŸ¤£', 'â¤ï¸', '.', ' ', '3', ':', ' ', 'ğŸ¤£', 'â¤ï¸', 'â¤ï¸', '.', '<end>')\n"
     ]
    }
   ],
   "source": [
    "test_string = \"<start>1: â¡ï¸. 2: ğŸ¤£â¤ï¸. 3: ğŸ¤£â¤ï¸â¤ï¸.<end>\"\n",
    "print(tweet_to_token_tuples(test_string))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:47:39.476143Z",
     "end_time": "2023-04-27T13:47:39.576310Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xCfzsITW8Yaj",
    "ExecuteTime": {
     "start_time": "2023-04-27T13:47:39.494853Z",
     "end_time": "2023-04-27T13:47:39.577420Z"
    }
   },
   "source": [
    "# a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data.\n",
    "# the data in the files are in the form: tweet_id,tweet_text\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" Creates a vocabulary from the data files\n",
    "    Returns:\n",
    "        vocabulary: a list of all the characters that appear in the data files\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    # iterate over all the data files\n",
    "    for file in data_files:\n",
    "        # read the data file\n",
    "        current_data = pd.read_csv(path + \"/\" + file, encoding=\"utf-8\")\n",
    "        # iterate over all the tweets in the data file\n",
    "        for tweet in current_data[\"tweet_text\"]:\n",
    "            # convert the tweet to a list of tokens\n",
    "            tweet = tweet_to_token_tuples(tweet)\n",
    "            # iterate over all the tokens in the tweet\n",
    "            for token in tweet:\n",
    "                # add the token to the vocabulary\n",
    "                vocabulary.add(token)\n",
    "    # sort the vocabulary\n",
    "    vocabulary = sorted(list(vocabulary))\n",
    "    return vocabulary"
   ],
   "execution_count": 273,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['<start>', '<end>', '\\n', '\\r', ' ', '!', '\"', '#', '#ï¸âƒ£', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '0âƒ£', '0ï¸âƒ£', '1', '1âƒ£', '1ï¸âƒ£', '2', '2âƒ£', '3', '3âƒ£', '3ï¸âƒ£', '4', '4âƒ£', '4ï¸âƒ£', '5', '6', '6ï¸âƒ£', '7', '7ï¸âƒ£', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x9d', 'Â¡', 'Â£', 'Â¤', 'Â¥', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', '\\xad', 'Â®', 'Â®ï¸', 'Â¯', 'Â°', 'Â²', 'Â´', 'Â¶', 'Â·', 'Â¸', 'Âº', 'Â»', 'Â½', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã…', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'Ã', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã™', 'Ãš', 'Ãœ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ä—', 'Ä', 'ÄŸ', 'Ä°', 'Ä±', 'Å„', 'Å', 'Å’', 'Å“', 'Å', 'ÅŸ', 'Å ', 'Å¸', 'Æ’', 'Ê”', 'Ê•', 'Ê–', 'Ê°', 'Ê³', 'Ê·', 'Ê¸', 'Ë–', 'Ë˜', 'Ëš', 'Ë›', 'Ë¡', 'Ë¢', 'Ì€', 'Ì', 'Ìƒ', 'Ìˆ', 'Ì¥', 'Ì®', 'Ì¯', 'Íœ', 'Í¡', 'Î”', 'Î˜', 'Î©', 'Ï…', 'Ï‰', 'Ğ', 'Ğ˜', 'Ğœ', 'Ğ', 'Ğ', 'ĞŸ', 'Ğ ', 'Ğ¤', 'Ğ¦', 'Ğ¯', 'Ğ°', 'Ğ±', 'Ğ²', 'Ğ³', 'Ğ´', 'Ğµ', 'Ğ·', 'Ğ¸', 'Ğº', 'Ğ»', 'Ğ¼', 'Ğ½', 'Ğ¾', 'Ğ¿', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ…', 'Ñ‹', 'Ñ', 'Ñ', 'Ñ', 'Ò’', 'Ò¯', 'ØŒ', 'Ø¢', 'Ø¦', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Ø¶', 'Ø·', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'ÛŒ', 'Û¶', 'à¤‚', 'à¤•', 'à¤—', 'à¤ª', 'à¤¬', 'à¤°', 'à¤¸', 'à¤¾', 'à¥‡', 'à¥', 'à·´', 'à¸', 'à¸‚', 'à¸‡', 'à¸ˆ', 'à¸', 'à¸”', 'à¸•', 'à¸–', 'à¸—', 'à¸™', 'à¸š', 'à¸›', 'à¸', 'à¸ ', 'à¸¡', 'à¸¢', 'à¸£', 'à¸¥', 'à¸§', 'à¸¨', 'à¸ª', 'à¸­', 'à¸°', 'à¸±', 'à¸²', 'à¸´', 'à¸µ', 'à¸¸', 'à¸¹', 'à¹€', 'à¹', 'à¹ˆ', 'à¹‰', 'à¹', 'à¹‘', 'àº¶', 'à¼', 'à¼º', 'à¼»', 'à¼¼', 'à¼½', 'á™“', 'á´—', 'á´¬', 'á´°', 'áµƒ', 'áµ‡', 'áµˆ', 'áµ‰', 'áµ', 'áµ', 'áµ’', 'áµ–', 'áµ—', 'áµ˜', 'áµ›', 'á¶œ', 'á¶ ', 'á¶¦', 'á¶°', '\\u2009', '\\u200a', '\\u200b', 'â€“', 'â€”', 'â€•', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€', 'â€ ', 'â€¢', 'â€¦', 'â€°', 'â€²', 'â€¹', 'â€º', 'â€»', 'â€¼', 'â€¼ï¸', 'â€¿', 'â‰', 'â‰ï¸', '\\u2066', '\\u2067', '\\u2069', 'â±', 'â·', 'â¿', 'â‚¬', 'â‚¹', 'â„ƒ', 'â„…', 'â„¢', 'â†', 'â†‘', 'â†’', 'â†“', 'â†”ï¸', 'â†•', 'â†—ï¸', 'â†˜ï¸', 'â†š', 'â†›', 'â†©', 'â†ª', 'â†¯', 'â†º', 'â‡˜', 'â‡¨', 'âˆ€', 'âˆ†', 'âˆ‡', 'âˆš', 'âˆ', 'âˆ´', 'âˆµ', 'â‰¤', 'â‰¥', 'â‰¦', 'â‰§', 'âŠ™', 'â‹…', 'â‹ª', 'â‹­', 'âŒš', 'âŒ›', 'âŒ£', 'â‹', 'â©', 'â°', 'â±', 'â³', 'â¸', 'â‘ ', 'â‘¥', 'â’', 'â’', 'â’', 'â’', 'â’‘', 'â“‚', 'â“‚ï¸', 'â“˜', 'â“™', 'â“¢', 'â“¦', 'â”€', 'â”', 'â”ƒ', 'â”„', 'â”†', 'â”', 'â”“', 'â”—', 'â”›', 'â”³', 'â”»', 'â•‘', 'â•”', 'â•—', 'â•š', 'â•', 'â•¦', 'â•©', 'â•¬', 'â•­', 'â•®', 'â•¯', 'â•°', 'â•±', 'â•²', 'â•´', 'â–ˆ', 'â–Š', 'â–', 'â–’', 'â–”', 'â–•', 'â–™', 'â–', 'â–£', 'â–¦', 'â–ª', 'â–ªï¸', 'â–²', 'â–³', 'â–¶', 'â–¶ï¸', 'â–¸', 'â–º', 'â–¼', 'â–½', 'â–¿', 'â—€', 'â—€ï¸', 'â—„', 'â—†', 'â—‡', 'â—ˆ', 'â—', 'â—', 'â—‘', 'â—•', 'â—¡', 'â—»ï¸', 'â—¼ï¸', 'â—½', 'â—¾', 'â˜€', 'â˜€ï¸', 'â˜', 'â˜ï¸', 'â˜ƒ', 'â˜„ï¸', 'â˜…', 'â˜†', 'â˜‰', 'â˜', 'â˜ï¸', 'â˜‘', 'â˜‘ï¸', 'â˜“', 'â˜”', 'â˜•', 'â˜˜', 'â˜˜ï¸', 'â˜™', 'â˜š', 'â˜›', 'â˜œ', 'â˜', 'â˜ï¸', 'â˜ğŸ½', 'â˜ğŸ¾', 'â˜', 'â˜ ï¸', 'â˜£', 'â˜ª', 'â˜®ï¸', 'â˜¯', 'â˜°', 'â˜¹', 'â˜¹ï¸', 'â˜º', 'â˜ºï¸', 'â˜¼', 'â˜½', 'â˜¾', 'â™€', 'â™‚', 'â™‹', 'â™', 'â™', 'â™', 'â™', 'â™“', 'â™›', 'â™¡', 'â™£', 'â™£ï¸', 'â™¤', 'â™¥', 'â™¥ï¸', 'â™¦', 'â™¦ï¸', 'â™©', 'â™ª', 'â™«', 'â™¬', 'â™¯', 'â™»', 'âš’', 'âš“', 'âš”', 'âš”ï¸', 'âš–ï¸', 'âš˜', 'âšœ', 'âšœï¸', 'âš', 'âš ', 'âš ï¸', 'âš¡', 'âšª', 'âš«', 'âš°', 'âš½', 'âš¾', 'â›„', 'â›…', 'â›ˆ', 'â›“', 'â›”', 'â›©', 'â›ª', 'â›³', 'â›·', 'â›½', 'âœ', 'âœ‚', 'âœ‚ï¸', 'âœƒ', 'âœ…', 'âœˆ', 'âœˆï¸', 'âœ‰', 'âœ‰ï¸', 'âœŠ', 'âœŠğŸ»', 'âœŠğŸ¼', 'âœŠğŸ½', 'âœŠğŸ¾', 'âœŠğŸ¿', 'âœ‹', 'âœ‹ğŸ»', 'âœ‹ğŸ¼', 'âœŒ', 'âœŒï¸', 'âœŒğŸ»', 'âœŒğŸ¼', 'âœŒğŸ½', 'âœŒğŸ¾', 'âœ', 'âœğŸ¼', 'âœğŸ½', 'âœ', 'âœï¸', 'âœ“', 'âœ”', 'âœ”ï¸', 'âœ–', 'âœï¸', 'âœ¡', 'âœ¡ï¸', 'âœ§', 'âœ¨', 'âœ©', 'âœ­', 'âœ°', 'âœ³', 'âœ³ï¸', 'âœ´', 'âœµ', 'âœ¶', 'âœ·', 'âœ¿', 'â€', 'â', 'â„', 'â„ï¸', 'â…', 'âˆ', 'â‹', 'âŒ', 'â', 'â“', 'â”', 'â—', 'â', 'â', 'â£', 'â£ï¸', 'â¤', 'â¤ï¸', 'â¥', 'âŠ', 'â‹', 'âŒ', 'â', 'â', 'â', 'â”', 'â–', 'â—', 'â™', 'â›', 'âœ', 'â', 'âŸ', 'â ', 'â¡', 'â¡ï¸', 'â¢', 'â¤', 'â°', 'â €', 'â¤µ', 'â¤µï¸', 'â¦‘', 'â¦’', 'â¬…', 'â¬…ï¸', 'â¬‡', 'â¬‡ï¸', 'â­', 'â¸„', 'â¸…', '\\u3000', 'ã€', 'ã€‚', 'ã€†', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€', 'ã€', 'ã€', 'ã€', 'ã€‘', 'ã€œ', 'ã€¡', 'ã€°', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ãŒ', 'ã', 'ã', 'ã—', 'ã›', 'ãœ', 'ãŸ', 'ã£', 'ã¥', 'ã¦', 'ã§', 'ã¨', 'ãª', 'ã«', 'ã­', 'ã®', 'ã¯', 'ã²', 'ã¿', 'ã‚€', 'ã‚‡', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚Œ', 'ã‚', 'ã‚’', 'ã‚œ', 'ã‚¤', 'ã‚§', 'ã‚¨', 'ã‚©', 'ã‚ª', 'ã‚«', 'ã‚­', 'ã‚¯', 'ã‚°', 'ã‚³', 'ã‚´', 'ã‚µ', 'ã‚¸', 'ã‚¹', 'ã‚»', 'ã‚¿', 'ãƒ', 'ãƒƒ', 'ãƒ„', 'ãƒ†', 'ãƒˆ', 'ãƒ‰', 'ãƒŠ', 'ãƒ‹', 'ãƒ', 'ãƒ', 'ãƒ‘', 'ãƒ’', 'ãƒ“', 'ãƒ”', 'ãƒ•', 'ãƒ–', 'ãƒ—', 'ãƒ', 'ãƒ ', 'ãƒ¡', 'ãƒ¥', 'ãƒ¦', 'ãƒ§', 'ãƒ©', 'ãƒ¬', 'ãƒ­', 'ãƒ®', 'ãƒ¯', 'ãƒ³', 'ãƒ»', 'ãƒ¼', 'ãƒ½', 'ã……', 'ã…ˆ', 'ã…‹', 'ã…', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£', 'ã…¤', '\\u31ef', 'ä¸–', 'ä¸­', 'ä¸»', 'äº’', 'äºº', 'ä»˜', 'åƒ', 'å„¿', 'å…', 'å…ˆ', 'å…¥', 'å†™', 'åˆ†', 'åˆ©', 'åˆ¶', 'åˆ¹', 'åŠ›', 'åŠª', 'å‹•', 'åˆ', 'å’', 'å—', 'åˆ', 'å‘Ÿ', 'å˜‰', 'å¢—', 'å¥½', 'å§¿', 'å«Œ', 'å­¦', 'å°”', 'å¸Œ', 'å½¡', 'å½±', 'å½¼', 'å¾Œ', 'æ‚ª', 'æ‰‹', 'æŠ•', 'æ‹¶', 'æŒ¨', 'æ’ƒ', 'æ’®', 'æ–‡', 'æ˜ ', 'æ™‚', 'æœˆ', 'æœ', 'æœ¬', 'æ—', 'æŸ±', 'æ¥­', 'æ©Ÿ', 'æ­Œ', 'æ­³', 'æ¯…', 'æ°—', 'æ´²', 'æ´¸', 'ç‹', 'ç”Ÿ', 'ç”¨', 'ç”»', 'ç•Œ', 'ç›¸', 'çœŸ', 'ç¬', 'çŸ¥', 'ç¨¿', 'ç©º', 'ç³Ÿ', 'çµ‚', 'çµ', 'ç¹‹', 'è€…', 'èŠ±', 'èœ', 'è¡Œ', 'è®¸', 'èµ«', 'è¸Š', 'è¾¼', 'é€š', 'é‚£', 'é–“', 'é¢¨', 'é­', 'ê ', 'ê°€', 'ê°“', 'ê°•', 'ê±¸', 'ê²€', 'ê²Œ', 'ê²©', 'ê²°', 'ê²½', 'ê³ ', 'ê³¡', 'ê³¼', 'êµ¬', 'êµ­', 'ê·œ', 'ê·¸', 'ê·¼', 'ê¸ˆ', 'ê¸°', 'ê¹€', 'ê¼¼', 'ë‚˜', 'ë‚¨', 'ë‚´', 'ë„ˆ', 'ë„', 'ë„¤', 'ë„·', 'ë…€', 'ë…„', 'ë…¸', 'ë…¼', 'ëˆ„', 'ëŠ”', 'ëŠ˜', 'ë‹ˆ', 'ë‹¤', 'ë‹¨', 'ë‹¹', 'ëŒ€', 'ë”', 'ë„', 'ë™', 'ë‘', 'ë‘‘', 'ë“€', 'ë“œ', 'ë“±', 'ë””', 'ë¼', 'ë½', 'ë‘', 'ë™', 'ëœ', 'ë¨', 'ëŸ¬', 'ëŸ°', 'ë ˆ', 'ë ›', 'ë¡œ', 'ë¡±', 'ë£Œ', 'ë£°', 'ë£¸', 'ë¦„', 'ë¦‰', 'ë¦¬', 'ë¦¼', 'ë§', 'ë§ˆ', 'ë§', 'ë§¤', 'ë§¨', 'ëª¬', 'ë¬´', 'ë¯¸', 'ë¯¼', 'ë°€', 'ë°”', 'ë°•', 'ë°©', 'ë°±', 'ë±€', 'ë²„', 'ë²…', 'ë²•', 'ë² ', 'ë²¨', 'ë²³', 'ë³´', 'ë³µ', 'ë³¸', 'ë´‰', 'ë·”', 'ë¸Œ', 'ë¸', 'ë¸”', 'ë¹„', 'ë¹…', 'ë¹¼', 'ì‚¬', 'ì‚´', 'ì‚¼', 'ìƒ', 'ìƒ', 'ìƒ¤', 'ìƒµ', 'ì„œ', 'ì„', 'ì„ ', 'ì„±', 'ì„¸', 'ì„¹', 'ì…”', 'ì…˜', 'ì…©', 'ì†Œ', 'ì†¡', 'ìˆ˜', 'ìŠˆ', 'ìŠ¤', 'ìŠ¨', 'ìŠ¬', 'ìŠ¹', 'ì‹œ', 'ì‹', 'ì‹ ', 'ì‹¤', 'ì‹¸', 'ì•„', 'ì•ˆ', 'ì••', 'ì• ', 'ì•¼', 'ì–‘', 'ì–´', 'ì—', 'ì—‘', 'ì—˜', 'ì— ', 'ì—£', 'ì—¬', 'ì—­', 'ì—°', 'ì˜', 'ì˜ˆ', 'ì˜¤', 'ì˜¨', 'ì™€', 'ì™•', 'ì™¸', 'ìš”', 'ìš©', 'ìš°', 'ìš¸', 'ì›Œ', 'ì›', 'ìœ„', 'ìœ ', 'ìœ¤', 'ì˜', 'ì´', 'ì¸', 'ì¼', 'ì„', 'ì˜', 'ì¥', 'ì¬', 'ì­', 'ì „', 'ì •', 'ì œ', 'ì ¤', 'ì¢…', 'ì£¼', 'ì¥”', 'ì¦ˆ', 'ì§€', 'ì§', 'ì§„', 'ì§‘', 'ì©œ', 'ì°Œ', 'ì°°', 'ì±„', 'ì²œ', 'ì² ', 'ì´ˆ', 'ìµœ', 'ì¶”', 'ì¶œ', 'ì¸ ', 'ì¹˜', 'ì¹´', 'ì»¤', 'ì½”', 'ì½˜', 'ì½¤', 'ì¿±', 'í¬', 'í‚¤', 'í‚¹', 'íƒ€', 'íƒ„', 'íƒ‘', 'íƒœ', 'í„°', 'í…', 'í† ', 'í†¡', 'íŠ¸', 'í‹´', 'íŒ', 'íŒŒ', 'íŒ¨', 'í€', 'í¬', 'í’€', 'í”„', 'í”Œ', 'í”¼', 'í•‘', 'í•˜', 'í•œ', 'í•´', 'í•¸', 'í—Œ', 'í—¤', 'í—¨', 'í˜', 'í˜„', 'í˜•', 'í˜¸', 'í™”', 'í™˜', 'í›ˆ', 'í', 'ï·»', 'ï¸', 'ï¸', 'ï¸µ', 'ï¹', 'ï¹ª', 'ï¼', 'ï¼‚', 'ï¼ƒ', 'ï¼ˆ', 'ï¼‰', 'ï¼Š', 'ï¼', 'ï¼“', 'ï¼–', 'ï¼—', 'ï¼˜', 'ï¼Ÿ', 'ï¼ ', 'ï¼¡', 'ï¼¢', 'ï¼£', 'ï¼¤', 'ï¼¥', 'ï¼¦', 'ï¼§', 'ï¼¨', 'ï¼©', 'ï¼«', 'ï¼¬', 'ï¼­', 'ï¼®', 'ï¼¯', 'ï¼°', 'ï¼±', 'ï¼²', 'ï¼³', 'ï¼´', 'ï¼µ', 'ï¼¶', 'ï¼·', 'ï¼¹', 'ï¼»', 'ï¼½', 'ï¼¿', 'ï½€', 'ï½', 'ï½‚', 'ï½ƒ', 'ï½„', 'ï½…', 'ï½‡', 'ï½‰', 'ï½Œ', 'ï½', 'ï½', 'ï½', 'ï½', 'ï½‘', 'ï½’', 'ï½“', 'ï½•', 'ï½–', 'ï½—', 'ï½˜', 'ï½™', 'ï½š', 'ï½œ', 'ï½¡', 'ï½¥', 'ï¾‰', 'ï¿£', 'ï¿¼', 'ï¿½', 'ğŸƒ', 'ğŸ…°', 'ğŸ…±', 'ğŸ…±ï¸', 'ğŸ…¾', 'ğŸ…¾ï¸', 'ğŸ…¿', 'ğŸ†‘', 'ğŸ†’', 'ğŸ†“', 'ğŸ†”', 'ğŸ†•', 'ğŸ†–', 'ğŸ†—', 'ğŸ†˜', 'ğŸ†™', 'ğŸ†š', 'ğŸ‡¦ğŸ‡©', 'ğŸ‡¦ğŸ‡±', 'ğŸ‡¦ğŸ‡·', 'ğŸ‡¦ğŸ‡¹', 'ğŸ‡§ğŸ‡ª', 'ğŸ‡§ğŸ‡´', 'ğŸ‡§ğŸ‡·', 'ğŸ‡§ğŸ‡¸', 'ğŸ‡¨ğŸ‡¦', 'ğŸ‡¨ğŸ‡©', 'ğŸ‡¨ğŸ‡®', 'ğŸ‡¨ğŸ‡±', 'ğŸ‡¨ğŸ‡³', 'ğŸ‡¨ğŸ‡´', 'ğŸ‡¨ğŸ‡º', 'ğŸ‡©ğŸ‡ª', 'ğŸ‡©ğŸ‡´', 'ğŸ‡©ğŸ‡¿', 'ğŸ‡ªğŸ‡­', 'ğŸ‡ªğŸ‡¸', 'ğŸ‡ªğŸ‡º', 'ğŸ‡«', 'ğŸ‡«ğŸ‡·', 'ğŸ‡¬ğŸ‡§', 'ğŸ‡¬ğŸ‡·', 'ğŸ‡­ğŸ‡°', 'ğŸ‡®ğŸ‡ª', 'ğŸ‡®ğŸ‡¹', 'ğŸ‡¯ğŸ‡µ', 'ğŸ‡°ğŸ‡µ', 'ğŸ‡°ğŸ‡·', 'ğŸ‡°ğŸ‡¼', 'ğŸ‡±', 'ğŸ‡±ğŸ‡¨', 'ğŸ‡±ğŸ‡º', 'ğŸ‡²ğŸ‡¦', 'ğŸ‡²ğŸ‡©', 'ğŸ‡²ğŸ‡´', 'ğŸ‡²ğŸ‡½', 'ğŸ‡³ğŸ‡¬', 'ğŸ‡³ğŸ‡±', 'ğŸ‡³ğŸ‡¿', 'ğŸ‡µğŸ‡­', 'ğŸ‡µğŸ‡°', 'ğŸ‡µğŸ‡·', 'ğŸ‡µğŸ‡¸', 'ğŸ‡µğŸ‡¹', 'ğŸ‡·ğŸ‡º', 'ğŸ‡¸ğŸ‡³', 'ğŸ‡¹ğŸ‡³', 'ğŸ‡ºğŸ‡¸', 'ğŸ‡ºğŸ‡¾', 'ğŸ‡»ğŸ‡ª', 'ğŸ‡»ğŸ‡³', 'ğŸ‡¼', 'ğŸˆµ', 'ğŸˆ¶', 'ğŸˆ·', 'ğŸŒ€', 'ğŸŒƒ', 'ğŸŒ„', 'ğŸŒ…', 'ğŸŒ†', 'ğŸŒ‡', 'ğŸŒˆ', 'ğŸŒŠ', 'ğŸŒ‹', 'ğŸŒŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ’', 'ğŸŒ“', 'ğŸŒ—', 'ğŸŒ™', 'ğŸŒš', 'ğŸŒ›', 'ğŸŒœ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒŸ', 'ğŸŒ ', 'ğŸŒ¤', 'ğŸŒ¥', 'ğŸŒ§', 'ğŸŒ¨', 'ğŸŒªï¸', 'ğŸŒ«', 'ğŸŒ¬', 'ğŸŒ®', 'ğŸŒ¯', 'ğŸŒ°', 'ğŸŒ±', 'ğŸŒ²', 'ğŸŒ³', 'ğŸŒ´', 'ğŸŒµ', 'ğŸŒ¶', 'ğŸŒ¶ï¸', 'ğŸŒ·', 'ğŸŒ¸', 'ğŸŒ¹', 'ğŸŒº', 'ğŸŒ»', 'ğŸŒ¼', 'ğŸŒ½', 'ğŸŒ¾', 'ğŸŒ¿', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸƒ', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹', 'ğŸŒ', 'ğŸ', 'ğŸ', 'ğŸ', 'ğŸ‘', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ•', 'ğŸ–', 'ğŸ—', 'ğŸš', 'ğŸ›', 'ğŸœ', 'ğŸ', 'ğŸ', 'ğŸŸ', 'ğŸ£', 'ğŸ¤', 'ğŸ¥', 'ğŸ¦', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ³', 'ğŸ´', 'ğŸµ', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹', 'ğŸ', 'ğŸ’', 'ğŸ“', 'ğŸ—', 'ğŸ™', 'ğŸ™ï¸', 'ğŸ', 'ğŸŸ', 'ğŸŸï¸', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ¤', 'ğŸ¥', 'ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ®', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ²', 'ğŸµ', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ¾', 'ğŸ¿', 'ğŸ€', 'ğŸ', 'ğŸƒ', 'ğŸƒğŸ¼', 'ğŸ„', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸŠğŸ»', 'ğŸŠğŸ»\\u200dâ™€ï¸', 'ğŸ‹ï¸', 'ğŸŒ', 'ğŸŒğŸ¾', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ–', 'ğŸ˜', 'ğŸ™', 'ğŸš', 'ğŸŸ', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ«', 'ğŸ°', 'ğŸ³\\u200dğŸŒˆ', 'ğŸ³ï¸\\u200dğŸŒˆ', 'ğŸ´\\u200dâ˜ ï¸', 'ğŸ¹', 'ğŸ»', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸ„', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ', 'ğŸ', 'ğŸ', 'ğŸ‘', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ•', 'ğŸ–', 'ğŸ˜', 'ğŸ™', 'ğŸš', 'ğŸœ', 'ğŸ', 'ğŸ', 'ğŸŸ', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ£', 'ğŸ¥', 'ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ®', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ²', 'ğŸ³', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ‘€', 'ğŸ‘', 'ğŸ‘‚', 'ğŸ‘„', 'ğŸ‘…', 'ğŸ‘†', 'ğŸ‘†ğŸ»', 'ğŸ‘‡', 'ğŸ‘‡ğŸ»', 'ğŸ‘‡ğŸ¼', 'ğŸ‘‡ğŸ½', 'ğŸ‘‡ğŸ¾', 'ğŸ‘ˆ', 'ğŸ‘ˆğŸ¼', 'ğŸ‘ˆğŸ¾', 'ğŸ‘‰', 'ğŸ‘‰ğŸ»', 'ğŸ‘‰ğŸ¼', 'ğŸ‘‰ğŸ½', 'ğŸ‘Š', 'ğŸ‘ŠğŸ»', 'ğŸ‘ŠğŸ¼', 'ğŸ‘ŠğŸ½', 'ğŸ‘ŠğŸ¾', 'ğŸ‘ŠğŸ¿', 'ğŸ‘‹', 'ğŸ‘‹ğŸ»', 'ğŸ‘‹ğŸ¼', 'ğŸ‘‹ğŸ½', 'ğŸ‘Œ', 'ğŸ‘ŒğŸ»', 'ğŸ‘ŒğŸ¼', 'ğŸ‘ŒğŸ½', 'ğŸ‘ŒğŸ¾', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¾', 'ğŸ‘ğŸ¿', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¾', 'ğŸ‘', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¾', 'ğŸ‘‘', 'ğŸ‘“', 'ğŸ‘•', 'ğŸ‘–', 'ğŸ‘—', 'ğŸ‘™', 'ğŸ‘', 'ğŸ‘Ÿ', 'ğŸ‘ ', 'ğŸ‘¡', 'ğŸ‘£', 'ğŸ‘¤', 'ğŸ‘¥', 'ğŸ‘¦', 'ğŸ‘¦ğŸ»', 'ğŸ‘§', 'ğŸ‘§ğŸ»', 'ğŸ‘§ğŸ¼', 'ğŸ‘¨', 'ğŸ‘¨\\u200dâ¤ï¸\\u200dğŸ‘¨', 'ğŸ‘¨\\u200dğŸŒ¾', 'ğŸ‘¨\\u200dğŸ¤', 'ğŸ‘¨\\u200dğŸ‘©\\u200dğŸ‘§\\u200dğŸ‘¦', 'ğŸ‘¨\\u200dğŸ’»', 'ğŸ‘¨ğŸ»', 'ğŸ‘¨ğŸ»\\u200dâš•ï¸', 'ğŸ‘¨ğŸ»\\u200dğŸ’»', 'ğŸ‘©', 'ğŸ‘©\\u200dâ¤ï¸\\u200dğŸ‘©', 'ğŸ‘©\\u200dğŸŒ¾', 'ğŸ‘©\\u200dğŸ‘§\\u200dğŸ‘§', 'ğŸ‘©ğŸ»\\u200dğŸ¤', 'ğŸ‘©ğŸ¼\\u200dğŸ¨', 'ğŸ‘©ğŸ¼\\u200dğŸ’»', 'ğŸ‘ª', 'ğŸ‘«', 'ğŸ‘­', 'ğŸ‘®', 'ğŸ‘®ğŸ»', 'ğŸ‘¯', 'ğŸ‘°', 'ğŸ‘°ğŸ½', 'ğŸ‘±', 'ğŸ‘±\\u200dâ™€ï¸', 'ğŸ‘±ğŸ»\\u200dâ™€ï¸', 'ğŸ‘²', 'ğŸ‘³', 'ğŸ‘³ğŸ¼', 'ğŸ‘µ', 'ğŸ‘µğŸ¼', 'ğŸ‘¶', 'ğŸ‘¶ğŸ»', 'ğŸ‘¶ğŸ¼', 'ğŸ‘¶ğŸ½', 'ğŸ‘·', 'ğŸ‘¸', 'ğŸ‘¸ğŸ»', 'ğŸ‘¸ğŸ¼', 'ğŸ‘¸ğŸ¾', 'ğŸ‘¹', 'ğŸ‘º', 'ğŸ‘»', 'ğŸ‘¼', 'ğŸ‘¼ğŸ»', 'ğŸ‘¼ğŸ¼', 'ğŸ‘½', 'ğŸ‘¿', 'ğŸ’€', 'ğŸ’', 'ğŸ’ğŸ»', 'ğŸ’ğŸ»\\u200dâ™‚ï¸', 'ğŸ’ğŸ¼', 'ğŸ’ğŸ½', 'ğŸ’ğŸ¾', 'ğŸ’ğŸ¿', 'ğŸ’‚', 'ğŸ’‚ğŸ»', 'ğŸ’‚ğŸ¿', 'ğŸ’ƒ', 'ğŸ’ƒğŸ»', 'ğŸ’ƒğŸ¼', 'ğŸ’ƒğŸ½', 'ğŸ’ƒğŸ¾', 'ğŸ’„', 'ğŸ’…', 'ğŸ’…ğŸ»', 'ğŸ’…ğŸ¼', 'ğŸ’…ğŸ½', 'ğŸ’†', 'ğŸ’†ğŸ»', 'ğŸ’†ğŸ½', 'ğŸ’†ğŸ¾', 'ğŸ’ˆ', 'ğŸ’‰', 'ğŸ’Š', 'ğŸ’‹', 'ğŸ’Œ', 'ğŸ’', 'ğŸ’', 'ğŸ’', 'ğŸ’', 'ğŸ’‘', 'ğŸ’“', 'ğŸ’”', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’˜', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ’œ', 'ğŸ’', 'ğŸ’', 'ğŸ’Ÿ', 'ğŸ’¡', 'ğŸ’¢', 'ğŸ’£', 'ğŸ’¤', 'ğŸ’¥', 'ğŸ’¦', 'ğŸ’§', 'ğŸ’¨', 'ğŸ’©', 'ğŸ’ª', 'ğŸ’ªğŸ»', 'ğŸ’ªğŸ¼', 'ğŸ’ªğŸ½', 'ğŸ’ªğŸ¾', 'ğŸ’«', 'ğŸ’¬', 'ğŸ’­', 'ğŸ’®', 'ğŸ’¯', 'ğŸ’°', 'ğŸ’³', 'ğŸ’µ', 'ğŸ’¶', 'ğŸ’¸', 'ğŸ’»', 'ğŸ’¼', 'ğŸ’½', 'ğŸ’¿', 'ğŸ“€', 'ğŸ“‚', 'ğŸ“…', 'ğŸ“†', 'ğŸ“ˆ', 'ğŸ“Š', 'ğŸ“‹', 'ğŸ“Œ', 'ğŸ“', 'ğŸ“', 'ğŸ““', 'ğŸ“–', 'ğŸ“š', 'ğŸ“›', 'ğŸ“', 'ğŸ“', 'ğŸ“¡', 'ğŸ“¢', 'ğŸ“£', 'ğŸ“¦', 'ğŸ“§', 'ğŸ“©', 'ğŸ“¬', 'ğŸ“¯', 'ğŸ“°', 'ğŸ“±', 'ğŸ“²', 'ğŸ“´', 'ğŸ“·', 'ğŸ“¸', 'ğŸ“¹', 'ğŸ“º', 'ğŸ“»', 'ğŸ“¼', 'ğŸ“½', 'ğŸ“½ï¸', 'ğŸ“¿', 'ğŸ”', 'ğŸ”‚', 'ğŸ”ƒ', 'ğŸ”„', 'ğŸ”…', 'ğŸ”‰', 'ğŸ”Š', 'ğŸ”‹', 'ğŸ”Œ', 'ğŸ”', 'ğŸ”‘', 'ğŸ”’', 'ğŸ”“', 'ğŸ””', 'ğŸ”˜', 'ğŸ”™', 'ğŸ”›', 'ğŸ”œ', 'ğŸ”', 'ğŸ”', 'ğŸ”¥', 'ğŸ”¨', 'ğŸ”©', 'ğŸ”ª', 'ğŸ”«', 'ğŸ”®', 'ğŸ”°', 'ğŸ”±', 'ğŸ”²', 'ğŸ”´', 'ğŸ”µ', 'ğŸ”¶', 'ğŸ”¸', 'ğŸ”¹', 'ğŸ”º', 'ğŸ”»', 'ğŸ”¼', 'ğŸ”½', 'ğŸ•Š', 'ğŸ•Œ', 'ğŸ•', 'ğŸ•’', 'ğŸ•˜', 'ğŸ•›', 'ğŸ•œ', 'ğŸ•Ÿ', 'ğŸ•¤', 'ğŸ•ª', 'ğŸ•¯', 'ğŸ•µ', 'ğŸ•¶', 'ğŸ•·ï¸', 'ğŸ•º', 'ğŸ•ºğŸ»', 'ğŸ•ºğŸ½', 'ğŸ–', 'ğŸ–ï¸', 'ğŸ–ğŸ¼', 'ğŸ–ğŸ½', 'ğŸ–’', 'ğŸ–•', 'ğŸ–•ğŸ»', 'ğŸ–•ğŸ¼', 'ğŸ–•ğŸ½', 'ğŸ–•ğŸ¾', 'ğŸ–•ğŸ¿', 'ğŸ––', 'ğŸ––ğŸ¾', 'ğŸ–¤', 'ğŸ–¥ï¸', 'ğŸ–¼', 'ğŸ—‚', 'ğŸ—“', 'ğŸ—“ï¸', 'ğŸ—', 'ğŸ—', 'ğŸ—¡', 'ğŸ—£', 'ğŸ—£ï¸', 'ğŸ—¨ï¸', 'ğŸ—³', 'ğŸ—»', 'ğŸ—¼', 'ğŸ—½', 'ğŸ—¾', 'ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‡', 'ğŸ˜ˆ', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜Œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜’', 'ğŸ˜“', 'ğŸ˜”', 'ğŸ˜•', 'ğŸ˜–', 'ğŸ˜—', 'ğŸ˜˜', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ˜›', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜Ÿ', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ˜£', 'ğŸ˜¤', 'ğŸ˜¥', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜ª', 'ğŸ˜«', 'ğŸ˜¬', 'ğŸ˜­', 'ğŸ˜®', 'ğŸ˜¯', 'ğŸ˜°', 'ğŸ˜±', 'ğŸ˜²', 'ğŸ˜³', 'ğŸ˜´', 'ğŸ˜µ', 'ğŸ˜¶', 'ğŸ˜·', 'ğŸ˜¸', 'ğŸ˜¹', 'ğŸ˜º', 'ğŸ˜»', 'ğŸ˜¼', 'ğŸ˜½', 'ğŸ˜¿', 'ğŸ™€', 'ğŸ™', 'ğŸ™‚', 'ğŸ™ƒ', 'ğŸ™„', 'ğŸ™…', 'ğŸ™…ğŸ»', 'ğŸ™…ğŸ¼', 'ğŸ™…ğŸ½\\u200dâ™‚ï¸', 'ğŸ™†', 'ğŸ™†\\u200dâ™‚ï¸', 'ğŸ™†ğŸ½', 'ğŸ™‡', 'ğŸ™‡\\u200dâ™€ï¸', 'ğŸ™‡ğŸ»', 'ğŸ™‡ğŸ½', 'ğŸ™ˆ', 'ğŸ™‰', 'ğŸ™Š', 'ğŸ™‹', 'ğŸ™‹ğŸ»', 'ğŸ™‹ğŸ»\\u200dâ™‚ï¸', 'ğŸ™‹ğŸ¼', 'ğŸ™‹ğŸ½', 'ğŸ™Œ', 'ğŸ™ŒğŸ»', 'ğŸ™ŒğŸ¼', 'ğŸ™ŒğŸ½', 'ğŸ™ŒğŸ¾', 'ğŸ™', 'ğŸ™ğŸ»', 'ğŸ™', 'ğŸ™', 'ğŸ™ğŸ»', 'ğŸ™ğŸ¼', 'ğŸ™ğŸ½', 'ğŸ™ğŸ¾', 'ğŸ™ğŸ¿', 'ğŸš€', 'ğŸš', 'ğŸš‡', 'ğŸšˆ', 'ğŸšŒ', 'ğŸš‘', 'ğŸš“', 'ğŸš”', 'ğŸš–', 'ğŸš—', 'ğŸš˜', 'ğŸš™', 'ğŸš¢', 'ğŸš£', 'ğŸš¦', 'ğŸš§', 'ğŸš¨', 'ğŸš©', 'ğŸš«', 'ğŸš¬', 'ğŸš®', 'ğŸš²', 'ğŸš´', 'ğŸš´\\u200dâ™€ï¸', 'ğŸš´ğŸ»', 'ğŸš´ğŸ»\\u200dâ™€ï¸', 'ğŸšµ', 'ğŸšµ\\u200dâ™€ï¸', 'ğŸš¶', 'ğŸš¶\\u200dâ™‚ï¸', 'ğŸš¶ğŸ¾', 'ğŸš»', 'ğŸš¼', 'ğŸš¿', 'ğŸ›€', 'ğŸ›ƒ', 'ğŸ›„', 'ğŸ›©ï¸', 'ğŸ›«', 'ğŸ›¬', 'ğŸ›°', 'ğŸ›³', 'ğŸ›´', 'ğŸ¤', 'ğŸ¤‘', 'ğŸ¤’', 'ğŸ¤“', 'ğŸ¤”', 'ğŸ¤•', 'ğŸ¤–', 'ğŸ¤—', 'ğŸ¤˜', 'ğŸ¤˜ğŸ»', 'ğŸ¤˜ğŸ¼', 'ğŸ¤˜ğŸ½', 'ğŸ¤˜ğŸ¾', 'ğŸ¤™', 'ğŸ¤™ğŸ»', 'ğŸ¤™ğŸ¼', 'ğŸ¤™ğŸ¾', 'ğŸ¤šğŸ»', 'ğŸ¤šğŸ½', 'ğŸ¤›ğŸ½', 'ğŸ¤œğŸ¿', 'ğŸ¤', 'ğŸ¤', 'ğŸ¤ğŸ»', 'ğŸ¤ğŸ¼', 'ğŸ¤ğŸ½', 'ğŸ¤ ', 'ğŸ¤¡', 'ğŸ¤¢', 'ğŸ¤£', 'ğŸ¤¤', 'ğŸ¤¥', 'ğŸ¤¦\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ»\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ»\\u200dâ™‚ï¸', 'ğŸ¤¦ğŸ¼\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ½\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ½\\u200dâ™‚ï¸', 'ğŸ¤¦ğŸ¾\\u200dâ™‚ï¸', 'ğŸ¤§', 'ğŸ¤³ğŸ½', 'ğŸ¤´ğŸ»', 'ğŸ¤·', 'ğŸ¤·\\u200dâ™€ï¸', 'ğŸ¤·\\u200dâ™‚ï¸', 'ğŸ¤·ğŸ»\\u200dâ™€ï¸', 'ğŸ¤·ğŸ»\\u200dâ™‚ï¸', 'ğŸ¤·ğŸ¼\\u200dâ™€ï¸', 'ğŸ¤·ğŸ½\\u200dâ™€ï¸', 'ğŸ¤·ğŸ½\\u200dâ™‚ï¸', 'ğŸ¤·ğŸ¾\\u200dâ™€ï¸', 'ğŸ¤·ğŸ¾\\u200dâ™‚ï¸', 'ğŸ¥€', 'ğŸ¥', 'ğŸ¥‚', 'ğŸ¥ƒ', 'ğŸ¥„', 'ğŸ¥…', 'ğŸ¥‡', 'ğŸ¥Š', 'ğŸ¥', 'ğŸ¥’', 'ğŸ¥”', 'ğŸ¥˜', 'ğŸ¥', 'ğŸ¦€', 'ğŸ¦', 'ğŸ¦ƒ', 'ğŸ¦„', 'ğŸ¦‡', 'ğŸ¦‰', 'ğŸ¦‹', 'ğŸ¦‘', '\\U000fe4e6']\n",
      "vocab size:  2057\n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "vocabulary = preprocess()\n",
    "# add <start> and <end> to the vocabulary\n",
    "vocabulary = [\"<start>\"] + [\"<end>\"] + vocabulary\n",
    "\n",
    "print(\"vocabulary: \", vocabulary)\n",
    "print(\"vocab size: \", len(vocabulary))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:47:39.506126Z",
     "end_time": "2023-04-27T13:47:42.872732Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb2PGj0Yc2TY"
   },
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function `lm` that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
    "\n",
    "{\n",
    "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
    "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
    "}\n",
    "\n",
    "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
    "\n",
    "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [],
   "source": [
    "# helper functions for lm\n",
    "\n",
    "def build_ngram_model(current_data, n):\n",
    "    \"\"\" Builds an n-gram model from the data\n",
    "    Args:\n",
    "        current_data: a pandas dataframe with a column named \"tweet_text\"\n",
    "        n: the n in n-gram\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "    if n == 1:\n",
    "        model = build_1gram_model(current_data, model)\n",
    "    else:\n",
    "        # iterate over all the tweets in the data file\n",
    "        for tweet in current_data[\"tweet_text\"]:\n",
    "            # convert the tweet to a tuple of tokens\n",
    "            tweet = tweet_to_token_tuples(tweet)\n",
    "\n",
    "            # define n_gram, n_minus_1_gram and n_th_token\n",
    "            n_gram = (\"<start>\", ) + tweet[0:n-1]\n",
    "            n_minus_1_gram = n_gram[:-1]\n",
    "            n_th_token = n_gram[-1]\n",
    "\n",
    "            # add the n-gram to the model\n",
    "            model = add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token)\n",
    "\n",
    "            # iterate over all the n-grams in the tweet\n",
    "            for i in range(len(tweet) - n + 1):\n",
    "                # define n_gram, n_minus_1_gram and n_th_token\n",
    "                n_gram = tweet[i:i+n]\n",
    "                n_minus_1_gram = n_gram[:-1]\n",
    "                n_th_token = n_gram[-1]\n",
    "\n",
    "                # add the n-gram to the model\n",
    "                model = add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token)\n",
    "\n",
    "            n_minus_1_gram = tweet[-n+1:]\n",
    "            n_th_token = \"<end>\"\n",
    "            # add the n-gram to the model\n",
    "            model = add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_1gram_model(current_data, model):\n",
    "    \"\"\" Builds an 1-gram model from the data\n",
    "    Args:\n",
    "        current_data: a pandas dataframe with a column named \"tweet_text\"\n",
    "        n: the n in n-gram\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}\n",
    "    \"\"\"\n",
    "    token_counter = {}\n",
    "    # iterate over all the tweets in the data file\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        # convert the tweet to a tuple of tokens\n",
    "        tweet_tokens = tweet_to_token_tuples(tweet)\n",
    "        for token in tweet_tokens:\n",
    "            if token not in token_counter:\n",
    "                token_counter[token] = 0\n",
    "            token_counter[token] += 1\n",
    "    # add the n-gram to the model\n",
    "    model[()] = token_counter\n",
    "    return model\n",
    "\n",
    "def add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token):\n",
    "    \"\"\" Adds a n-gram to the model and counts the n_th token\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "        n_minus_1_gram: a tuple of n-1 tokens\n",
    "        n_th_token: the n_th token\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "    # add the n-gram to the model\n",
    "    if n_minus_1_gram not in model:\n",
    "        model[n_minus_1_gram] = {}\n",
    "    # add the n_th token to the model\n",
    "    if n_th_token not in model[n_minus_1_gram]:\n",
    "        model[n_minus_1_gram][n_th_token] = 0\n",
    "    # count the n_th token, i.e. add 1 to its count\n",
    "    model[n_minus_1_gram][n_th_token] += 1\n",
    "    return model\n",
    "\n",
    "\n",
    "def add_one_smoothing(model, vocabulary):\n",
    "    \"\"\" Adds add_one smoothing to the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # add spacial key <not in model>\n",
    "        model[n_minus_1_gram][\"<notInModel>\"] = 0\n",
    "        # iterate over all the tokens in the vocabulary\n",
    "        for token in vocabulary:\n",
    "            # if the token is not in the model replace it with <notInModel>\n",
    "            if token not in model[n_minus_1_gram]:\n",
    "                token = \"<notInModel>\"\n",
    "\n",
    "            # count the token, i.e. add 1 to its count (add one smoothing)\n",
    "            model[n_minus_1_gram][token] += 1\n",
    "    return model\n",
    "\n",
    "def calculate_probabilities(model):\n",
    "    \"\"\" Calculates the probabilities for the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # get the counts of all the tokens\n",
    "        token_counts = model[n_minus_1_gram].values()\n",
    "        # if model[n_minus_1_gram] has the key <notInModel> get its count\n",
    "        if \"<notInModel>\" in model[n_minus_1_gram]:\n",
    "            token_notInModel_count = model[n_minus_1_gram][\"<notInModel>\"]\n",
    "        else:\n",
    "            token_notInModel_count = 0\n",
    "        # calculate the total count\n",
    "        total_count = sum(token_counts)\n",
    "        # iterate over all the tokens in the model\n",
    "        for token in model[n_minus_1_gram]:\n",
    "            # calculate the probability, i.e. divide the count by the total count\n",
    "            model[n_minus_1_gram][token] /= total_count\n",
    "\n",
    "        model[n_minus_1_gram][\"meta_data\"] = {\"total_count\": total_count, \"<notInModel>_count\": token_notInModel_count}\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:56:39.937556Z",
     "end_time": "2023-04-27T13:56:39.955186Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kMC_u8eQbVvZ",
    "ExecuteTime": {
     "start_time": "2023-04-27T13:56:40.440045Z",
     "end_time": "2023-04-27T13:56:40.465748Z"
    }
   },
   "source": [
    "def lm(n, vocabulary, data_file_path, add_one):\n",
    "    \"\"\" Builds an n-gram model from the given data\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "        data_file_path: the data_file from which we record probabilities for our model\n",
    "        add_one: True/False (use add_one smoothing or not)\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file_path,  encoding=\"utf-8\")\n",
    "    # build the n-gram model\n",
    "    model = build_ngram_model(current_data, n)\n",
    "    if add_one:\n",
    "        # add one smoothing\n",
    "        model = add_one_smoothing(model, vocabulary)\n",
    "    # calculate the probabilities\n",
    "    model = calculate_probabilities(model)\n",
    "    return model"
   ],
   "execution_count": 287,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [],
   "source": [
    "# # call the function for the first data file\n",
    "#lm_model_True = lm(1, vocabulary, path + \"/\" + data_files[0], True)\n",
    "\n",
    "#lm_model_False = lm(1, vocabulary, path + \"/\" + data_files[0], False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:57:54.414142Z",
     "end_time": "2023-04-27T13:57:54.441322Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8TchtI22I3"
   },
   "source": [
    "**Part 3**\n",
    "\n",
    "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "# a function that calculates the perplexity of a model\n",
    "def calculate_perplexity(current_data, n, model):\n",
    "    \"\"\" Calculates the perplexity of a model\n",
    "    Args:\n",
    "        current_data: the data from which we record probabilities for our model\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    Returns:\n",
    "        the perplexity of the model\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0\n",
    "    n_gram_count = 0\n",
    "\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        # convert the tweet to a list of tokens\n",
    "        tweet = tweet_to_token_tuples(tweet)\n",
    "\n",
    "        # add the first n-gram\n",
    "        n_minus_1_gram = (\"<start>\", ) + tweet[:n-2]\n",
    "        n_th_token = tweet[n-1]\n",
    "        # if the n-gram is in the model, add its log probability to the sum and count it\n",
    "        if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "            log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "            n_gram_count += 1\n",
    "        elif n_minus_1_gram in model and \"<notInModel>\" in model[n_minus_1_gram]:\n",
    "            meta_data = model[n_minus_1_gram][\"meta_data\"]\n",
    "            total_count = meta_data[\"total_count\"]\n",
    "            notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "            prob_notInModel = model[n_minus_1_gram][\"<notInModel>\"]\n",
    "            prob = prob_notInModel / notInModel_count\n",
    "\n",
    "            log_prob_sum += -log2(prob)\n",
    "            n_gram_count += 1\n",
    "\n",
    "        # iterate over all the n-grams in the tweet\n",
    "        for i in range(len(tweet) - n + 1):\n",
    "            n_gram = tweet[i:i+n]\n",
    "            n_minus_1_gram = n_gram[:-1]\n",
    "            n_th_token = n_gram[-1]\n",
    "\n",
    "            # if the n-gram is in the model, add its log probability to the sum and count it\n",
    "            if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "                log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "                n_gram_count += 1\n",
    "            elif n_minus_1_gram in model and \"<notInModel>\" in model[n_minus_1_gram]:\n",
    "                meta_data = model[n_minus_1_gram][\"meta_data\"]\n",
    "                total_count = meta_data[\"total_count\"]\n",
    "                notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "                prob_notInModel = model[n_minus_1_gram][\"<notInModel>\"]\n",
    "                prob = prob_notInModel / notInModel_count\n",
    "\n",
    "                log_prob_sum += -log2(prob)\n",
    "                n_gram_count += 1\n",
    "\n",
    "        # add the last n-gram\n",
    "        n_minus_1_gram = tweet[-n+1:]\n",
    "        n_th_token = \"<end>\"\n",
    "        # if the n-gram is in the model, add its log probability to the sum and count it\n",
    "        if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "            log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "            n_gram_count += 1\n",
    "        elif n_minus_1_gram in model and \"<notInModel>\" in model[n_minus_1_gram]:\n",
    "            meta_data = model[n_minus_1_gram][\"meta_data\"]\n",
    "            total_count = meta_data[\"total_count\"]\n",
    "            notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "            prob_notInModel = model[n_minus_1_gram][\"<notInModel>\"]\n",
    "            prob = prob_notInModel / notInModel_count\n",
    "\n",
    "            log_prob_sum += -log2(prob)\n",
    "            n_gram_count += 1\n",
    "\n",
    "    # return infinite perplexity if no n-grams found\n",
    "    if n_gram_count == 0:\n",
    "        return float('inf')\n",
    "\n",
    "    entropy = log_prob_sum / n_gram_count\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:57:56.197092Z",
     "end_time": "2023-04-27T13:57:56.213128Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F0kkMn328-lJ",
    "ExecuteTime": {
     "start_time": "2023-04-27T13:58:09.958856Z",
     "end_time": "2023-04-27T13:58:09.980930Z"
    }
   },
   "source": [
    "def eval(n, model, data_file):\n",
    "    \"\"\" Evaluates the perplexity of a model\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "        data_file: the data file path for which we want to calculate the perplexity\n",
    "    Returns:\n",
    "        the perplexity of the model\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file, encoding=\"utf-8\")\n",
    "    # calculate the perplexity\n",
    "    perplexity = calculate_perplexity(current_data, n, model)\n",
    "    return perplexity"
   ],
   "execution_count": 294,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity True:  inf\n",
      "perplexity False:  inf\n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "perplexity = eval(2, lm_model_True, path + \"/\" + data_files[0])\n",
    "print(\"perplexity True: \", perplexity)\n",
    "perplexity = eval(2, lm_model_False, path + \"/\" + data_files[0])\n",
    "print(\"perplexity False: \", perplexity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:58:10.404058Z",
     "end_time": "2023-04-27T13:58:11.786514Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enGmtLE3921p"
   },
   "source": [
    "**Part 4**\n",
    "\n",
    "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values.\n",
    "\n",
    "Save the dataframe to a CSV with the name format: {student_id_1}\\_...\\_{student_id_n}\\_part4.csv"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "caAxLE9s_fvn",
    "ExecuteTime": {
     "start_time": "2023-04-27T13:58:11.791037Z",
     "end_time": "2023-04-27T13:58:11.807575Z"
    }
   },
   "source": [
    "def match(n, add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Then, calculate the perplexity of all possible pairs.\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        df: a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # create a dataframe\n",
    "    df = pd.DataFrame(columns=data_files, index=data_files)\n",
    "\n",
    "    # create models for every language\n",
    "    models = compute_data_files_models(data_files, n, vocabulary, path, add_one)\n",
    "\n",
    "    # calculate the perplexity of all possible pairs\n",
    "    for lang1 in data_files: # will be the model\n",
    "        # define the model\n",
    "        current_model = models[lang1]\n",
    "        for lang2 in data_files: # will be the data file\n",
    "            # define the data file\n",
    "            current_data_file = path + \"/\" + lang2\n",
    "            # evaluate the model\n",
    "            perplexity = eval(n, current_model, current_data_file)\n",
    "            # save the perplexity to the dataframe\n",
    "            df[lang1][lang2] = perplexity\n",
    "    return df, models # return the dataframe and the models, so that we can use them later\n",
    "\n",
    "def compute_data_files_models(data_files, n, vocabulary, path , add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Args:\n",
    "        data_files: the data files to create models for\n",
    "        n: the n in n-gram\n",
    "        vocabulary: the vocabulary\n",
    "        path: the path to the data files\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for data_file in data_files:\n",
    "        models[data_file] = lm(n, vocabulary, path + \"/\" + data_file, add_one)\n",
    "    return models\n"
   ],
   "execution_count": 296,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe: \n",
      "           en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  22.248408  34.185733  30.216157  31.410126  33.682035   28.77212  \\\n",
      "es.csv  29.216445  20.068307  27.049098  31.101854  26.101069  32.651946   \n",
      "fr.csv  30.665244  30.428988   20.98617  36.398282  31.121401  32.437457   \n",
      "in.csv  31.672853  36.653321  35.175326  22.523729  35.954072  32.307979   \n",
      "it.csv  29.289859  26.183449  28.052424  30.700143  20.585567  32.598952   \n",
      "nl.csv  29.226886  35.222767  31.441997  31.577295  35.364402  21.716157   \n",
      "pt.csv  32.757231  26.531058  29.984988  34.982492  28.610848  35.772499   \n",
      "tl.csv  31.437952  36.823627  37.499098  28.504951  35.601919   34.20112   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  35.489606  29.235357  \n",
      "es.csv  24.894132   30.45191  \n",
      "fr.csv  31.132026  36.096482  \n",
      "in.csv    38.2836  27.415603  \n",
      "it.csv  27.562218  29.441187  \n",
      "nl.csv  36.894215  32.950128  \n",
      "pt.csv  21.272877  34.498428  \n",
      "tl.csv  38.351392  22.825525  \n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "df_part4, models_part4 = match(2, True)\n",
    "print(\"dataframe: \")\n",
    "print(df_part4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:58:12.244257Z",
     "end_time": "2023-04-27T13:59:07.556250Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "outputs": [],
   "source": [
    "# # save the dataframe to a CSV\n",
    "# df_part4.to_csv(\"language_perplexity_part4.csv\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:59:07.557656Z",
     "end_time": "2023-04-27T13:59:07.570197Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waGMwA8H_n17"
   },
   "source": [
    "**Part 5**\n",
    "\n",
    "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another.\n",
    "\n",
    "Load each result to a dataframe and save to a CSV with the name format: \n",
    "\n",
    "for cases with add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_part5.csv\n",
    "\n",
    "For cases without add_one:\n",
    "{student_id_1}\\_...\\_{student_id_n}\\_n1\\_wo\\_addone\\_part5.csv\n",
    "\n",
    "Follow the same format for n2,n3, and n4\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nk32naXyAMdl",
    "ExecuteTime": {
     "start_time": "2023-04-27T13:59:07.571194Z",
     "end_time": "2023-04-27T13:59:07.615276Z"
    }
   },
   "source": [
    "def run_match():\n",
    "    \"\"\" Runs the match function for all the n values and add_one values\n",
    "    Returns:\n",
    "        dataframes: a dictionary of the dataframes, so that we can use them later,\n",
    "                    i.e {n, add_one: dataframe} where dataframe is a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "        language_models_dict: a dictionary of the models, so that we can use them later,\n",
    "                              i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    n_values = [1, 2, 3, 4]\n",
    "    add_one_values = [True, False]\n",
    "    # create dictionaries for the dataframes, key = (n, add_one), value = dataframe\n",
    "    dataframes = {}\n",
    "    # create a dictionary for the language models, key = (n, add_one), value = language models = {language: model}\n",
    "    language_models_dict = {}\n",
    "    # iterate over all the n values\n",
    "    for n in n_values:\n",
    "        # iterate over all the add_one values\n",
    "        for add_one in add_one_values:\n",
    "            # create the dataframe and the language models, using the match function\n",
    "            current_df, current_language_models = match(n, add_one)\n",
    "            print(\"completed n = \" + str(n) + \", add_one = \" + str(add_one) + \"!\")\n",
    "            # add the dataframe to the dictionary\n",
    "            dataframes[(n, add_one)] = current_df\n",
    "            # add the language models to the dictionary\n",
    "            language_models_dict[(n, add_one)] = current_language_models\n",
    "            # save the dataframe to a CSV\n",
    "            if add_one:\n",
    "                current_df.to_csv(\"language_perplexity_n\" + str(n) + \"_part5.csv\")\n",
    "            else:\n",
    "                current_df.to_csv(\"language_perplexity_n\" + str(n) + \"_wo_addone_part5.csv\")\n",
    "    return dataframes, language_models_dict # return the dataframes and the language models, so that we can use them later\n"
   ],
   "execution_count": 299,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed n = 1, add_one = True!\n",
      "completed n = 1, add_one = False!\n",
      "completed n = 2, add_one = True!\n",
      "completed n = 2, add_one = False!\n",
      "completed n = 3, add_one = True!\n",
      "completed n = 3, add_one = False!\n",
      "completed n = 4, add_one = True!\n",
      "completed n = 4, add_one = False!\n"
     ]
    }
   ],
   "source": [
    "run_match_dataframes, run_match_language_models = run_match()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T13:59:07.588619Z",
     "end_time": "2023-04-27T14:08:11.107385Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframes: \n",
      "{(1, True):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv   36.97883  40.344183  39.919648  40.791018  39.775404  39.080576  \\\n",
      "es.csv  39.117828  34.658211  38.029495  41.551793  37.269341  39.001978   \n",
      "fr.csv  41.052901  39.246763  36.003454  45.209961  38.876972  40.189209   \n",
      "in.csv  39.897347  42.016823  42.806054  35.910772  41.816864  39.943185   \n",
      "it.csv  38.964196  38.439151   38.25033  41.577323  36.080772  39.279074   \n",
      "nl.csv  38.105911  39.851316  39.285313  40.200744  39.448152  36.046425   \n",
      "pt.csv  41.474929  37.965584  39.372278  43.727248  39.474348  41.043186   \n",
      "tl.csv  42.941807  45.390176   47.27405  40.888516  44.572879  44.558153   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  40.767984  40.447831  \n",
      "es.csv  35.891097  40.994352  \n",
      "fr.csv  38.999606  45.714265  \n",
      "in.csv  41.353627  37.545341  \n",
      "it.csv  38.848265   41.00097  \n",
      "nl.csv  39.958031  41.095473  \n",
      "pt.csv  35.475826  43.149219  \n",
      "tl.csv  45.281756  39.014428  , (1, False):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv   36.91888  39.742104  39.303438  40.252658  39.073119  38.340234  \\\n",
      "es.csv  37.270373  34.594661  37.561327  37.533237  36.890511  38.129746   \n",
      "fr.csv  38.576527  37.712112  35.946189  43.644595  37.734342  39.568761   \n",
      "in.csv  39.255938  41.282183  42.112517   35.84445  41.022447  39.210703   \n",
      "it.csv  37.307558  37.264296  37.708305  40.088208  36.007654  38.492017   \n",
      "nl.csv   37.75902  39.317399  38.848431   39.75254   38.97664  35.979859   \n",
      "pt.csv  37.842233  35.215631  37.138275  38.916358  36.635688  38.792824   \n",
      "tl.csv  42.366509    44.7003  46.667873  40.345634  43.881536  43.873667   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  40.128553  39.736817  \n",
      "es.csv  35.526538  40.411983  \n",
      "fr.csv  37.906432  43.886726  \n",
      "in.csv  40.609249  36.890109  \n",
      "it.csv  37.543843  39.517918  \n",
      "nl.csv  39.375226  40.639072  \n",
      "pt.csv  35.392275  41.814313  \n",
      "tl.csv  44.517747   38.93218  , (2, True):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  22.248408  34.185733  30.216157  31.410126  33.682035   28.77212  \\\n",
      "es.csv  29.216445  20.068307  27.049098  31.101854  26.101069  32.651946   \n",
      "fr.csv  30.665244  30.428988   20.98617  36.398282  31.121401  32.437457   \n",
      "in.csv  31.672853  36.653321  35.175326  22.523729  35.954072  32.307979   \n",
      "it.csv  29.289859  26.183449  28.052424  30.700143  20.585567  32.598952   \n",
      "nl.csv  29.226886  35.222767  31.441997  31.577295  35.364402  21.716157   \n",
      "pt.csv  32.757231  26.531058  29.984988  34.982492  28.610848  35.772499   \n",
      "tl.csv  31.437952  36.823627  37.499098  28.504951  35.601919   34.20112   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  35.489606  29.235357  \n",
      "es.csv  24.894132   30.45191  \n",
      "fr.csv  31.132026  36.096482  \n",
      "in.csv    38.2836  27.415603  \n",
      "it.csv  27.562218  29.441187  \n",
      "nl.csv  36.894215  32.950128  \n",
      "pt.csv  21.272877  34.498428  \n",
      "tl.csv  38.351392  22.825525  , (2, False):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv   18.75693  27.539228  24.878303  25.445844    27.4473  23.833932  \\\n",
      "es.csv  21.855713  16.795064  20.635272  23.570386  20.203406   24.88387   \n",
      "fr.csv  22.429841  23.019354  17.695885  25.261443   23.82032  24.951356   \n",
      "in.csv  26.178683  29.308283  28.556437  18.673946  28.847628  26.727413   \n",
      "it.csv  23.556727  20.919624  23.078292  24.023618  17.170311  26.344302   \n",
      "nl.csv  24.391358   28.57262  26.020645  25.749314  28.917983  18.399292   \n",
      "pt.csv  23.780375  20.779006  23.017574  25.318392  21.461032  26.664362   \n",
      "tl.csv  25.426681  28.948527  29.738407  22.861179  28.123278  27.510504   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  27.556253  23.733775  \n",
      "es.csv  19.710956  22.764784  \n",
      "fr.csv  23.565468  25.162554  \n",
      "in.csv  29.836825  22.447539  \n",
      "it.csv  21.245004  23.093038  \n",
      "nl.csv    29.1329  26.738064  \n",
      "pt.csv  17.082495  23.999985  \n",
      "tl.csv   29.11257  18.574983  , (3, True):            en.csv      es.csv      fr.csv     in.csv      it.csv     nl.csv   \n",
      "en.csv  30.521669   81.684671   66.459341  72.081472     80.1053  61.382157  \\\n",
      "es.csv  61.466866    28.02159   51.831359  71.206102   51.892183  75.172976   \n",
      "fr.csv  60.454971    64.99659   28.029363  80.443443    66.43646  71.175344   \n",
      "in.csv  87.697192  102.206928   97.094421  35.036287  105.813229  89.008639   \n",
      "it.csv  66.695396   54.216497   61.106801  75.635111   28.648685  82.138939   \n",
      "nl.csv  67.411545    92.39986   77.099981  80.402173   97.271456  30.848308   \n",
      "pt.csv  70.697022   50.872449   64.074038  79.408789   57.363286  85.340525   \n",
      "tl.csv  80.635836  101.715641  101.903355  65.764469   93.594338  88.841613   \n",
      "\n",
      "            pt.csv     tl.csv  \n",
      "en.csv   92.149324  62.480004  \n",
      "es.csv   51.399163  69.901448  \n",
      "fr.csv   75.386425  80.133677  \n",
      "in.csv   117.77002  66.928924  \n",
      "it.csv   64.489061  71.488273  \n",
      "nl.csv  109.429017  86.360404  \n",
      "pt.csv   30.022856  78.061104  \n",
      "tl.csv  108.316654   33.96254  , (3, False):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv   9.044354  15.282312  14.272734   14.73625  15.137754  13.498138  \\\n",
      "es.csv  14.449873   8.674021  13.049967  15.629115  12.254028  15.771715   \n",
      "fr.csv  12.934168  13.211857    8.64769  15.358583  13.559946  13.706211   \n",
      "in.csv  18.273194  17.397721  18.455231  10.164792  17.695734  18.145291   \n",
      "it.csv  15.390192  12.688286   14.38265   15.64579   8.596889  17.065461   \n",
      "nl.csv  14.279845  16.054064  15.384319  15.972773  17.251473   9.185398   \n",
      "pt.csv  15.591489  11.710184  15.005122  16.325042  12.802958  17.193836   \n",
      "tl.csv  16.008774  16.522049  17.339498  13.609511  15.355238  16.315699   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  14.987073  12.484028  \n",
      "es.csv  11.084769  14.122375  \n",
      "fr.csv  13.752872  14.049846  \n",
      "in.csv  17.620391  14.333229  \n",
      "it.csv   13.02045  14.471503  \n",
      "nl.csv  17.213256  15.364592  \n",
      "pt.csv    8.14822  14.625475  \n",
      "tl.csv  15.258595   8.654417  , (4, True):             en.csv      es.csv      fr.csv      in.csv      it.csv   \n",
      "en.csv   72.847083  272.462572  221.468915  249.599391  262.525824  \\\n",
      "es.csv  250.640213   66.740055  182.375398  302.991299  161.626926   \n",
      "fr.csv  195.162431  192.710876   64.108225  295.277533  213.385566   \n",
      "in.csv  359.048159  362.236938    382.7054   92.191281  365.777364   \n",
      "it.csv  266.451215  178.791531  228.598522  305.294724   68.485433   \n",
      "nl.csv  214.533175  273.989317  242.940794   289.94421  299.563189   \n",
      "pt.csv  281.807934  142.529919   231.28663  327.185134  179.708603   \n",
      "tl.csv   283.70745  341.784853  362.597788  215.882077  300.419926   \n",
      "\n",
      "            nl.csv      pt.csv      tl.csv  \n",
      "en.csv  203.759601  306.921905  194.880118  \n",
      "es.csv  285.427619  148.450841  270.817196  \n",
      "fr.csv  222.587384  242.836809  283.981324  \n",
      "in.csv  358.906006  415.533101   248.28918  \n",
      "it.csv  320.700743  223.953208  277.262598  \n",
      "nl.csv   73.231516  349.223738  298.154089  \n",
      "pt.csv  337.783743   71.115302  292.709882  \n",
      "tl.csv  323.888757  354.202515   81.833891  , (4, False):           en.csv    es.csv    fr.csv    in.csv    it.csv    nl.csv    pt.csv   \n",
      "en.csv  4.483196    8.2004  8.007279   7.70137  7.641266  7.769921   7.60685  \\\n",
      "es.csv   9.80938  4.738417   8.75328  9.994932  8.149262  9.143294  7.029518   \n",
      "fr.csv  8.488768  7.961529  4.477069  8.989619  8.043753  8.194154  8.252752   \n",
      "in.csv  9.608732  9.198318  9.509478  5.024408  9.007372  9.589626   8.78601   \n",
      "it.csv  9.041425  8.029467  8.968213  9.499557  4.595224  9.176466  7.952916   \n",
      "nl.csv  8.153166  7.838007  8.101399  8.730806  7.833639  4.532329  7.928893   \n",
      "pt.csv  9.205177  7.025105  8.972975  9.509722   7.94442  9.237863  4.363594   \n",
      "tl.csv  8.203324   8.05161  8.152144   8.04439  7.625808  8.416823  7.489535   \n",
      "\n",
      "          tl.csv  \n",
      "en.csv  6.700533  \n",
      "es.csv  9.011313  \n",
      "fr.csv   8.50649  \n",
      "in.csv  8.914977  \n",
      "it.csv  8.806239  \n",
      "nl.csv  8.265987  \n",
      "pt.csv  8.512109  \n",
      "tl.csv  4.295471  }\n"
     ]
    }
   ],
   "source": [
    "print(\"dataframes: \")\n",
    "print(run_match_dataframes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:08:11.108631Z",
     "end_time": "2023-04-27T14:08:11.150085Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_models_to_pickle(language_models_dict=run_match_language_models, filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Saves the language models to a pickle file\n",
    "    Args:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # save run_match_language_models to a pickle file, so that we can use it later\n",
    "    with open('run_match_language_models.pickle', 'wb') as handle:\n",
    "        pickle.dump(language_models_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "def save_dataframes_to_pickle(dataframes_dict=run_match_dataframes, filename=\"run_match_dataframes.pickle\"):\n",
    "    \"\"\" Saves the dataframes to a pickle file\n",
    "    Args:\n",
    "        dataframes_dict: a dictionary of the dataframes, i.e {n, add_one: dataframe} where dataframe is a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "    \"\"\"\n",
    "    # save run_match_dataframes to a pickle file, so that we can use it later\n",
    "    with open('run_match_dataframes.pickle', 'wb') as handle:\n",
    "        pickle.dump(dataframes_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:08:11.141087Z",
     "end_time": "2023-04-27T14:08:11.714207Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_models_from_pickle(filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Loads the language models from a pickle file\n",
    "    Returns:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # load run_match_language_models from a pickle file\n",
    "    with open('run_match_language_models.pickle', 'rb') as handle:\n",
    "        language_models_dict = pickle.load(handle)\n",
    "    return language_models_dict\n",
    "\n",
    "def load_dataframes_from_pickle(filename=\"run_match_dataframes.pickle\"):\n",
    "    \"\"\" Loads the dataframes from a pickle file\n",
    "    Returns:\n",
    "        dataframes_dict: a dictionary of the dataframes, i.e {n, add_one: dataframe} where dataframe is a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "    \"\"\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:08:11.716212Z",
     "end_time": "2023-04-27T14:08:11.730045Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "outputs": [],
   "source": [
    "# save the language models to a pickle file (remove/put # in front of the next line to save/load the language models)\n",
    "save_models_to_pickle(run_match_language_models, \"run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:08:11.732042Z",
     "end_time": "2023-04-27T14:08:13.261246Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "outputs": [],
   "source": [
    "# load the language models from a pickle file\n",
    "run_match_language_models = load_models_from_pickle(\"run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:08:13.264253Z",
     "end_time": "2023-04-27T14:08:15.759649Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 keys: \n",
      "[('<start>', 'R', 'T'), ('R', 'T', ' '), ('T', ' ', '@'), (' ', '@', 'O'), ('@', 'O', 'N'), ('O', 'N', 'H'), ('N', 'H', 'E'), ('H', 'E', 'R'), ('E', 'R', 'P'), ('R', 'P', 'E')]\n",
      "first key: \n",
      "{' ': 0.6959798994974874, ':': 0.0002955956251847473, '<notInModel>': 0.30372450487732783, 'meta_data': {'total_count': 6766, '<notInModel>_count': 2055}}\n"
     ]
    }
   ],
   "source": [
    "# test the language models dictionary\n",
    "\n",
    "test_n = 4\n",
    "test_add_one = True\n",
    "test_language = \"en.csv\"\n",
    "test_language_models = run_match_language_models[(test_n, test_add_one)][test_language]\n",
    "\n",
    "# print the first 10 keys\n",
    "print(\"10 keys: \")\n",
    "print(list(test_language_models.keys())[:10])\n",
    "\n",
    "# print the first 10 values of the first key\n",
    "print(\"first key: \")\n",
    "first_key = list(test_language_models.keys())[0]\n",
    "print(test_language_models[first_key])"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:08:15.766245Z",
     "end_time": "2023-04-27T14:08:15.774268Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg4h5Cl0q2nR"
   },
   "source": [
    "**Part 6**\n",
    "\n",
    "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
    "\n",
    "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be accepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qD6IRIQLrlZF",
    "ExecuteTime": {
     "start_time": "2023-04-27T14:08:15.775271Z",
     "end_time": "2023-04-27T14:08:15.818320Z"
    }
   },
   "source": [
    "def classify(n=3, add_one=False):\n",
    "    \"\"\" Classifies the test sentences\n",
    "    Returns:\n",
    "        classification_result: a list of tuples, where each tuple contains the tweet_id, the sentence, the true language, and the predicted language\n",
    "    \"\"\"\n",
    "    # we will use the language models from part 5, with n = 3 and add_one = True\n",
    "    language_models = run_match_language_models[(n, add_one)]\n",
    "\n",
    "    # read the test file, tweet_id, tweet_text, label\n",
    "    test_data = pd.read_csv(path + \"/test.csv\",  encoding=\"utf-8\")\n",
    "\n",
    "    # classify the sentences\n",
    "    classification_result = []\n",
    "\n",
    "    # iterate over the rows in the test data\n",
    "    for index, row in test_data.iterrows():\n",
    "        tweet_id = row['tweet_id']\n",
    "        sentence = row['tweet_text']\n",
    "        true_language = row['label']\n",
    "        predicted_language = ''\n",
    "\n",
    "        predicted_language = single_classification(sentence, language_models, n)\n",
    "\n",
    "        # add the result to the classification_result list\n",
    "        classification_result.append((tweet_id, sentence, true_language, predicted_language))\n",
    "\n",
    "    return classification_result\n",
    "\n",
    "\n",
    "def single_classification(sentence, language_models = run_match_language_models[(3, True)], n=3):\n",
    "    \"\"\" Classifies a single sentence\n",
    "    Args:\n",
    "        sentence: the sentence to classify\n",
    "        language_models: the language models to use for classification\n",
    "                         i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    Returns:\n",
    "        predicted_language: the predicted language of the sentence\n",
    "    \"\"\"\n",
    "    predicted_language = ''\n",
    "    min_perplexity = float('inf')\n",
    "    # iterate over the language models\n",
    "    for data_file in data_files:\n",
    "        current_model = language_models[data_file]\n",
    "        # create a temporary DataFrame, with the sentence as the only row\n",
    "        temp_df = pd.DataFrame([sentence], columns=['tweet_text'])\n",
    "        # calculate the perplexity using the temporary DataFrame\n",
    "        current_perplexity = calculate_perplexity(temp_df, n, current_model)\n",
    "\n",
    "        if current_perplexity < min_perplexity:\n",
    "            min_perplexity = current_perplexity\n",
    "            predicted_language = data_file[:-4] # remove the .csv from the end of the file name\n",
    "    return predicted_language\n",
    "\n"
   ],
   "execution_count": 307,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed classification for n = 1, add_one = False\n",
      "completed classification for n = 1, add_one = True\n",
      "completed classification for n = 2, add_one = False\n",
      "completed classification for n = 2, add_one = True\n",
      "completed classification for n = 3, add_one = False\n",
      "completed classification for n = 3, add_one = True\n",
      "completed classification for n = 4, add_one = False\n",
      "completed classification for n = 4, add_one = True\n"
     ]
    }
   ],
   "source": [
    "# classify the test sentences\n",
    "classification_result_n1 = classify(n=1, add_one=False)\n",
    "print(\"completed classification for n = 1, add_one = False\")\n",
    "\n",
    "classification_result_n1_add_one = classify(n=1, add_one=True)\n",
    "print(\"completed classification for n = 1, add_one = True\")\n",
    "\n",
    "classification_result_n2 = classify(n=2, add_one=False)\n",
    "print(\"completed classification for n = 2, add_one = False\")\n",
    "\n",
    "classification_result_n2_add_one = classify(n=2, add_one=True)\n",
    "print(\"completed classification for n = 2, add_one = True\")\n",
    "\n",
    "classification_result_n3 = classify(n=3, add_one=False)\n",
    "print(\"completed classification for n = 3, add_one = False\")\n",
    "\n",
    "classification_result_n3_add_one = classify(n=3, add_one=True)\n",
    "print(\"completed classification for n = 3, add_one = True\")\n",
    "\n",
    "classification_result_n4 = classify(n=4, add_one=False)\n",
    "print(\"completed classification for n = 4, add_one = False\")\n",
    "\n",
    "classification_result_n4_add_one = classify(n=4, add_one=True)\n",
    "print(\"completed classification for n = 4, add_one = True\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:08:15.791283Z",
     "end_time": "2023-04-27T14:10:07.587257Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "outputs": [],
   "source": [
    "def print_classification_result(classification_result):\n",
    "    for result in classification_result_n1:\n",
    "        predicted_language = result[3]\n",
    "        true_language = result[2]\n",
    "        print(\"predicted_language: \" + predicted_language + \", true_language: \" + true_language)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:10:07.588284Z",
     "end_time": "2023-04-27T14:10:07.603487Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "outputs": [],
   "source": [
    "# print(\"classification result for n = 1, add_one = True\")\n",
    "# print_classification_result(classification_result_n1)\n",
    "# print(\"classification result for n = 2, add_one = True\")\n",
    "# print_classification_result(classification_result_n2)\n",
    "# print(\"classification result for n = 3, add_one = True\")\n",
    "# print_classification_result(classification_result_n3)\n",
    "# print(\"classification result for n = 4, add_one = True\")\n",
    "# print_classification_result(classification_result_n4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:10:07.605488Z",
     "end_time": "2023-04-27T14:10:07.636091Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "outputs": [],
   "source": [
    "def get_accuracy(classification_result):\n",
    "    count_correct = 0\n",
    "    for result in classification_result:\n",
    "        predicted_language = result[3]\n",
    "        true_language = result[2]\n",
    "        if predicted_language == true_language:\n",
    "            count_correct += 1\n",
    "    return count_correct / len(classification_result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:10:07.623067Z",
     "end_time": "2023-04-27T14:10:07.637092Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for n = 1, add_one = False: 0.5631953994249281\n",
      "accuracy for n = 2, add_one = False: 0.8516064508063508\n",
      "accuracy for n = 3, add_one = False: 0.8756094511813977\n",
      "accuracy for n = 4, add_one = False: 0.7858482310288786\n",
      "accuracy for n = 1, add_one = True: 0.6647080885110639\n",
      "accuracy for n = 2, add_one = True: 0.8727340917614702\n",
      "accuracy for n = 3, add_one = True: 0.9229903737967246\n",
      "accuracy for n = 4, add_one = True: 0.9108638579822478\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy of the classification\n",
    "print(\"accuracy for n = 1, add_one = False: \" + str(get_accuracy(classification_result_n1)))\n",
    "print(\"accuracy for n = 2, add_one = False: \" + str(get_accuracy(classification_result_n2)))\n",
    "print(\"accuracy for n = 3, add_one = False: \" + str(get_accuracy(classification_result_n3)))\n",
    "print(\"accuracy for n = 4, add_one = False: \" + str(get_accuracy(classification_result_n4)))\n",
    "\n",
    "print(\"accuracy for n = 1, add_one = True: \" + str(get_accuracy(classification_result_n1_add_one)))\n",
    "print(\"accuracy for n = 2, add_one = True: \" + str(get_accuracy(classification_result_n2_add_one)))\n",
    "print(\"accuracy for n = 3, add_one = True: \" + str(get_accuracy(classification_result_n3_add_one)))\n",
    "print(\"accuracy for n = 4, add_one = True: \" + str(get_accuracy(classification_result_n4_add_one)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:10:07.638096Z",
     "end_time": "2023-04-27T14:10:07.651476Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ECmLd3rktZ"
   },
   "source": [
    "**Part 7**\n",
    "\n",
    "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n",
    "\n",
    "Load the results to a CSV (using a DataFrame), with a model_name and f1_score Name it {student_id_1}\\_...\\_{student_id_n}\\_part7.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "  model_name  f1_score\n",
    "0    Model A      0.85\n",
    "1    Model B      0.92\n",
    "2    Model C      0.87\n",
    "3    Model D      0.90\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "KF3ImVdPgAGC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "outputs": [],
   "source": [
    "def map_language_number(classification_result):\n",
    "    #we will use the following dictionary to convert the strings to numbers\n",
    "    language_to_number = {}\n",
    "    # we will use the following dictionary to convert the numbers back to strings\n",
    "    number_to_language = {}\n",
    "\n",
    "    number = 0\n",
    "    # iterate over the classification results\n",
    "    for result in classification_result:\n",
    "        if result[2] not in language_to_number:\n",
    "            language_to_number[result[2]] = number\n",
    "            number_to_language[number] = result[2]\n",
    "            number += 1\n",
    "        if result[3] not in language_to_number:\n",
    "            language_to_number[result[3]] = number\n",
    "            number_to_language[number] = result[3]\n",
    "            number += 1\n",
    "    return language_to_number, number_to_language"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:10:07.652478Z",
     "end_time": "2023-04-27T14:10:07.695306Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VOBO3YQls66r",
    "ExecuteTime": {
     "start_time": "2023-04-27T14:10:07.668861Z",
     "end_time": "2023-04-27T14:10:07.700433Z"
    }
   },
   "source": [
    "import sklearn.metrics as metrics\n",
    "# The f1 score = 2 *(TP / (2TP + FP + FN))\n",
    "def calc_f1(result):\n",
    "    \"\"\" Calculates the f1 score of the classification result\n",
    "    Args:\n",
    "        result: a list of tuples, where each tuple contains the tweet_id, the sentence, the true language, and the predicted language\n",
    "    Returns:\n",
    "        f1_score: the f1 score of the classification result\n",
    "    \"\"\"\n",
    "    # create mappings from language to number and number to language\n",
    "    language_to_number, number_to_language = map_language_number(result)\n",
    "\n",
    "    # create a DataFrame with the results\n",
    "    df = pd.DataFrame(result, columns=['tweet_id', 'tweet_text', 'true_language', 'predicted_language'])\n",
    "    # drop the tweet_id and tweet_text columns\n",
    "    df = df.drop(columns=['tweet_id', 'tweet_text'])\n",
    "    # convert the true_language and predicted_language columns to numbers\n",
    "    df['true_language'] = df['true_language'].apply(lambda x: language_to_number[x])\n",
    "    df['predicted_language'] = df['predicted_language'].apply(lambda x: language_to_number[x])\n",
    "    # calculate the f1 score\n",
    "    f1_score = metrics.f1_score(df['true_language'], df['predicted_language'], average='weighted')\n",
    "    return f1_score"
   ],
   "execution_count": 314,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, add_one = True, f1_score = 0.6654860914834124\n",
      "n = 2, add_one = True, f1_score = 0.8732924241025811\n",
      "n = 3, add_one = True, f1_score = 0.9232531590282077\n",
      "n = 4, add_one = True, f1_score = 0.9112637804795779\n",
      "n = 1, add_one = False, f1_score = 0.5576411944196741\n",
      "n = 2, add_one = False, f1_score = 0.8517550463128027\n",
      "n = 3, add_one = False, f1_score = 0.8761205856696248\n",
      "n = 4, add_one = False, f1_score = 0.7862544739107611\n"
     ]
    }
   ],
   "source": [
    "# calculate the f1 score for each n\n",
    "f1_score_n1_add_one = calc_f1(classification_result_n1_add_one)\n",
    "print(\"n = 1, add_one = True, f1_score = \" + str(f1_score_n1_add_one))\n",
    "\n",
    "f1_score_n2_add_one = calc_f1(classification_result_n2_add_one)\n",
    "print(\"n = 2, add_one = True, f1_score = \" + str(f1_score_n2_add_one))\n",
    "\n",
    "f1_score_n3_add_one = calc_f1(classification_result_n3_add_one)\n",
    "print(\"n = 3, add_one = True, f1_score = \" + str(f1_score_n3_add_one))\n",
    "\n",
    "f1_score_n4_add_one = calc_f1(classification_result_n4_add_one)\n",
    "print(\"n = 4, add_one = True, f1_score = \" + str(f1_score_n4_add_one))\n",
    "\n",
    "f1_score_n1 = calc_f1(classification_result_n1)\n",
    "print(\"n = 1, add_one = False, f1_score = \" + str(f1_score_n1))\n",
    "\n",
    "f1_score_n2 = calc_f1(classification_result_n2)\n",
    "print(\"n = 2, add_one = False, f1_score = \" + str(f1_score_n2))\n",
    "\n",
    "f1_score_n3 = calc_f1(classification_result_n3)\n",
    "print(\"n = 3, add_one = False, f1_score = \" + str(f1_score_n3))\n",
    "\n",
    "f1_score_n4 = calc_f1(classification_result_n4)\n",
    "print(\"n = 4, add_one = False, f1_score = \" + str(f1_score_n4))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:10:07.684754Z",
     "end_time": "2023-04-27T14:10:07.790813Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br><br><br>\n",
    "**Part 8**  \n",
    "Let's use your Language model (dictionary) for generation (NLG).\n",
    "\n",
    "When it comes to sampling from a language model decoder during text generation, there are several different methods that can be used to control the randomness and diversity of the generated text. \n",
    "\n",
    "Some of the most commonly used methods include:\n",
    "\n",
    "> `Greedy sampling`\n",
    "In this method, the model simply selects the word with the highest probability as the next word at each time step. This method can produce fluent text, but it can also lead to repetitive or predictable output.\n",
    "\n",
    "> `Temperature scaling`  \n",
    "Temperature scaling involves scaling the logits output of the language model by a temperature parameter before softmax normalization. This has the effect of smoothing the distribution of probabilities and increasing the probability of lower-probability words, which can lead to more diverse and creative output.\n",
    "\n",
    "> `Top-K sampling`  \n",
    "In this method, the model restricts the sampling to the top-K most likely words at each time step, where K is a predefined hyperparameter. This can generate more diverse output than greedy sampling, while limiting the number of low-probability words that are sampled.\n",
    "\n",
    "> `Nucleus sampling` (also known as top-p sampling)  \n",
    "This method restricts the sampling to the smallest possible set of words whose cumulative probability exceeds a certain threshold, defined by a hyperparameter p. Like top-K sampling, this can generate more diverse output than greedy sampling, while avoiding sampling extremely low probability words.\n",
    "\n",
    "> `Beam search`  \n",
    "Beam search involves maintaining a fixed number k of candidate output sequences at each time step, and then selecting the k most likely sequences based on their probabilities. This can improve the fluency and coherence of the output, but may not produce as much diversity as sampling methods.\n",
    "\n",
    "The choice of sampling method depends on the specific application and desired balance between fluency, diversity, and randomness. Hyperparameters such as temperature, K, p, and beam size can also be tuned to adjust the behavior of the language model during sampling.\n",
    "\n",
    "\n",
    "You may read more about this concept in <a href='https://huggingface.co/blog/how-to-generate#:~:text=pad_token_id%3Dtokenizer.eos_token_id)-,Greedy%20Search,-Greedy%20search%20simply'>this</a> blog post.\n"
   ],
   "metadata": {
    "id": "NfBYgfjADNPL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Please added the needed code for each sampeling method:**"
   ],
   "metadata": {
    "id": "GbReeHtwNWKS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 458,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def softmax(probabilities):\n",
    "    \"\"\" Applies the softmax function to the probabilities\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities (not yet probalities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a dictionary of probabilities (normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    # convert the dictionary to a numpy array\n",
    "    np_probabilities = np.array(list(probabilities.values()))\n",
    "    # apply the softmax function\n",
    "    np_probabilities = np.exp(np_probabilities)\n",
    "    np_probabilities = np_probabilities / np.sum(np_probabilities)\n",
    "    # convert the numpy array back to a dictionary\n",
    "    probabilities = {key: value for key, value in zip(probabilities.keys(), np_probabilities)}\n",
    "    return probabilities\n",
    "\n",
    "def make_prob_1(probabilities):\n",
    "    \"\"\" Makes the sum of the probabilities equal to 1\n",
    "    Args:\n",
    "        probabilities: a list of probabilities (not yet probalities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a list of probabilities (normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    # by dividing each probability by the sum of all probabilities\n",
    "    sum_prob = sum(probabilities)\n",
    "    probabilities = [prob / sum_prob for prob in probabilities]\n",
    "    return probabilities\n",
    "\n",
    "def get_correct_model(all_models=run_match_language_models, prefix=\"h\", language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Gets the correct language model, the correct n-1 prefix, and the probabilities of the next token\n",
    "    Args:\n",
    "        all_models: a dictionary of language models\n",
    "        prefix: the prefix\n",
    "        language: the language\n",
    "        add_one: whether to use add-one smoothing or not\n",
    "    Returns:\n",
    "        correct_model: the correct language model (dictionary), where the keys are the prefixes and the values are the probabilities of the next token\n",
    "        current_n_minus_1_prefix: the correct key for the language model\n",
    "        next_token_probabilities: the probabilities of the next token (dictionary), where the keys are the tokens and the values are the probabilities\n",
    "    \"\"\"\n",
    "    prefix_tokens = tweet_to_token_tuples(prefix)\n",
    "\n",
    "    # we want to use maximum n-gram we can, but not more than max_n\n",
    "    n = min(max_n, len(prefix_tokens) + 1)\n",
    "\n",
    "    # get the n-gram model\n",
    "    correct_model = all_models[(n, add_one)][language]\n",
    "\n",
    "    # get the n-1 tokens of the prefix, i.e. the key for the language model\n",
    "    tuple_key_prefix = tuple(prefix_tokens[-(n - 1):])\n",
    "\n",
    "    # if the prefix is not in the language model, sample a random key from the language model\n",
    "    if tuple_key_prefix not in correct_model:\n",
    "        tuple_key_prefix = random.choice(list(correct_model.keys()))\n",
    "\n",
    "\n",
    "    # get the probabilities of the next token\n",
    "    next_token_probabilities = correct_model[tuple_key_prefix]\n",
    "    return correct_model, tuple_key_prefix, next_token_probabilities\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T14:58:38.821210Z",
     "end_time": "2023-04-27T14:58:38.926740Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_greedy(i_probabilities, k=1):\n",
    "    \"\"\" Samples the next token greedily, i.e. the token with the highest probability\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "    Returns:\n",
    "        max_token: the token with the highest probability\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities = i_probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities['meta_data']\n",
    "    del probabilities['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "    # reduce the probability of the token <notInModel> by the number of times it was sampled\n",
    "    if '<notInModel>' in probabilities:\n",
    "        probabilities['<notInModel>'] /= notInModel_count\n",
    "\n",
    "\n",
    "    # sample the token with the k highest probability\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    # if k is larger than the number of probabilities, set k to the number of probabilities\n",
    "    k = k if len(sorted_probabilities) >= k else len(sorted_probabilities)\n",
    "    # sample the token with the k highest probability\n",
    "    next_token = sorted_probabilities[k - 1][0]\n",
    "\n",
    "    if next_token == '<notInModel>':\n",
    "        tokens_not_in_model_vocab = set(vocabulary) - set(probabilities.keys())\n",
    "        next_token = random.choice(list(tokens_not_in_model_vocab))\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_temperature(i_probabilities, temperature=1.0, k=1):\n",
    "    \"\"\" Samples the next token using temperature sampling\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        temperature: the temperature\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities = i_probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities['meta_data']\n",
    "    del probabilities['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # add all tokens not in the model to the probabilities dictionary\n",
    "    if '<notInModel>' in probabilities:\n",
    "        single_notInModel_prob = probabilities['<notInModel>'] / notInModel_count\n",
    "        tokens_not_in_model_vocab = set(vocabulary) - set(probabilities.keys())\n",
    "        for token in tokens_not_in_model_vocab:\n",
    "            probabilities[token] = single_notInModel_prob\n",
    "        del probabilities['<notInModel>']\n",
    "\n",
    "    # scale the probabilities by the temperature\n",
    "    probabilities = {key: value ** (1 / temperature) for key, value in probabilities.items()}\n",
    "    # softmax the probabilities\n",
    "    probabilities = softmax(probabilities)\n",
    "    # sample from the probabilities dictionary, use the np.random.choice function\n",
    "    np_probabilities = np.array(list(probabilities.values()))\n",
    "    np_tokens = np.array(list(probabilities.keys()))\n",
    "    # convert the tokens (tuple) to a list of strings\n",
    "    np_tokens = [\"\".join(token) for token in np_tokens]\n",
    "    # sample the next token\n",
    "    next_token = np.random.choice(np_tokens, p=np_probabilities)\n",
    "\n",
    "    # return the sampled token\n",
    "    return next_token\n",
    "\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_topK(i_probabilities, k=1):\n",
    "    \"\"\" Samples the next token using top-k sampling, i.e. only the top k tokens are considered\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        k: the number of tokens to consider\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities = i_probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities['meta_data']\n",
    "    del probabilities['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "    # add all tokens not in the model to the probabilities dictionary\n",
    "    if '<notInModel>' in probabilities:\n",
    "        single_notInModel_prob = probabilities['<notInModel>'] / notInModel_count\n",
    "        tokens_not_in_model_vocab = set(vocabulary) - set(probabilities.keys())\n",
    "        for token in tokens_not_in_model_vocab:\n",
    "            probabilities[token] = single_notInModel_prob\n",
    "        del probabilities['<notInModel>']\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    # take the top k\n",
    "    top_k = sorted_probabilities[:k]\n",
    "    # split the top k into tokens and probabilities\n",
    "    top_k_probs = [prob for (token, prob) in top_k]\n",
    "    top_k_tokens = [token for (token, prob) in top_k]\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_k_probs = make_prob_1(top_k_probs)\n",
    "    # sample from the top k tokens\n",
    "    next_token = np.random.choice(top_k_tokens, p=top_k_probs)\n",
    "    return next_token\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_topP(i_probabilities, p=0.9):\n",
    "    \"\"\" Samples the next token using top-p sampling,\n",
    "    i.e. only the tokens with the highest probabilities are considered, until the sum of the probabilities is greater than p\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        p: the threshold\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities = i_probabilities.copy()\n",
    "     # remember meta_data and remove\n",
    "    meta_data = probabilities['meta_data']\n",
    "    del probabilities['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "    # add all tokens not in the model to the probabilities dictionary\n",
    "    if '<notInModel>' in probabilities:\n",
    "        single_notInModel_prob = probabilities['<notInModel>'] / notInModel_count\n",
    "        tokens_not_in_model_vocab = set(vocabulary) - set(probabilities.keys())\n",
    "        for token in tokens_not_in_model_vocab:\n",
    "            probabilities[token] = single_notInModel_prob\n",
    "        del probabilities['<notInModel>']\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    current_sum = 0\n",
    "    top_p_tokens = []\n",
    "    top_p_probs = []\n",
    "    current_index = 0\n",
    "    while current_sum < p:\n",
    "        top_p_tokens.append(sorted_probabilities[current_index][0])\n",
    "        top_p_probs.append(sorted_probabilities[current_index][1])\n",
    "        current_sum += sorted_probabilities[current_index][1]\n",
    "        current_index += 1\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_p_probs = make_prob_1(top_p_probs)\n",
    "    # convert the tokens (tuple) to a list of strings\n",
    "    top_p_tokens = [\"\".join(token) for token in top_p_tokens]\n",
    "    # sample from the top p tokens\n",
    "    next_token = np.random.choice(top_p_tokens, p=top_p_probs)\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def sample_beam(i_probabilities, num_beams = 3):\n",
    "    \"\"\" Samples the next tokens using beam search, i.e., keeps the top num_beams hypotheses at each step.\n",
    "        Helper function for beam_search\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        num_beams: the number of beams to keep, i.e. the number of hypotheses to keep at each step\n",
    "    Returns:\n",
    "        beam_tokens: a list of top num_beams tokens\n",
    "        beam_probs: a list of the corresponding probabilities of the top num_beams tokens\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities = i_probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities['meta_data']\n",
    "    del probabilities['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "    # add all tokens not in the model to the probabilities dictionary\n",
    "    if '<notInModel>' in probabilities:\n",
    "        single_notInModel_prob = probabilities['<notInModel>'] / notInModel_count\n",
    "        tokens_not_in_model_vocab = set(vocabulary) - set(probabilities.keys())\n",
    "        for token in tokens_not_in_model_vocab:\n",
    "            probabilities[token] = single_notInModel_prob\n",
    "        del probabilities['<notInModel>']\n",
    "\n",
    "    sorted_probabilities = sorted(probabilities.items(), key=lambda x: x[1], reverse=True)\n",
    "    top_beams = sorted_probabilities[:num_beams]\n",
    "\n",
    "    beam_tokens = [token for (token, prob) in top_beams]\n",
    "    beam_probs = [prob for (token, prob) in top_beams]\n",
    "\n",
    "    return beam_tokens, beam_probs\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "v4TrLs1kI3fW",
    "ExecuteTime": {
     "start_time": "2023-04-27T15:03:49.416664Z",
     "end_time": "2023-04-27T15:03:49.432155Z"
    }
   },
   "execution_count": 486,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your Language Model to generate each one out of the following examples with the coresponding params.    \n",
    "Notice the 4 core issues: \n",
    "- Starting tokens\n",
    "- Length of the generation\n",
    "- Sampling methond (use all)\n",
    "- Stop Token (if this token is sampled, stop generating)"
   ],
   "metadata": {
    "id": "Giylo6-lI21t"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "test_ = {\n",
    "    'example1' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['greedy','beam'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example2' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['temperature','topK','topP'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example3' : {\n",
    "        'start_tokens' : \"He\",\n",
    "        'sampling_method' : ['greedy','beam','temperature','topK','topP'],\n",
    "        'gen_length' : \"20\",\n",
    "        'stop_token' : \"me\",\n",
    "        'generation' : []\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "id": "9pUOkRtjN1mI",
    "ExecuteTime": {
     "start_time": "2023-04-27T15:03:49.993224Z",
     "end_time": "2023-04-27T15:03:50.006264Z"
    }
   },
   "execution_count": 487,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your LM to generate a string based on the parametes of each examples, and store the generation sequance at the generation list."
   ],
   "metadata": {
    "id": "YTbF-9zKVchQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_string(all_models, prefix, sampling_method, gen_length, stop_token, num_beams=5, add_one=False):\n",
    "    \"\"\" Generates a string using the given language model\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP', 'beam'\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        num_beams: the number of beams to keep (only relevant for beam search)\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    if sampling_method == 'beam':\n",
    "        return beam_search(all_models= all_models, prefix= prefix, gen_length= gen_length, stop_token= stop_token, num_beams= num_beams, add_one= add_one)\n",
    "    else:\n",
    "        return generate_string_not_beam(all_models= all_models, prefix= prefix, sampling_method= sampling_method, gen_length= gen_length, stop_token= stop_token, add_one= add_one)\n",
    "\n",
    "def beam_search(all_models, prefix, gen_length, stop_token, num_beams, language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Generates a string using beam search\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        num_beams: the number of beams to keep (only relevant for beam search)\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    # initialize the beams\n",
    "    beams = [(prefix, 0)]  # (prefix, log_prob)\n",
    "\n",
    "    # generate the string token by token\n",
    "    for _ in range(gen_length):\n",
    "        new_beams = []\n",
    "\n",
    "        # sample the next token for each beam\n",
    "        for beam_prefix, beam_log_prob in beams:\n",
    "            # get the correct language model\n",
    "            current_lm, current_tuple_key_prefix, next_token_probabilities = get_correct_model(all_models=all_models,\n",
    "                                                                                               prefix=beam_prefix, language=language,\n",
    "                                                                                               add_one=add_one, max_n=max_n)\n",
    "\n",
    "            # sample the top num_beams tokens and probabilities\n",
    "            beam_tokens, beam_probs = sample_beam(next_token_probabilities, num_beams)\n",
    "\n",
    "            # update the beams\n",
    "            for token, prob in zip(beam_tokens, beam_probs):\n",
    "                if not (beam_prefix.endswith(stop_token) or beam_prefix.endswith(\"<end>\")):\n",
    "                    new_prefix, new_log_prob = update_beam(beam_prefix, beam_log_prob, token, prob, stop_token)\n",
    "                    new_beams.append((new_prefix, new_log_prob))\n",
    "                else:\n",
    "                    new_beams.append((beam_prefix, beam_log_prob))\n",
    "\n",
    "\n",
    "        # keep the top num_beams beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:num_beams]\n",
    "\n",
    "    # get the best beam\n",
    "    best_beam = beams[0][0]\n",
    "\n",
    "    return best_beam\n",
    "\n",
    "def generate_string_not_beam(all_models, prefix, gen_length, stop_token, sampling_method, language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Samples the next tokens using the given language model\n",
    "    Args:\n",
    "        all_models: where all_models = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    current_prefix = prefix\n",
    "\n",
    "    # generate the string token by token\n",
    "    for _ in range(gen_length):\n",
    "        # get the correct language model\n",
    "        current_lm, current_tuple_key_prefix, next_token_probabilities = get_correct_model(all_models=all_models,\n",
    "                                                                                           prefix=current_prefix, language=language,\n",
    "                                                                                           add_one=add_one, max_n=max_n)\n",
    "\n",
    "        # sample the next token\n",
    "        next_token = select_next_token(next_token_probabilities, sampling_method)\n",
    "\n",
    "        # if next_token == \"<notInModel>\": uniform sample from the group (vocabulary/next_token_probabilities.keys())\n",
    "        if next_token == \"<notInModel>\":\n",
    "            tokens_not_in_model = list(set(vocabulary) - set(next_token_probabilities.keys()))\n",
    "            next_token = random.choice(tokens_not_in_model)\n",
    "\n",
    "        # update the current prefix\n",
    "        current_prefix += next_token\n",
    "\n",
    "        # stop if the stop token was sampled\n",
    "        if current_prefix.endswith(stop_token) or next_token == \"<end>\":\n",
    "            break\n",
    "\n",
    "\n",
    "    return current_prefix\n",
    "\n",
    "def update_beam(beam_prefix, beam_log_prob, token, prob, stop_token):\n",
    "    \"\"\" Updates the beam, i.e. the current prefix and the current log probability\n",
    "    Args:\n",
    "        beam_prefix: the current beam prefix\n",
    "        beam_log_prob: the current beam log probability\n",
    "        token: the next token\n",
    "        prob: the probability of the next token\n",
    "        stop_token: the token that stops the generation\n",
    "    Returns:\n",
    "        new_prefix: the new beam prefix\n",
    "        new_log_prob: the new beam log probability\n",
    "    \"\"\"\n",
    "\n",
    "    # update the prefix and the log probability\n",
    "    new_prefix = beam_prefix + token\n",
    "    new_log_prob = beam_log_prob + np.log(prob)\n",
    "\n",
    "\n",
    "\n",
    "    return new_prefix, new_log_prob\n",
    "\n",
    "def select_next_token(next_token_probabilities, sampling_method, k_greedy=1, temperature=0.5, top_k=5, p=0.1):\n",
    "    \"\"\" Selects the next token using the given sampling method\n",
    "    Args:\n",
    "        next_token_probabilities: the probabilities of the next tokens\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "    Returns:\n",
    "        next_token: the next token\n",
    "    \"\"\"\n",
    "    if sampling_method == 'greedy':\n",
    "        return sample_greedy(next_token_probabilities, k_greedy)\n",
    "    elif sampling_method == 'temperature':\n",
    "        return sample_temperature(next_token_probabilities, temperature)\n",
    "    elif sampling_method == 'topK':\n",
    "        return sample_topK(next_token_probabilities, top_k)\n",
    "    elif sampling_method == 'topP':\n",
    "        return sample_topP(next_token_probabilities, p)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown sampling method: {sampling_method}')\n",
    "\n"
   ],
   "metadata": {
    "id": "3zf-omUXQezz",
    "ExecuteTime": {
     "start_time": "2023-04-27T15:03:50.629295Z",
     "end_time": "2023-04-27T15:03:50.668600Z"
    }
   },
   "execution_count": 488,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 489,
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "all_models = run_match_language_models\n",
    "language = \"en.csv\"\n",
    "add_one = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T15:03:51.014144Z",
     "end_time": "2023-04-27T15:03:51.038028Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 490,
   "outputs": [],
   "source": [
    "# clear the generations, useful if you want to run the code multiple times\n",
    "for example in test_:\n",
    "    test_[example]['generation'] = []\n",
    "\n",
    "# generate the strings for each example\n",
    "for example in test_:\n",
    "    # generate the string for each sampling method\n",
    "    for i in range(len(test_[example]['sampling_method'])):\n",
    "        # get the parameters\n",
    "        sampling_method = test_[example]['sampling_method'][i]\n",
    "        gen_length = int(test_[example]['gen_length'])\n",
    "        stop_token = test_[example]['stop_token']\n",
    "        prefix = test_[example]['start_tokens']\n",
    "\n",
    "        # generate the string\n",
    "        generated_string = generate_string(all_models, prefix=prefix, sampling_method=sampling_method, gen_length=gen_length, stop_token=stop_token, add_one=add_one)\n",
    "        # cut the start_token from the generated string\n",
    "        generated_string = generated_string[len(prefix):]\n",
    "        # store the string\n",
    "        test_[example]['generation'].append(generated_string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T15:03:51.354831Z",
     "end_time": "2023-04-27T15:03:51.387750Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 491,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example example1:\n",
      "Start tokens: H\n",
      "Generation using greedy : ouse the s\n",
      "generation length: 10\n",
      "number of tokens: 10\n",
      "Generation using beam : ealthcare \n",
      "generation length: 10\n",
      "number of tokens: 10\n",
      "\n",
      "Example example2:\n",
      "Start tokens: H\n",
      "Generation using temperature : uffi: Lord\n",
      "generation length: 10\n",
      "number of tokens: 10\n",
      "Generation using topK : ill shite \n",
      "generation length: 10\n",
      "number of tokens: 10\n",
      "Generation using topP : ouse the w\n",
      "generation length: 10\n",
      "number of tokens: 10\n",
      "\n",
      "Example example3:\n",
      "Start tokens: He\n",
      "Generation using greedy : alth the so much 24t\n",
      "generation length: 20\n",
      "number of tokens: 20\n",
      "Generation using beam : althcare in that the\n",
      "generation length: 20\n",
      "number of tokens: 20\n",
      "Generation using temperature : lf! toanavMkV<end>\n",
      "generation length: 18\n",
      "number of tokens: 14\n",
      "Generation using topK : llary https://t.co/V\n",
      "generation length: 20\n",
      "number of tokens: 20\n",
      "Generation using topP : alth the so much 24t\n",
      "generation length: 20\n",
      "number of tokens: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the generations and the number of tokens\n",
    "\n",
    "# iterate over the examples\n",
    "for example in test_:\n",
    "    print(f\"Example {example}:\")\n",
    "    print(\"Start tokens:\", test_[example]['start_tokens'])\n",
    "    # iterate over the sampling methods\n",
    "    for i in range(len(test_[example]['generation'])):\n",
    "        print(\"Generation using\", test_[example]['sampling_method'][i], \":\", test_[example]['generation'][i])\n",
    "        print(\"generation length:\", len(test_[example]['generation'][i]))\n",
    "        print(\"number of tokens:\", len(tweet_to_token_tuples(test_[example]['generation'][i])))\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-27T15:03:52.145227Z",
     "end_time": "2023-04-27T15:03:52.150724Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### do not change ###\n",
    "print('-------- NLG --------')\n",
    "\n",
    "for k,v in test_.items():\n",
    "  l = ''.join([f'\\t{sm} >> {v[\"start_tokens\"]}{g}\\n' for sm,g in zip(v['sampling_method'],v['generation'])])\n",
    "  print(f'{k}:')\n",
    "  print(l)"
   ],
   "metadata": {
    "id": "bvla30-lVw8n",
    "ExecuteTime": {
     "start_time": "2023-04-27T15:03:53.470113Z",
     "end_time": "2023-04-27T15:03:53.519048Z"
    }
   },
   "execution_count": 492,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- NLG --------\n",
      "example1:\n",
      "\tgreedy >> House the s\n",
      "\tbeam >> Healthcare \n",
      "\n",
      "example2:\n",
      "\ttemperature >> Huffi: Lord\n",
      "\ttopK >> Hill shite \n",
      "\ttopP >> House the w\n",
      "\n",
      "example3:\n",
      "\tgreedy >> Health the so much 24t\n",
      "\tbeam >> Healthcare in that the\n",
      "\ttemperature >> Helf! toanavMkV<end>\n",
      "\ttopK >> Hellary https://t.co/V\n",
      "\ttopP >> Health the so much 24t\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEtckSWNANqW"
   },
   "source": [
    "<br><br><br>\n",
    "# **Good luck!**"
   ]
  },
  {
   "cell_type": "code",
   "source": [],
   "metadata": {
    "id": "jmFOMp2FWj-4",
    "ExecuteTime": {
     "start_time": "2023-04-27T14:58:39.218154Z",
     "end_time": "2023-04-27T14:58:39.234274Z"
    }
   },
   "execution_count": 465,
   "outputs": []
  }
 ]
}
