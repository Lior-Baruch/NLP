{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce5pQK3bFn_"
   },
   "source": [
    "# Assignment 1\n",
    "In this assignment you will be creating tools for learning and testing language models.\n",
    "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
    "\n",
    "Do make sure all results are uploaded to CSVs (as well as printed to console) for your assignment to be fully graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwG8v-Ll49KM"
   },
   "source": [
    "*As a preparation for this task, download the data files from the course git repository.\n",
    "\n",
    "The relevant files are under **lm-languages-data-new**:\n",
    "\n",
    "\n",
    "*   en.csv (or the equivalent JSON file)\n",
    "*   es.csv (or the equivalent JSON file)\n",
    "*   fr.csv (or the equivalent JSON file)\n",
    "*   in.csv (or the equivalent JSON file)\n",
    "*   it.csv (or the equivalent JSON file)\n",
    "*   nl.csv (or the equivalent JSON file)\n",
    "*   pt.csv (or the equivalent JSON file)\n",
    "*   tl.csv (or the equivalent JSON file)\n",
    "*   test.csv (or the equivalent JSON file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# !pip install numpy pandas emoji\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:45.998407Z",
     "end_time": "2023-04-28T13:24:46.421714Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xC-87z2GWMq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b7f40be-bf9b-4d6c-da81-25d60d710a75",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:46.423778Z",
     "end_time": "2023-04-28T13:24:46.437641Z"
    }
   },
   "source": [
    "#!git clone https://github.com/kfirbar/nlp-course.git"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOVb4IhsqimJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYdhPfbAGkip",
    "outputId": "af6566c6-e6e6-409a-c569-8fab9bdf400e",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:46.438646Z",
     "end_time": "2023-04-28T13:24:46.461479Z"
    }
   },
   "source": [
    "\n",
    "#!ls nlp-course/lm-languages-data-new\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "student_id_1= \"123456789\" #TODO: your student id here\n",
    "student_id_2= \"327156998\"\n",
    "\n",
    "path = \"nlp-course/lm-languages-data-new\"\n",
    "data_files = [\"en.csv\", \"es.csv\", \"fr.csv\", \"in.csv\", \"it.csv\", \"nl.csv\", \"pt.csv\", \"tl.csv\"]\n",
    "test_file = \"test.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:46.457195Z",
     "end_time": "2023-04-28T13:24:46.468477Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ashyu_mT28o6"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "import emoji\n",
    "\n",
    "def tweet_to_token_tuples(tweet, start_token=\"<start>\", end_token=\"<end>\"):\n",
    "    \"\"\" Converts a tweet to a list of tokens (characters, emojis, and start and end tokens)\n",
    "    Args:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    Returns:\n",
    "        token_tuple: a tuple of tokens (characters, emojis, and start and end tokens)\n",
    "    \"\"\"\n",
    "    token_tuple = []\n",
    "\n",
    "    # remove the start and end tokens from the tweet (if they exist) and return the tweet and the start and end tokens\n",
    "    tweet, start_token, end_token = get_start_end_tokens_and_remove_from_tweet(tweet, start_token, end_token)\n",
    "\n",
    "    # get the start and end index of all the emojis in the tweet\n",
    "    emojis_info = emoji.emoji_list(tweet) # a list of dictionaries, each dictionary contains the start and end index of an emoji in the tweet\n",
    "    emojis_info = {info[\"match_start\"]: info for info in emojis_info}\n",
    "\n",
    "    # add the start token to the token list\n",
    "    if start_token is not None:\n",
    "        token_tuple.append(start_token)\n",
    "\n",
    "    # iterate over all the characters in the tweet\n",
    "    char_index = 0\n",
    "    while char_index < len(tweet):\n",
    "        # if the current character is the start of an emoji, add the emoji to the token list and move the char_index to the end of the emoji\n",
    "        if char_index in emojis_info:\n",
    "            token_tuple.append(emojis_info[char_index][\"emoji\"])\n",
    "            char_index = emojis_info[char_index][\"match_end\"]\n",
    "        else:\n",
    "            token_tuple.append(tweet[char_index])\n",
    "            char_index += 1\n",
    "\n",
    "    # add the end token to the token list\n",
    "    if end_token is not None:\n",
    "        token_tuple.append(end_token)\n",
    "\n",
    "    # convert the token list to a tuple (so it will be hashable)\n",
    "    token_tuple = tuple(token_tuple)\n",
    "    return token_tuple\n",
    "\n",
    "def get_start_end_tokens_and_remove_from_tweet(tweet, start_token, end_token):\n",
    "    \"\"\" Removes the start and end tokens from the tweet (if they exist) and returns the tweet and the start and end tokens\n",
    "    Args:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    Returns:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    \"\"\"\n",
    "    if tweet.startswith(start_token):\n",
    "        tweet = tweet[len(start_token):]\n",
    "        #print(tweet)\n",
    "    else:\n",
    "        start_token = None\n",
    "\n",
    "    if tweet.endswith(end_token):\n",
    "        tweet = tweet[:-len(end_token)]\n",
    "        #print(tweet)\n",
    "    else:\n",
    "        end_token = None\n",
    "\n",
    "    return tweet, start_token, end_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:46.474476Z",
     "end_time": "2023-04-28T13:24:46.484191Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('<start>', '1', ':', ' ', 'â¡ï¸', '.', ' ', '2', ':', ' ', 'ğŸ¤£', 'â¤ï¸', '.', ' ', '3', ':', ' ', 'ğŸ¤£', 'â¤ï¸', 'â¤ï¸', '.', '<end>')\n",
      "number of tokens: 22\n",
      "number of characters: 36\n"
     ]
    }
   ],
   "source": [
    "test_string = \"<start>1: â¡ï¸. 2: ğŸ¤£â¤ï¸. 3: ğŸ¤£â¤ï¸â¤ï¸.<end>\"\n",
    "test_tokens = tweet_to_token_tuples(test_string)\n",
    "print(test_tokens)\n",
    "print(\"number of tokens:\", len(test_tokens))\n",
    "print(\"number of characters:\", len(test_string))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:46.490798Z",
     "end_time": "2023-04-28T13:24:46.527725Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xCfzsITW8Yaj",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:46.501727Z",
     "end_time": "2023-04-28T13:24:46.536608Z"
    }
   },
   "source": [
    "# a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data.\n",
    "# the data in the files are in the form: tweet_id,tweet_text\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" Creates a vocabulary from the data files\n",
    "    Returns:\n",
    "        vocabulary: a list of all the characters that appear in the data files\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    # iterate over all the data files\n",
    "    for file in data_files:\n",
    "        # read the data file\n",
    "        current_data = pd.read_csv(path + \"/\" + file, encoding=\"utf-8\")\n",
    "        # iterate over all the tweets in the data file\n",
    "        for tweet in current_data[\"tweet_text\"]:\n",
    "            # convert the tweet to a list of tokens\n",
    "            tweet = tweet_to_token_tuples(tweet)\n",
    "            # iterate over all the tokens in the tweet\n",
    "            for token in tweet:\n",
    "                # add the token to the vocabulary\n",
    "                vocabulary.add(token)\n",
    "    # sort the vocabulary\n",
    "    vocabulary = sorted(list(vocabulary))\n",
    "    return vocabulary"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocabulary:  ['<start>', '<end>', '\\n', '\\r', ' ', '!', '\"', '#', '#ï¸âƒ£', '$', '%', '&', \"'\", '(', ')', '*', '+', ',', '-', '.', '/', '0', '0âƒ£', '0ï¸âƒ£', '1', '1âƒ£', '1ï¸âƒ£', '2', '2âƒ£', '3', '3âƒ£', '3ï¸âƒ£', '4', '4âƒ£', '4ï¸âƒ£', '5', '6', '6ï¸âƒ£', '7', '7ï¸âƒ£', '8', '9', ':', ';', '<', '=', '>', '?', '@', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '[', '\\\\', ']', '^', '_', '`', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z', '{', '|', '}', '~', '\\x7f', '\\x80', '\\x91', '\\x92', '\\x9d', 'Â¡', 'Â£', 'Â¤', 'Â¥', 'Â§', 'Â¨', 'Â©', 'Âª', 'Â«', '\\xad', 'Â®', 'Â®ï¸', 'Â¯', 'Â°', 'Â²', 'Â´', 'Â¶', 'Â·', 'Â¸', 'Âº', 'Â»', 'Â½', 'Â¿', 'Ã€', 'Ã', 'Ã‚', 'Ãƒ', 'Ã…', 'Ã‡', 'Ãˆ', 'Ã‰', 'ÃŠ', 'Ã‹', 'ÃŒ', 'Ã', 'Ã', 'Ã‘', 'Ã’', 'Ã“', 'Ã”', 'Ã•', 'Ã–', 'Ã—', 'Ã™', 'Ãš', 'Ãœ', 'Ã ', 'Ã¡', 'Ã¢', 'Ã£', 'Ã¤', 'Ã§', 'Ã¨', 'Ã©', 'Ãª', 'Ã«', 'Ã¬', 'Ã­', 'Ã®', 'Ã¯', 'Ã°', 'Ã±', 'Ã²', 'Ã³', 'Ã´', 'Ãµ', 'Ã¶', 'Ã¸', 'Ã¹', 'Ãº', 'Ã»', 'Ã¼', 'Ä—', 'Ä', 'ÄŸ', 'Ä°', 'Ä±', 'Å„', 'Å', 'Å’', 'Å“', 'Å', 'ÅŸ', 'Å ', 'Å¸', 'Æ’', 'Ê”', 'Ê•', 'Ê–', 'Ê°', 'Ê³', 'Ê·', 'Ê¸', 'Ë–', 'Ë˜', 'Ëš', 'Ë›', 'Ë¡', 'Ë¢', 'Ì€', 'Ì', 'Ìƒ', 'Ìˆ', 'Ì¥', 'Ì®', 'Ì¯', 'Íœ', 'Í¡', 'Î”', 'Î˜', 'Î©', 'Ï…', 'Ï‰', 'Ğ', 'Ğ˜', 'Ğœ', 'Ğ', 'Ğ', 'ĞŸ', 'Ğ ', 'Ğ¤', 'Ğ¦', 'Ğ¯', 'Ğ°', 'Ğ±', 'Ğ²', 'Ğ³', 'Ğ´', 'Ğµ', 'Ğ·', 'Ğ¸', 'Ğº', 'Ğ»', 'Ğ¼', 'Ğ½', 'Ğ¾', 'Ğ¿', 'Ñ€', 'Ñ', 'Ñ‚', 'Ñƒ', 'Ñ…', 'Ñ‹', 'Ñ', 'Ñ', 'Ñ', 'Ò’', 'Ò¯', 'ØŒ', 'Ø¢', 'Ø¦', 'Ø§', 'Ø¨', 'Ø©', 'Øª', 'Ø«', 'Ø¬', 'Ø­', 'Ø®', 'Ø¯', 'Ø°', 'Ø±', 'Ø²', 'Ø³', 'Ø´', 'Ø¶', 'Ø·', 'Ø¹', 'Øº', 'Ù', 'Ù‚', 'Ùƒ', 'Ù„', 'Ù…', 'Ù†', 'Ù‡', 'Ùˆ', 'ÙŠ', 'ÛŒ', 'Û¶', 'à¤‚', 'à¤•', 'à¤—', 'à¤ª', 'à¤¬', 'à¤°', 'à¤¸', 'à¤¾', 'à¥‡', 'à¥', 'à·´', 'à¸', 'à¸‚', 'à¸‡', 'à¸ˆ', 'à¸', 'à¸”', 'à¸•', 'à¸–', 'à¸—', 'à¸™', 'à¸š', 'à¸›', 'à¸', 'à¸ ', 'à¸¡', 'à¸¢', 'à¸£', 'à¸¥', 'à¸§', 'à¸¨', 'à¸ª', 'à¸­', 'à¸°', 'à¸±', 'à¸²', 'à¸´', 'à¸µ', 'à¸¸', 'à¸¹', 'à¹€', 'à¹', 'à¹ˆ', 'à¹‰', 'à¹', 'à¹‘', 'àº¶', 'à¼', 'à¼º', 'à¼»', 'à¼¼', 'à¼½', 'á™“', 'á´—', 'á´¬', 'á´°', 'áµƒ', 'áµ‡', 'áµˆ', 'áµ‰', 'áµ', 'áµ', 'áµ’', 'áµ–', 'áµ—', 'áµ˜', 'áµ›', 'á¶œ', 'á¶ ', 'á¶¦', 'á¶°', '\\u2009', '\\u200a', '\\u200b', 'â€“', 'â€”', 'â€•', 'â€˜', 'â€™', 'â€œ', 'â€', 'â€', 'â€ ', 'â€¢', 'â€¦', 'â€°', 'â€²', 'â€¹', 'â€º', 'â€»', 'â€¼', 'â€¼ï¸', 'â€¿', 'â‰', 'â‰ï¸', '\\u2066', '\\u2067', '\\u2069', 'â±', 'â·', 'â¿', 'â‚¬', 'â‚¹', 'â„ƒ', 'â„…', 'â„¢', 'â†', 'â†‘', 'â†’', 'â†“', 'â†”ï¸', 'â†•', 'â†—ï¸', 'â†˜ï¸', 'â†š', 'â†›', 'â†©', 'â†ª', 'â†¯', 'â†º', 'â‡˜', 'â‡¨', 'âˆ€', 'âˆ†', 'âˆ‡', 'âˆš', 'âˆ', 'âˆ´', 'âˆµ', 'â‰¤', 'â‰¥', 'â‰¦', 'â‰§', 'âŠ™', 'â‹…', 'â‹ª', 'â‹­', 'âŒš', 'âŒ›', 'âŒ£', 'â‹', 'â©', 'â°', 'â±', 'â³', 'â¸', 'â‘ ', 'â‘¥', 'â’', 'â’', 'â’', 'â’', 'â’‘', 'â“‚', 'â“‚ï¸', 'â“˜', 'â“™', 'â“¢', 'â“¦', 'â”€', 'â”', 'â”ƒ', 'â”„', 'â”†', 'â”', 'â”“', 'â”—', 'â”›', 'â”³', 'â”»', 'â•‘', 'â•”', 'â•—', 'â•š', 'â•', 'â•¦', 'â•©', 'â•¬', 'â•­', 'â•®', 'â•¯', 'â•°', 'â•±', 'â•²', 'â•´', 'â–ˆ', 'â–Š', 'â–', 'â–’', 'â–”', 'â–•', 'â–™', 'â–', 'â–£', 'â–¦', 'â–ª', 'â–ªï¸', 'â–²', 'â–³', 'â–¶', 'â–¶ï¸', 'â–¸', 'â–º', 'â–¼', 'â–½', 'â–¿', 'â—€', 'â—€ï¸', 'â—„', 'â—†', 'â—‡', 'â—ˆ', 'â—', 'â—', 'â—‘', 'â—•', 'â—¡', 'â—»ï¸', 'â—¼ï¸', 'â—½', 'â—¾', 'â˜€', 'â˜€ï¸', 'â˜', 'â˜ï¸', 'â˜ƒ', 'â˜„ï¸', 'â˜…', 'â˜†', 'â˜‰', 'â˜', 'â˜ï¸', 'â˜‘', 'â˜‘ï¸', 'â˜“', 'â˜”', 'â˜•', 'â˜˜', 'â˜˜ï¸', 'â˜™', 'â˜š', 'â˜›', 'â˜œ', 'â˜', 'â˜ï¸', 'â˜ğŸ½', 'â˜ğŸ¾', 'â˜', 'â˜ ï¸', 'â˜£', 'â˜ª', 'â˜®ï¸', 'â˜¯', 'â˜°', 'â˜¹', 'â˜¹ï¸', 'â˜º', 'â˜ºï¸', 'â˜¼', 'â˜½', 'â˜¾', 'â™€', 'â™‚', 'â™‹', 'â™', 'â™', 'â™', 'â™', 'â™“', 'â™›', 'â™¡', 'â™£', 'â™£ï¸', 'â™¤', 'â™¥', 'â™¥ï¸', 'â™¦', 'â™¦ï¸', 'â™©', 'â™ª', 'â™«', 'â™¬', 'â™¯', 'â™»', 'âš’', 'âš“', 'âš”', 'âš”ï¸', 'âš–ï¸', 'âš˜', 'âšœ', 'âšœï¸', 'âš', 'âš ', 'âš ï¸', 'âš¡', 'âšª', 'âš«', 'âš°', 'âš½', 'âš¾', 'â›„', 'â›…', 'â›ˆ', 'â›“', 'â›”', 'â›©', 'â›ª', 'â›³', 'â›·', 'â›½', 'âœ', 'âœ‚', 'âœ‚ï¸', 'âœƒ', 'âœ…', 'âœˆ', 'âœˆï¸', 'âœ‰', 'âœ‰ï¸', 'âœŠ', 'âœŠğŸ»', 'âœŠğŸ¼', 'âœŠğŸ½', 'âœŠğŸ¾', 'âœŠğŸ¿', 'âœ‹', 'âœ‹ğŸ»', 'âœ‹ğŸ¼', 'âœŒ', 'âœŒï¸', 'âœŒğŸ»', 'âœŒğŸ¼', 'âœŒğŸ½', 'âœŒğŸ¾', 'âœ', 'âœğŸ¼', 'âœğŸ½', 'âœ', 'âœï¸', 'âœ“', 'âœ”', 'âœ”ï¸', 'âœ–', 'âœï¸', 'âœ¡', 'âœ¡ï¸', 'âœ§', 'âœ¨', 'âœ©', 'âœ­', 'âœ°', 'âœ³', 'âœ³ï¸', 'âœ´', 'âœµ', 'âœ¶', 'âœ·', 'âœ¿', 'â€', 'â', 'â„', 'â„ï¸', 'â…', 'âˆ', 'â‹', 'âŒ', 'â', 'â“', 'â”', 'â—', 'â', 'â', 'â£', 'â£ï¸', 'â¤', 'â¤ï¸', 'â¥', 'âŠ', 'â‹', 'âŒ', 'â', 'â', 'â', 'â”', 'â–', 'â—', 'â™', 'â›', 'âœ', 'â', 'âŸ', 'â ', 'â¡', 'â¡ï¸', 'â¢', 'â¤', 'â°', 'â €', 'â¤µ', 'â¤µï¸', 'â¦‘', 'â¦’', 'â¬…', 'â¬…ï¸', 'â¬‡', 'â¬‡ï¸', 'â­', 'â¸„', 'â¸…', '\\u3000', 'ã€', 'ã€‚', 'ã€†', 'ã€Š', 'ã€‹', 'ã€Œ', 'ã€', 'ã€', 'ã€', 'ã€', 'ã€‘', 'ã€œ', 'ã€¡', 'ã€°', 'ã„', 'ã†', 'ãˆ', 'ãŠ', 'ã‹', 'ãŒ', 'ã', 'ã', 'ã—', 'ã›', 'ãœ', 'ãŸ', 'ã£', 'ã¥', 'ã¦', 'ã§', 'ã¨', 'ãª', 'ã«', 'ã­', 'ã®', 'ã¯', 'ã²', 'ã¿', 'ã‚€', 'ã‚‡', 'ã‚‰', 'ã‚Š', 'ã‚‹', 'ã‚Œ', 'ã‚', 'ã‚’', 'ã‚œ', 'ã‚¤', 'ã‚§', 'ã‚¨', 'ã‚©', 'ã‚ª', 'ã‚«', 'ã‚­', 'ã‚¯', 'ã‚°', 'ã‚³', 'ã‚´', 'ã‚µ', 'ã‚¸', 'ã‚¹', 'ã‚»', 'ã‚¿', 'ãƒ', 'ãƒƒ', 'ãƒ„', 'ãƒ†', 'ãƒˆ', 'ãƒ‰', 'ãƒŠ', 'ãƒ‹', 'ãƒ', 'ãƒ', 'ãƒ‘', 'ãƒ’', 'ãƒ“', 'ãƒ”', 'ãƒ•', 'ãƒ–', 'ãƒ—', 'ãƒ', 'ãƒ ', 'ãƒ¡', 'ãƒ¥', 'ãƒ¦', 'ãƒ§', 'ãƒ©', 'ãƒ¬', 'ãƒ­', 'ãƒ®', 'ãƒ¯', 'ãƒ³', 'ãƒ»', 'ãƒ¼', 'ãƒ½', 'ã……', 'ã…ˆ', 'ã…‹', 'ã…', 'ã…œ', 'ã… ', 'ã…¡', 'ã…£', 'ã…¤', '\\u31ef', 'ä¸–', 'ä¸­', 'ä¸»', 'äº’', 'äºº', 'ä»˜', 'åƒ', 'å„¿', 'å…', 'å…ˆ', 'å…¥', 'å†™', 'åˆ†', 'åˆ©', 'åˆ¶', 'åˆ¹', 'åŠ›', 'åŠª', 'å‹•', 'åˆ', 'å’', 'å—', 'åˆ', 'å‘Ÿ', 'å˜‰', 'å¢—', 'å¥½', 'å§¿', 'å«Œ', 'å­¦', 'å°”', 'å¸Œ', 'å½¡', 'å½±', 'å½¼', 'å¾Œ', 'æ‚ª', 'æ‰‹', 'æŠ•', 'æ‹¶', 'æŒ¨', 'æ’ƒ', 'æ’®', 'æ–‡', 'æ˜ ', 'æ™‚', 'æœˆ', 'æœ', 'æœ¬', 'æ—', 'æŸ±', 'æ¥­', 'æ©Ÿ', 'æ­Œ', 'æ­³', 'æ¯…', 'æ°—', 'æ´²', 'æ´¸', 'ç‹', 'ç”Ÿ', 'ç”¨', 'ç”»', 'ç•Œ', 'ç›¸', 'çœŸ', 'ç¬', 'çŸ¥', 'ç¨¿', 'ç©º', 'ç³Ÿ', 'çµ‚', 'çµ', 'ç¹‹', 'è€…', 'èŠ±', 'èœ', 'è¡Œ', 'è®¸', 'èµ«', 'è¸Š', 'è¾¼', 'é€š', 'é‚£', 'é–“', 'é¢¨', 'é­', 'ê ', 'ê°€', 'ê°“', 'ê°•', 'ê±¸', 'ê²€', 'ê²Œ', 'ê²©', 'ê²°', 'ê²½', 'ê³ ', 'ê³¡', 'ê³¼', 'êµ¬', 'êµ­', 'ê·œ', 'ê·¸', 'ê·¼', 'ê¸ˆ', 'ê¸°', 'ê¹€', 'ê¼¼', 'ë‚˜', 'ë‚¨', 'ë‚´', 'ë„ˆ', 'ë„', 'ë„¤', 'ë„·', 'ë…€', 'ë…„', 'ë…¸', 'ë…¼', 'ëˆ„', 'ëŠ”', 'ëŠ˜', 'ë‹ˆ', 'ë‹¤', 'ë‹¨', 'ë‹¹', 'ëŒ€', 'ë”', 'ë„', 'ë™', 'ë‘', 'ë‘‘', 'ë“€', 'ë“œ', 'ë“±', 'ë””', 'ë¼', 'ë½', 'ë‘', 'ë™', 'ëœ', 'ë¨', 'ëŸ¬', 'ëŸ°', 'ë ˆ', 'ë ›', 'ë¡œ', 'ë¡±', 'ë£Œ', 'ë£°', 'ë£¸', 'ë¦„', 'ë¦‰', 'ë¦¬', 'ë¦¼', 'ë§', 'ë§ˆ', 'ë§', 'ë§¤', 'ë§¨', 'ëª¬', 'ë¬´', 'ë¯¸', 'ë¯¼', 'ë°€', 'ë°”', 'ë°•', 'ë°©', 'ë°±', 'ë±€', 'ë²„', 'ë²…', 'ë²•', 'ë² ', 'ë²¨', 'ë²³', 'ë³´', 'ë³µ', 'ë³¸', 'ë´‰', 'ë·”', 'ë¸Œ', 'ë¸', 'ë¸”', 'ë¹„', 'ë¹…', 'ë¹¼', 'ì‚¬', 'ì‚´', 'ì‚¼', 'ìƒ', 'ìƒ', 'ìƒ¤', 'ìƒµ', 'ì„œ', 'ì„', 'ì„ ', 'ì„±', 'ì„¸', 'ì„¹', 'ì…”', 'ì…˜', 'ì…©', 'ì†Œ', 'ì†¡', 'ìˆ˜', 'ìŠˆ', 'ìŠ¤', 'ìŠ¨', 'ìŠ¬', 'ìŠ¹', 'ì‹œ', 'ì‹', 'ì‹ ', 'ì‹¤', 'ì‹¸', 'ì•„', 'ì•ˆ', 'ì••', 'ì• ', 'ì•¼', 'ì–‘', 'ì–´', 'ì—', 'ì—‘', 'ì—˜', 'ì— ', 'ì—£', 'ì—¬', 'ì—­', 'ì—°', 'ì˜', 'ì˜ˆ', 'ì˜¤', 'ì˜¨', 'ì™€', 'ì™•', 'ì™¸', 'ìš”', 'ìš©', 'ìš°', 'ìš¸', 'ì›Œ', 'ì›', 'ìœ„', 'ìœ ', 'ìœ¤', 'ì˜', 'ì´', 'ì¸', 'ì¼', 'ì„', 'ì˜', 'ì¥', 'ì¬', 'ì­', 'ì „', 'ì •', 'ì œ', 'ì ¤', 'ì¢…', 'ì£¼', 'ì¥”', 'ì¦ˆ', 'ì§€', 'ì§', 'ì§„', 'ì§‘', 'ì©œ', 'ì°Œ', 'ì°°', 'ì±„', 'ì²œ', 'ì² ', 'ì´ˆ', 'ìµœ', 'ì¶”', 'ì¶œ', 'ì¸ ', 'ì¹˜', 'ì¹´', 'ì»¤', 'ì½”', 'ì½˜', 'ì½¤', 'ì¿±', 'í¬', 'í‚¤', 'í‚¹', 'íƒ€', 'íƒ„', 'íƒ‘', 'íƒœ', 'í„°', 'í…', 'í† ', 'í†¡', 'íŠ¸', 'í‹´', 'íŒ', 'íŒŒ', 'íŒ¨', 'í€', 'í¬', 'í’€', 'í”„', 'í”Œ', 'í”¼', 'í•‘', 'í•˜', 'í•œ', 'í•´', 'í•¸', 'í—Œ', 'í—¤', 'í—¨', 'í˜', 'í˜„', 'í˜•', 'í˜¸', 'í™”', 'í™˜', 'í›ˆ', 'í', 'ï·»', 'ï¸', 'ï¸', 'ï¸µ', 'ï¹', 'ï¹ª', 'ï¼', 'ï¼‚', 'ï¼ƒ', 'ï¼ˆ', 'ï¼‰', 'ï¼Š', 'ï¼', 'ï¼“', 'ï¼–', 'ï¼—', 'ï¼˜', 'ï¼Ÿ', 'ï¼ ', 'ï¼¡', 'ï¼¢', 'ï¼£', 'ï¼¤', 'ï¼¥', 'ï¼¦', 'ï¼§', 'ï¼¨', 'ï¼©', 'ï¼«', 'ï¼¬', 'ï¼­', 'ï¼®', 'ï¼¯', 'ï¼°', 'ï¼±', 'ï¼²', 'ï¼³', 'ï¼´', 'ï¼µ', 'ï¼¶', 'ï¼·', 'ï¼¹', 'ï¼»', 'ï¼½', 'ï¼¿', 'ï½€', 'ï½', 'ï½‚', 'ï½ƒ', 'ï½„', 'ï½…', 'ï½‡', 'ï½‰', 'ï½Œ', 'ï½', 'ï½', 'ï½', 'ï½', 'ï½‘', 'ï½’', 'ï½“', 'ï½•', 'ï½–', 'ï½—', 'ï½˜', 'ï½™', 'ï½š', 'ï½œ', 'ï½¡', 'ï½¥', 'ï¾‰', 'ï¿£', 'ï¿¼', 'ï¿½', 'ğŸƒ', 'ğŸ…°', 'ğŸ…±', 'ğŸ…±ï¸', 'ğŸ…¾', 'ğŸ…¾ï¸', 'ğŸ…¿', 'ğŸ†‘', 'ğŸ†’', 'ğŸ†“', 'ğŸ†”', 'ğŸ†•', 'ğŸ†–', 'ğŸ†—', 'ğŸ†˜', 'ğŸ†™', 'ğŸ†š', 'ğŸ‡¦ğŸ‡©', 'ğŸ‡¦ğŸ‡±', 'ğŸ‡¦ğŸ‡·', 'ğŸ‡¦ğŸ‡¹', 'ğŸ‡§ğŸ‡ª', 'ğŸ‡§ğŸ‡´', 'ğŸ‡§ğŸ‡·', 'ğŸ‡§ğŸ‡¸', 'ğŸ‡¨ğŸ‡¦', 'ğŸ‡¨ğŸ‡©', 'ğŸ‡¨ğŸ‡®', 'ğŸ‡¨ğŸ‡±', 'ğŸ‡¨ğŸ‡³', 'ğŸ‡¨ğŸ‡´', 'ğŸ‡¨ğŸ‡º', 'ğŸ‡©ğŸ‡ª', 'ğŸ‡©ğŸ‡´', 'ğŸ‡©ğŸ‡¿', 'ğŸ‡ªğŸ‡­', 'ğŸ‡ªğŸ‡¸', 'ğŸ‡ªğŸ‡º', 'ğŸ‡«', 'ğŸ‡«ğŸ‡·', 'ğŸ‡¬ğŸ‡§', 'ğŸ‡¬ğŸ‡·', 'ğŸ‡­ğŸ‡°', 'ğŸ‡®ğŸ‡ª', 'ğŸ‡®ğŸ‡¹', 'ğŸ‡¯ğŸ‡µ', 'ğŸ‡°ğŸ‡µ', 'ğŸ‡°ğŸ‡·', 'ğŸ‡°ğŸ‡¼', 'ğŸ‡±', 'ğŸ‡±ğŸ‡¨', 'ğŸ‡±ğŸ‡º', 'ğŸ‡²ğŸ‡¦', 'ğŸ‡²ğŸ‡©', 'ğŸ‡²ğŸ‡´', 'ğŸ‡²ğŸ‡½', 'ğŸ‡³ğŸ‡¬', 'ğŸ‡³ğŸ‡±', 'ğŸ‡³ğŸ‡¿', 'ğŸ‡µğŸ‡­', 'ğŸ‡µğŸ‡°', 'ğŸ‡µğŸ‡·', 'ğŸ‡µğŸ‡¸', 'ğŸ‡µğŸ‡¹', 'ğŸ‡·ğŸ‡º', 'ğŸ‡¸ğŸ‡³', 'ğŸ‡¹ğŸ‡³', 'ğŸ‡ºğŸ‡¸', 'ğŸ‡ºğŸ‡¾', 'ğŸ‡»ğŸ‡ª', 'ğŸ‡»ğŸ‡³', 'ğŸ‡¼', 'ğŸˆµ', 'ğŸˆ¶', 'ğŸˆ·', 'ğŸŒ€', 'ğŸŒƒ', 'ğŸŒ„', 'ğŸŒ…', 'ğŸŒ†', 'ğŸŒ‡', 'ğŸŒˆ', 'ğŸŒŠ', 'ğŸŒ‹', 'ğŸŒŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒ’', 'ğŸŒ“', 'ğŸŒ—', 'ğŸŒ™', 'ğŸŒš', 'ğŸŒ›', 'ğŸŒœ', 'ğŸŒ', 'ğŸŒ', 'ğŸŒŸ', 'ğŸŒ ', 'ğŸŒ¤', 'ğŸŒ¥', 'ğŸŒ§', 'ğŸŒ¨', 'ğŸŒªï¸', 'ğŸŒ«', 'ğŸŒ¬', 'ğŸŒ®', 'ğŸŒ¯', 'ğŸŒ°', 'ğŸŒ±', 'ğŸŒ²', 'ğŸŒ³', 'ğŸŒ´', 'ğŸŒµ', 'ğŸŒ¶', 'ğŸŒ¶ï¸', 'ğŸŒ·', 'ğŸŒ¸', 'ğŸŒ¹', 'ğŸŒº', 'ğŸŒ»', 'ğŸŒ¼', 'ğŸŒ½', 'ğŸŒ¾', 'ğŸŒ¿', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸƒ', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹', 'ğŸŒ', 'ğŸ', 'ğŸ', 'ğŸ', 'ğŸ‘', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ•', 'ğŸ–', 'ğŸ—', 'ğŸš', 'ğŸ›', 'ğŸœ', 'ğŸ', 'ğŸ', 'ğŸŸ', 'ğŸ£', 'ğŸ¤', 'ğŸ¥', 'ğŸ¦', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ³', 'ğŸ´', 'ğŸµ', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ‹', 'ğŸ', 'ğŸ’', 'ğŸ“', 'ğŸ—', 'ğŸ™', 'ğŸ™ï¸', 'ğŸ', 'ğŸŸ', 'ğŸŸï¸', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ¤', 'ğŸ¥', 'ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸª', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ®', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ²', 'ğŸµ', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ¾', 'ğŸ¿', 'ğŸ€', 'ğŸ', 'ğŸƒ', 'ğŸƒğŸ¼', 'ğŸ„', 'ğŸ…', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸŠğŸ»', 'ğŸŠğŸ»\\u200dâ™€ï¸', 'ğŸ‹ï¸', 'ğŸŒ', 'ğŸŒğŸ¾', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ–', 'ğŸ˜', 'ğŸ™', 'ğŸš', 'ğŸŸ', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ«', 'ğŸ°', 'ğŸ³\\u200dğŸŒˆ', 'ğŸ³ï¸\\u200dğŸŒˆ', 'ğŸ´\\u200dâ˜ ï¸', 'ğŸ¹', 'ğŸ»', 'ğŸ€', 'ğŸ', 'ğŸ‚', 'ğŸ„', 'ğŸ†', 'ğŸ‡', 'ğŸˆ', 'ğŸ‰', 'ğŸŠ', 'ğŸ', 'ğŸ', 'ğŸ', 'ğŸ‘', 'ğŸ’', 'ğŸ“', 'ğŸ”', 'ğŸ•', 'ğŸ–', 'ğŸ˜', 'ğŸ™', 'ğŸš', 'ğŸœ', 'ğŸ', 'ğŸ', 'ğŸŸ', 'ğŸ ', 'ğŸ¡', 'ğŸ¢', 'ğŸ£', 'ğŸ¥', 'ğŸ¦', 'ğŸ§', 'ğŸ¨', 'ğŸ©', 'ğŸ«', 'ğŸ¬', 'ğŸ­', 'ğŸ®', 'ğŸ¯', 'ğŸ°', 'ğŸ±', 'ğŸ²', 'ğŸ³', 'ğŸ¶', 'ğŸ·', 'ğŸ¸', 'ğŸ¹', 'ğŸº', 'ğŸ»', 'ğŸ¼', 'ğŸ½', 'ğŸ¾', 'ğŸ¿', 'ğŸ‘€', 'ğŸ‘', 'ğŸ‘‚', 'ğŸ‘„', 'ğŸ‘…', 'ğŸ‘†', 'ğŸ‘†ğŸ»', 'ğŸ‘‡', 'ğŸ‘‡ğŸ»', 'ğŸ‘‡ğŸ¼', 'ğŸ‘‡ğŸ½', 'ğŸ‘‡ğŸ¾', 'ğŸ‘ˆ', 'ğŸ‘ˆğŸ¼', 'ğŸ‘ˆğŸ¾', 'ğŸ‘‰', 'ğŸ‘‰ğŸ»', 'ğŸ‘‰ğŸ¼', 'ğŸ‘‰ğŸ½', 'ğŸ‘Š', 'ğŸ‘ŠğŸ»', 'ğŸ‘ŠğŸ¼', 'ğŸ‘ŠğŸ½', 'ğŸ‘ŠğŸ¾', 'ğŸ‘ŠğŸ¿', 'ğŸ‘‹', 'ğŸ‘‹ğŸ»', 'ğŸ‘‹ğŸ¼', 'ğŸ‘‹ğŸ½', 'ğŸ‘Œ', 'ğŸ‘ŒğŸ»', 'ğŸ‘ŒğŸ¼', 'ğŸ‘ŒğŸ½', 'ğŸ‘ŒğŸ¾', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¾', 'ğŸ‘ğŸ¿', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘', 'ğŸ‘ğŸ»', 'ğŸ‘ğŸ¼', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¾', 'ğŸ‘', 'ğŸ‘ğŸ½', 'ğŸ‘ğŸ¾', 'ğŸ‘‘', 'ğŸ‘“', 'ğŸ‘•', 'ğŸ‘–', 'ğŸ‘—', 'ğŸ‘™', 'ğŸ‘', 'ğŸ‘Ÿ', 'ğŸ‘ ', 'ğŸ‘¡', 'ğŸ‘£', 'ğŸ‘¤', 'ğŸ‘¥', 'ğŸ‘¦', 'ğŸ‘¦ğŸ»', 'ğŸ‘§', 'ğŸ‘§ğŸ»', 'ğŸ‘§ğŸ¼', 'ğŸ‘¨', 'ğŸ‘¨\\u200dâ¤ï¸\\u200dğŸ‘¨', 'ğŸ‘¨\\u200dğŸŒ¾', 'ğŸ‘¨\\u200dğŸ¤', 'ğŸ‘¨\\u200dğŸ‘©\\u200dğŸ‘§\\u200dğŸ‘¦', 'ğŸ‘¨\\u200dğŸ’»', 'ğŸ‘¨ğŸ»', 'ğŸ‘¨ğŸ»\\u200dâš•ï¸', 'ğŸ‘¨ğŸ»\\u200dğŸ’»', 'ğŸ‘©', 'ğŸ‘©\\u200dâ¤ï¸\\u200dğŸ‘©', 'ğŸ‘©\\u200dğŸŒ¾', 'ğŸ‘©\\u200dğŸ‘§\\u200dğŸ‘§', 'ğŸ‘©ğŸ»\\u200dğŸ¤', 'ğŸ‘©ğŸ¼\\u200dğŸ¨', 'ğŸ‘©ğŸ¼\\u200dğŸ’»', 'ğŸ‘ª', 'ğŸ‘«', 'ğŸ‘­', 'ğŸ‘®', 'ğŸ‘®ğŸ»', 'ğŸ‘¯', 'ğŸ‘°', 'ğŸ‘°ğŸ½', 'ğŸ‘±', 'ğŸ‘±\\u200dâ™€ï¸', 'ğŸ‘±ğŸ»\\u200dâ™€ï¸', 'ğŸ‘²', 'ğŸ‘³', 'ğŸ‘³ğŸ¼', 'ğŸ‘µ', 'ğŸ‘µğŸ¼', 'ğŸ‘¶', 'ğŸ‘¶ğŸ»', 'ğŸ‘¶ğŸ¼', 'ğŸ‘¶ğŸ½', 'ğŸ‘·', 'ğŸ‘¸', 'ğŸ‘¸ğŸ»', 'ğŸ‘¸ğŸ¼', 'ğŸ‘¸ğŸ¾', 'ğŸ‘¹', 'ğŸ‘º', 'ğŸ‘»', 'ğŸ‘¼', 'ğŸ‘¼ğŸ»', 'ğŸ‘¼ğŸ¼', 'ğŸ‘½', 'ğŸ‘¿', 'ğŸ’€', 'ğŸ’', 'ğŸ’ğŸ»', 'ğŸ’ğŸ»\\u200dâ™‚ï¸', 'ğŸ’ğŸ¼', 'ğŸ’ğŸ½', 'ğŸ’ğŸ¾', 'ğŸ’ğŸ¿', 'ğŸ’‚', 'ğŸ’‚ğŸ»', 'ğŸ’‚ğŸ¿', 'ğŸ’ƒ', 'ğŸ’ƒğŸ»', 'ğŸ’ƒğŸ¼', 'ğŸ’ƒğŸ½', 'ğŸ’ƒğŸ¾', 'ğŸ’„', 'ğŸ’…', 'ğŸ’…ğŸ»', 'ğŸ’…ğŸ¼', 'ğŸ’…ğŸ½', 'ğŸ’†', 'ğŸ’†ğŸ»', 'ğŸ’†ğŸ½', 'ğŸ’†ğŸ¾', 'ğŸ’ˆ', 'ğŸ’‰', 'ğŸ’Š', 'ğŸ’‹', 'ğŸ’Œ', 'ğŸ’', 'ğŸ’', 'ğŸ’', 'ğŸ’', 'ğŸ’‘', 'ğŸ’“', 'ğŸ’”', 'ğŸ’•', 'ğŸ’–', 'ğŸ’—', 'ğŸ’˜', 'ğŸ’™', 'ğŸ’š', 'ğŸ’›', 'ğŸ’œ', 'ğŸ’', 'ğŸ’', 'ğŸ’Ÿ', 'ğŸ’¡', 'ğŸ’¢', 'ğŸ’£', 'ğŸ’¤', 'ğŸ’¥', 'ğŸ’¦', 'ğŸ’§', 'ğŸ’¨', 'ğŸ’©', 'ğŸ’ª', 'ğŸ’ªğŸ»', 'ğŸ’ªğŸ¼', 'ğŸ’ªğŸ½', 'ğŸ’ªğŸ¾', 'ğŸ’«', 'ğŸ’¬', 'ğŸ’­', 'ğŸ’®', 'ğŸ’¯', 'ğŸ’°', 'ğŸ’³', 'ğŸ’µ', 'ğŸ’¶', 'ğŸ’¸', 'ğŸ’»', 'ğŸ’¼', 'ğŸ’½', 'ğŸ’¿', 'ğŸ“€', 'ğŸ“‚', 'ğŸ“…', 'ğŸ“†', 'ğŸ“ˆ', 'ğŸ“Š', 'ğŸ“‹', 'ğŸ“Œ', 'ğŸ“', 'ğŸ“', 'ğŸ““', 'ğŸ“–', 'ğŸ“š', 'ğŸ“›', 'ğŸ“', 'ğŸ“', 'ğŸ“¡', 'ğŸ“¢', 'ğŸ“£', 'ğŸ“¦', 'ğŸ“§', 'ğŸ“©', 'ğŸ“¬', 'ğŸ“¯', 'ğŸ“°', 'ğŸ“±', 'ğŸ“²', 'ğŸ“´', 'ğŸ“·', 'ğŸ“¸', 'ğŸ“¹', 'ğŸ“º', 'ğŸ“»', 'ğŸ“¼', 'ğŸ“½', 'ğŸ“½ï¸', 'ğŸ“¿', 'ğŸ”', 'ğŸ”‚', 'ğŸ”ƒ', 'ğŸ”„', 'ğŸ”…', 'ğŸ”‰', 'ğŸ”Š', 'ğŸ”‹', 'ğŸ”Œ', 'ğŸ”', 'ğŸ”‘', 'ğŸ”’', 'ğŸ”“', 'ğŸ””', 'ğŸ”˜', 'ğŸ”™', 'ğŸ”›', 'ğŸ”œ', 'ğŸ”', 'ğŸ”', 'ğŸ”¥', 'ğŸ”¨', 'ğŸ”©', 'ğŸ”ª', 'ğŸ”«', 'ğŸ”®', 'ğŸ”°', 'ğŸ”±', 'ğŸ”²', 'ğŸ”´', 'ğŸ”µ', 'ğŸ”¶', 'ğŸ”¸', 'ğŸ”¹', 'ğŸ”º', 'ğŸ”»', 'ğŸ”¼', 'ğŸ”½', 'ğŸ•Š', 'ğŸ•Œ', 'ğŸ•', 'ğŸ•’', 'ğŸ•˜', 'ğŸ•›', 'ğŸ•œ', 'ğŸ•Ÿ', 'ğŸ•¤', 'ğŸ•ª', 'ğŸ•¯', 'ğŸ•µ', 'ğŸ•¶', 'ğŸ•·ï¸', 'ğŸ•º', 'ğŸ•ºğŸ»', 'ğŸ•ºğŸ½', 'ğŸ–', 'ğŸ–ï¸', 'ğŸ–ğŸ¼', 'ğŸ–ğŸ½', 'ğŸ–’', 'ğŸ–•', 'ğŸ–•ğŸ»', 'ğŸ–•ğŸ¼', 'ğŸ–•ğŸ½', 'ğŸ–•ğŸ¾', 'ğŸ–•ğŸ¿', 'ğŸ––', 'ğŸ––ğŸ¾', 'ğŸ–¤', 'ğŸ–¥ï¸', 'ğŸ–¼', 'ğŸ—‚', 'ğŸ—“', 'ğŸ—“ï¸', 'ğŸ—', 'ğŸ—', 'ğŸ—¡', 'ğŸ—£', 'ğŸ—£ï¸', 'ğŸ—¨ï¸', 'ğŸ—³', 'ğŸ—»', 'ğŸ—¼', 'ğŸ—½', 'ğŸ—¾', 'ğŸ˜€', 'ğŸ˜', 'ğŸ˜‚', 'ğŸ˜ƒ', 'ğŸ˜„', 'ğŸ˜…', 'ğŸ˜†', 'ğŸ˜‡', 'ğŸ˜ˆ', 'ğŸ˜‰', 'ğŸ˜Š', 'ğŸ˜‹', 'ğŸ˜Œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜‘', 'ğŸ˜’', 'ğŸ˜“', 'ğŸ˜”', 'ğŸ˜•', 'ğŸ˜–', 'ğŸ˜—', 'ğŸ˜˜', 'ğŸ˜™', 'ğŸ˜š', 'ğŸ˜›', 'ğŸ˜œ', 'ğŸ˜', 'ğŸ˜', 'ğŸ˜Ÿ', 'ğŸ˜ ', 'ğŸ˜¡', 'ğŸ˜¢', 'ğŸ˜£', 'ğŸ˜¤', 'ğŸ˜¥', 'ğŸ˜¦', 'ğŸ˜§', 'ğŸ˜¨', 'ğŸ˜©', 'ğŸ˜ª', 'ğŸ˜«', 'ğŸ˜¬', 'ğŸ˜­', 'ğŸ˜®', 'ğŸ˜¯', 'ğŸ˜°', 'ğŸ˜±', 'ğŸ˜²', 'ğŸ˜³', 'ğŸ˜´', 'ğŸ˜µ', 'ğŸ˜¶', 'ğŸ˜·', 'ğŸ˜¸', 'ğŸ˜¹', 'ğŸ˜º', 'ğŸ˜»', 'ğŸ˜¼', 'ğŸ˜½', 'ğŸ˜¿', 'ğŸ™€', 'ğŸ™', 'ğŸ™‚', 'ğŸ™ƒ', 'ğŸ™„', 'ğŸ™…', 'ğŸ™…ğŸ»', 'ğŸ™…ğŸ¼', 'ğŸ™…ğŸ½\\u200dâ™‚ï¸', 'ğŸ™†', 'ğŸ™†\\u200dâ™‚ï¸', 'ğŸ™†ğŸ½', 'ğŸ™‡', 'ğŸ™‡\\u200dâ™€ï¸', 'ğŸ™‡ğŸ»', 'ğŸ™‡ğŸ½', 'ğŸ™ˆ', 'ğŸ™‰', 'ğŸ™Š', 'ğŸ™‹', 'ğŸ™‹ğŸ»', 'ğŸ™‹ğŸ»\\u200dâ™‚ï¸', 'ğŸ™‹ğŸ¼', 'ğŸ™‹ğŸ½', 'ğŸ™Œ', 'ğŸ™ŒğŸ»', 'ğŸ™ŒğŸ¼', 'ğŸ™ŒğŸ½', 'ğŸ™ŒğŸ¾', 'ğŸ™', 'ğŸ™ğŸ»', 'ğŸ™', 'ğŸ™', 'ğŸ™ğŸ»', 'ğŸ™ğŸ¼', 'ğŸ™ğŸ½', 'ğŸ™ğŸ¾', 'ğŸ™ğŸ¿', 'ğŸš€', 'ğŸš', 'ğŸš‡', 'ğŸšˆ', 'ğŸšŒ', 'ğŸš‘', 'ğŸš“', 'ğŸš”', 'ğŸš–', 'ğŸš—', 'ğŸš˜', 'ğŸš™', 'ğŸš¢', 'ğŸš£', 'ğŸš¦', 'ğŸš§', 'ğŸš¨', 'ğŸš©', 'ğŸš«', 'ğŸš¬', 'ğŸš®', 'ğŸš²', 'ğŸš´', 'ğŸš´\\u200dâ™€ï¸', 'ğŸš´ğŸ»', 'ğŸš´ğŸ»\\u200dâ™€ï¸', 'ğŸšµ', 'ğŸšµ\\u200dâ™€ï¸', 'ğŸš¶', 'ğŸš¶\\u200dâ™‚ï¸', 'ğŸš¶ğŸ¾', 'ğŸš»', 'ğŸš¼', 'ğŸš¿', 'ğŸ›€', 'ğŸ›ƒ', 'ğŸ›„', 'ğŸ›©ï¸', 'ğŸ›«', 'ğŸ›¬', 'ğŸ›°', 'ğŸ›³', 'ğŸ›´', 'ğŸ¤', 'ğŸ¤‘', 'ğŸ¤’', 'ğŸ¤“', 'ğŸ¤”', 'ğŸ¤•', 'ğŸ¤–', 'ğŸ¤—', 'ğŸ¤˜', 'ğŸ¤˜ğŸ»', 'ğŸ¤˜ğŸ¼', 'ğŸ¤˜ğŸ½', 'ğŸ¤˜ğŸ¾', 'ğŸ¤™', 'ğŸ¤™ğŸ»', 'ğŸ¤™ğŸ¼', 'ğŸ¤™ğŸ¾', 'ğŸ¤šğŸ»', 'ğŸ¤šğŸ½', 'ğŸ¤›ğŸ½', 'ğŸ¤œğŸ¿', 'ğŸ¤', 'ğŸ¤', 'ğŸ¤ğŸ»', 'ğŸ¤ğŸ¼', 'ğŸ¤ğŸ½', 'ğŸ¤ ', 'ğŸ¤¡', 'ğŸ¤¢', 'ğŸ¤£', 'ğŸ¤¤', 'ğŸ¤¥', 'ğŸ¤¦\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ»\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ»\\u200dâ™‚ï¸', 'ğŸ¤¦ğŸ¼\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ½\\u200dâ™€ï¸', 'ğŸ¤¦ğŸ½\\u200dâ™‚ï¸', 'ğŸ¤¦ğŸ¾\\u200dâ™‚ï¸', 'ğŸ¤§', 'ğŸ¤³ğŸ½', 'ğŸ¤´ğŸ»', 'ğŸ¤·', 'ğŸ¤·\\u200dâ™€ï¸', 'ğŸ¤·\\u200dâ™‚ï¸', 'ğŸ¤·ğŸ»\\u200dâ™€ï¸', 'ğŸ¤·ğŸ»\\u200dâ™‚ï¸', 'ğŸ¤·ğŸ¼\\u200dâ™€ï¸', 'ğŸ¤·ğŸ½\\u200dâ™€ï¸', 'ğŸ¤·ğŸ½\\u200dâ™‚ï¸', 'ğŸ¤·ğŸ¾\\u200dâ™€ï¸', 'ğŸ¤·ğŸ¾\\u200dâ™‚ï¸', 'ğŸ¥€', 'ğŸ¥', 'ğŸ¥‚', 'ğŸ¥ƒ', 'ğŸ¥„', 'ğŸ¥…', 'ğŸ¥‡', 'ğŸ¥Š', 'ğŸ¥', 'ğŸ¥’', 'ğŸ¥”', 'ğŸ¥˜', 'ğŸ¥', 'ğŸ¦€', 'ğŸ¦', 'ğŸ¦ƒ', 'ğŸ¦„', 'ğŸ¦‡', 'ğŸ¦‰', 'ğŸ¦‹', 'ğŸ¦‘', '\\U000fe4e6']\n",
      "vocab size:  2057\n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "vocabulary = preprocess()\n",
    "# add <start> and <end> to the vocabulary\n",
    "vocabulary = [\"<start>\"] + [\"<end>\"] + vocabulary\n",
    "\n",
    "print(\"vocabulary: \", vocabulary)\n",
    "print(\"vocab size: \", len(vocabulary))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:46.517053Z",
     "end_time": "2023-04-28T13:24:48.975286Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb2PGj0Yc2TY"
   },
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function `lm` that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
    "\n",
    "{\n",
    "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
    "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
    "}\n",
    "\n",
    "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
    "\n",
    "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# helper functions for lm\n",
    "\n",
    "def build_ngram_model(current_data, n):\n",
    "    \"\"\" Builds an n-gram model from the data\n",
    "    Args:\n",
    "        current_data: a pandas dataframe with a column named \"tweet_text\"\n",
    "        n: the n in n-gram\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "\n",
    "    if n == 1: # if n is 1, build a 1-gram model\n",
    "        model = build_1gram_model(current_data, model)\n",
    "    else: # if n is not 1, build an n-gram model\n",
    "        # iterate over all the tweets in the data file\n",
    "        for tweet in current_data[\"tweet_text\"]:\n",
    "            # add the start and end tokens to the tweet\n",
    "            tweet = \"<start> \" + tweet + \" <end>\"\n",
    "            # convert the tweet to a tuple of tokens\n",
    "            tweet = tweet_to_token_tuples(tweet)\n",
    "\n",
    "            # iterate over all the n-grams in the tweet\n",
    "            for i in range(len(tweet) - n + 1):\n",
    "                # define n_gram, n_minus_1_gram and n_th_token\n",
    "                n_gram = tweet[i:i+n]\n",
    "                n_minus_1_gram = n_gram[:-1]\n",
    "                n_th_token = n_gram[-1]\n",
    "\n",
    "                # add the n-gram to the model\n",
    "                model = add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_1gram_model(current_data, model):\n",
    "    \"\"\" Builds an 1-gram model from the data\n",
    "    Args:\n",
    "        current_data: a pandas dataframe with a column named \"tweet_text\"\n",
    "        n: the n in n-gram\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, spacial case for 1-gram: {(): {n_th_token: count}}\n",
    "    \"\"\"\n",
    "    token_counter = {}\n",
    "    # iterate over all the tweets in the data file\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        tweet = \"<start> \" + tweet + \" <end>\"\n",
    "        # convert the tweet to a tuple of tokens\n",
    "        tweet_tokens = tweet_to_token_tuples(tweet)\n",
    "        # iterate over all the tokens in the tweet\n",
    "        for token in tweet_tokens:\n",
    "            if token not in token_counter: # if the token is not in the counter, add it\n",
    "                token_counter[token] = 0\n",
    "            # count the token\n",
    "            token_counter[token] += 1\n",
    "\n",
    "    # add the n-gram to the model\n",
    "    model[()] = token_counter\n",
    "    return model\n",
    "\n",
    "def add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token):\n",
    "    \"\"\" Adds and counts an n-gram to the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "        n_minus_1_gram: a tuple of n-1 tokens\n",
    "        n_th_token: the n_th token\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "    # add the n-gram to the model\n",
    "    if n_minus_1_gram not in model:\n",
    "        model[n_minus_1_gram] = {}\n",
    "    # add the n_th token to the model\n",
    "    if n_th_token not in model[n_minus_1_gram]:\n",
    "        model[n_minus_1_gram][n_th_token] = 0\n",
    "    # count the n_th token, i.e. add 1 to its count\n",
    "    model[n_minus_1_gram][n_th_token] += 1\n",
    "    return model\n",
    "\n",
    "\n",
    "def add_one_smoothing(model, vocabulary):\n",
    "    \"\"\" Adds add_one smoothing to the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # add spacial key <not in model>\n",
    "        model[n_minus_1_gram][\"<notInModel>\"] = 0\n",
    "        # iterate over all the tokens in the vocabulary\n",
    "        for token in vocabulary:\n",
    "            # if the token is not in the model replace it with <notInModel>\n",
    "            if token not in model[n_minus_1_gram]:\n",
    "                token = \"<notInModel>\"\n",
    "\n",
    "            # count the token, i.e. add 1 to its count (add one smoothing)\n",
    "            model[n_minus_1_gram][token] += 1\n",
    "    return model\n",
    "\n",
    "def calculate_probabilities(model):\n",
    "    \"\"\" Calculates the probabilities of the model,\n",
    "        also adds meta_data to the model (total_count=total number of tokens, <notInModel>_count=number of tokens that are not in the model)\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # get the counts of all the tokens\n",
    "        token_counts = model[n_minus_1_gram].values()\n",
    "        # if model[n_minus_1_gram] has the key <notInModel> get its count\n",
    "        if \"<notInModel>\" in model[n_minus_1_gram]:\n",
    "            token_notInModel_count = model[n_minus_1_gram][\"<notInModel>\"]\n",
    "        else:\n",
    "            token_notInModel_count = 0\n",
    "        # calculate the total count\n",
    "        total_count = sum(token_counts)\n",
    "        # iterate over all the tokens in the model\n",
    "        for token in model[n_minus_1_gram]:\n",
    "            # calculate the probability, i.e. divide the count by the total count\n",
    "            model[n_minus_1_gram][token] /= total_count\n",
    "\n",
    "        model[n_minus_1_gram][\"meta_data\"] = {\"total_count\": total_count, \"<notInModel>_count\": token_notInModel_count}\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:48.982423Z",
     "end_time": "2023-04-28T13:24:48.991910Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kMC_u8eQbVvZ",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:48.994910Z",
     "end_time": "2023-04-28T13:24:49.037775Z"
    }
   },
   "source": [
    "def lm(n, vocabulary, data_file_path, add_one):\n",
    "    \"\"\" Builds an n-gram model from the given data\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "        data_file_path: the data_file from which we record probabilities for our model\n",
    "        add_one: True/False (use add_one smoothing or not)\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file_path,  encoding=\"utf-8\")\n",
    "    # build the n-gram model\n",
    "    model = build_ngram_model(current_data, n)\n",
    "    if add_one:\n",
    "        # add one smoothing\n",
    "        model = add_one_smoothing(model, vocabulary)\n",
    "    # calculate the probabilities\n",
    "    model = calculate_probabilities(model)\n",
    "    return model"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "# call the function for the first data file\n",
    "lm_model_True = lm(2, vocabulary, path + \"/\" + data_files[0], True)\n",
    "\n",
    "lm_model_False = lm(2, vocabulary, path + \"/\" + data_files[0], False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:49.009530Z",
     "end_time": "2023-04-28T13:24:50.497218Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8TchtI22I3"
   },
   "source": [
    "**Part 3**\n",
    "\n",
    "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "# a function that calculates the perplexity of a model\n",
    "def calculate_perplexity(current_data, n, model):\n",
    "    \"\"\" Calculates the perplexity of a model running over a given data file\n",
    "    Args:\n",
    "        current_data: a data frame representing the data\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    Returns:\n",
    "        perplexity: the perplexity of the model\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0\n",
    "    n_gram_count = 0\n",
    "\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        # add start and end tokens\n",
    "        tweet = \"<start> \" + tweet + \" <end>\"\n",
    "        # convert the tweet to a list of tokens\n",
    "        tweet = tweet_to_token_tuples(tweet)\n",
    "\n",
    "        # iterate over all the n-grams in the tweet\n",
    "        for i in range(len(tweet) - n + 1):\n",
    "            n_gram = tweet[i:i+n]\n",
    "            n_minus_1_gram = n_gram[:-1]\n",
    "            n_th_token = n_gram[-1]\n",
    "\n",
    "            # if the n-gram is in the model, add its log probability to the sum\n",
    "            if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "                log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "                n_gram_count += 1\n",
    "            # <notInModel> is for add_one smoothing case\n",
    "            elif n_minus_1_gram in model and \"<notInModel>\" in model[n_minus_1_gram]:\n",
    "                # get the meta_data of the n-1 gram\n",
    "                meta_data = model[n_minus_1_gram][\"meta_data\"]\n",
    "                total_count = meta_data[\"total_count\"]\n",
    "                notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "                # calculate the probability of <notInModel>\n",
    "                prob_notInModel = model[n_minus_1_gram][\"<notInModel>\"]\n",
    "                # calculate the probability of single unseen token\n",
    "                prob = prob_notInModel / notInModel_count\n",
    "                # calculate the log probability of the n-gram and add it to the sum\n",
    "                log_prob_sum += -log2(prob)\n",
    "                n_gram_count += 1\n",
    "\n",
    "    # return infinite perplexity if no n-grams found\n",
    "    if n_gram_count == 0:\n",
    "        return float('inf')\n",
    "    # calculate the entropy and the perplexity\n",
    "    entropy = log_prob_sum / n_gram_count\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:50.498239Z",
     "end_time": "2023-04-28T13:24:50.513300Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F0kkMn328-lJ",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:50.515295Z",
     "end_time": "2023-04-28T13:24:50.539311Z"
    }
   },
   "source": [
    "def eval(n, model, data_file):\n",
    "    \"\"\" Evaluates the perplexity of a model\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "        data_file: the data file path for which we want to calculate the perplexity\n",
    "    Returns:\n",
    "        the perplexity of the model\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file, encoding=\"utf-8\")\n",
    "    # calculate the perplexity\n",
    "    perplexity = calculate_perplexity(current_data, n, model)\n",
    "    return perplexity"
   ],
   "execution_count": 13,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity lm_model_True:  20.89777888632233\n",
      "perplexity lm_model_False:  17.724467632988453\n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "perplexity = eval(2, lm_model_True, path + \"/\" + data_files[0])\n",
    "print(\"perplexity lm_model_True: \", perplexity)\n",
    "perplexity = eval(2, lm_model_False, path + \"/\" + data_files[0])\n",
    "print(\"perplexity lm_model_False: \", perplexity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:50.532789Z",
     "end_time": "2023-04-28T13:24:51.884080Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enGmtLE3921p"
   },
   "source": [
    "**Part 4**\n",
    "\n",
    "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values.\n",
    "\n",
    "Save the dataframe to a CSV with the name format: {student_id_1}\\_...\\_{student_id_n}\\_part4.csv"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "caAxLE9s_fvn",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:51.888351Z",
     "end_time": "2023-04-28T13:24:51.911551Z"
    }
   },
   "source": [
    "def match(n, add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Then, calculate the perplexity of all possible pairs.\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        df: a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # create a dataframe\n",
    "    df = pd.DataFrame(columns=data_files, index=data_files)\n",
    "\n",
    "    # create models for every language\n",
    "    models = compute_data_files_models(data_files, n, vocabulary, path, add_one)\n",
    "\n",
    "    # calculate the perplexity of all possible pairs\n",
    "    for lang1 in data_files: # will be the model\n",
    "        # define the model\n",
    "        current_model = models[lang1]\n",
    "        for lang2 in data_files: # will be the data file\n",
    "            # define the data file\n",
    "            current_data_file = path + \"/\" + lang2\n",
    "            # evaluate the model\n",
    "            perplexity = eval(n, current_model, current_data_file)\n",
    "            # save the perplexity to the dataframe\n",
    "            df[lang1][lang2] = perplexity\n",
    "    return df, models # return the dataframe and the models, so that we can use them later\n",
    "\n",
    "def compute_data_files_models(data_files, n, vocabulary, path , add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Args:\n",
    "        data_files: the data files to create models for\n",
    "        n: the n in n-gram\n",
    "        vocabulary: the vocabulary\n",
    "        path: the path to the data files\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for data_file in data_files:\n",
    "        models[data_file] = lm(n, vocabulary, path + \"/\" + data_file, add_one)\n",
    "    return models\n"
   ],
   "execution_count": 15,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframe: \n",
      "           en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  20.897779  31.469961  27.982361   29.28984  31.027015  26.810963  \\\n",
      "es.csv   27.07923  18.784248   25.12035  28.965038  24.237032  30.308435   \n",
      "fr.csv  28.585712  28.319004  19.703018  33.973468  28.874987   30.31644   \n",
      "in.csv  28.924859  33.147844  31.945387  20.857584  32.624991  29.551127   \n",
      "it.csv  26.927061  24.111037  25.776475  28.363825  19.153515  29.954258   \n",
      "nl.csv   27.05875  32.310611  29.025613  29.288145  32.508412  20.315714   \n",
      "pt.csv  29.501319  23.932835  26.986815  31.655019  25.779086  32.238053   \n",
      "tl.csv  28.454305  33.063345  33.712831  26.072283  32.076746  30.991691   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  32.798601  27.153291  \n",
      "es.csv  23.319751  28.253442  \n",
      "fr.csv  29.122319  33.539102  \n",
      "in.csv  34.845109  25.171219  \n",
      "it.csv  25.530354  27.124208  \n",
      "nl.csv  34.036014  30.440644  \n",
      "pt.csv  19.563727  31.095087  \n",
      "tl.csv  34.666478  20.957046  \n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "df_part4, models_part4 = match(2, True)\n",
    "print(\"dataframe: \")\n",
    "print(df_part4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:24:51.901048Z",
     "end_time": "2023-04-28T13:25:38.566340Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "# save the dataframe to a CSV of format {student_id_1}\\_...\\_{student_id_n}\\_part4.csv\n",
    "df_part4.to_csv(student_id_1 + \"_\" + student_id_2 + \"_part4.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:25:38.567350Z",
     "end_time": "2023-04-28T13:25:38.584769Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waGMwA8H_n17"
   },
   "source": [
    "**Part 5**\n",
    "\n",
    "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another.\n",
    "\n",
    "Load each result to a dataframe and save to a CSV with the name format: \n",
    "\n",
    "for cases with add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_part5.csv\n",
    "\n",
    "For cases without add_one:\n",
    "{student_id_1}\\_...\\_{student_id_n}\\_n1\\_wo\\_addone\\_part5.csv\n",
    "\n",
    "Follow the same format for n2,n3, and n4\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nk32naXyAMdl",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:25:38.585776Z",
     "end_time": "2023-04-28T13:25:38.626770Z"
    }
   },
   "source": [
    "def run_match(n_values = [1, 2, 3, 4],add_one_values = [True, False] ):\n",
    "    \"\"\" Runs match with n values 1-4, once with add_one and once without, and print the 8 tables to this notebook.\n",
    "    Args:\n",
    "        n_values: the n values to run match with\n",
    "        add_one_values: the add_one values to run match with\n",
    "    Returns:\n",
    "        dataframes: a dictionary of the dataframes, so that we can use them later,\n",
    "                i.e {n, add_one: dataframe}\n",
    "        language_models_dict: a dictionary of the language models, so that we can use them later,\n",
    "                i.e {n, add_one: {language: model}}\n",
    "    \"\"\"\n",
    "    # create dictionaries for the dataframes, key = (n, add_one), value = dataframe\n",
    "    dataframes = {}\n",
    "    # create a dictionary for the language models, key = (n, add_one), value = language models = {language: model}\n",
    "    language_models_dict = {}\n",
    "    # iterate over all the n values\n",
    "    for n in n_values:\n",
    "        # iterate over all the add_one values\n",
    "        for add_one in add_one_values:\n",
    "            # create the dataframe and the language models, using the match function\n",
    "            current_df, current_language_models = match(n, add_one)\n",
    "            print(\"completed n = \" + str(n) + \", add_one = \" + str(add_one) + \"!\")\n",
    "            # add the dataframe to the dataframes dictionary\n",
    "            dataframes[(n, add_one)] = current_df\n",
    "            # add the language models to the language_models_dict dictionary\n",
    "            language_models_dict[(n, add_one)] = current_language_models\n",
    "            # # save the dataframe to a CSV\n",
    "            # if add_one:\n",
    "            #     current_df.to_csv(\"language_perplexity_n\" + str(n) + \"_part5.csv\")\n",
    "            # else:\n",
    "            #     current_df.to_csv(\"language_perplexity_n\" + str(n) + \"_wo_addone_part5.csv\")\n",
    "    return dataframes, language_models_dict # return the dataframes and the language models, so that we can use them later\n"
   ],
   "execution_count": 18,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed n = 1, add_one = True!\n",
      "completed n = 1, add_one = False!\n",
      "completed n = 2, add_one = True!\n",
      "completed n = 2, add_one = False!\n",
      "completed n = 3, add_one = True!\n",
      "completed n = 3, add_one = False!\n",
      "completed n = 4, add_one = True!\n",
      "completed n = 4, add_one = False!\n"
     ]
    }
   ],
   "source": [
    "run_match_dataframes, run_match_language_models = run_match()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:25:38.599774Z",
     "end_time": "2023-04-28T13:34:27.160775Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataframes: \n",
      "{(1, True):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  37.178192  40.419081  40.010312  40.845231  39.864997  39.194549  \\\n",
      "es.csv  39.195904  34.907042  38.153696  41.520361  37.414257  39.079891   \n",
      "fr.csv   41.12382  39.387318  36.252997  45.115759  39.022463  40.281632   \n",
      "in.csv  39.933617  41.950442  42.708112  36.119644  41.765169  39.987242   \n",
      "it.csv  39.086583  38.582739    38.4033  41.593138  36.323042  39.396963   \n",
      "nl.csv  38.336395  40.021781  39.470987   40.36988  39.640239  36.355625   \n",
      "pt.csv  41.206373   37.88238  39.235636  43.290318  39.304747  40.809172   \n",
      "tl.csv  42.751288  45.053611  46.842864  40.803226  44.293566  44.298361   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  40.869391  40.520973  \n",
      "es.csv  36.120973  40.988471  \n",
      "fr.csv  39.197119  45.607366  \n",
      "in.csv  41.340695  37.687806  \n",
      "it.csv  39.007692  41.049021  \n",
      "nl.csv  40.179656  41.245413  \n",
      "pt.csv  35.496439  42.728082  \n",
      "tl.csv  44.952481  39.022114  , (1, False):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  37.120455  39.838487  39.415695  40.326669  39.187508  38.479732  \\\n",
      "es.csv  37.415614  34.845863   37.70165  37.653333  37.049102  38.239763   \n",
      "fr.csv  38.729857  37.901749  36.197689  43.606789  37.916659  39.682175   \n",
      "in.csv  39.318588  41.247667  42.044969   36.05612  41.004994  39.284779   \n",
      "it.csv  37.491749   37.45121  37.881275  40.162899  36.252804  38.639594   \n",
      "nl.csv  38.000959   39.50633  39.048921  39.937543  39.184971  36.291371   \n",
      "pt.csv   37.76541  35.268064  37.114922   38.74375  36.610819  38.677158   \n",
      "tl.csv  42.204158  44.399429  46.268709  40.286055  43.637342  43.648274   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  40.253192  39.835764  \n",
      "es.csv  35.769651  40.429135  \n",
      "fr.csv  38.138173  43.846747  \n",
      "in.csv  40.628347  37.058328  \n",
      "it.csv  37.750827  39.623599  \n",
      "nl.csv  39.617148  40.805859  \n",
      "pt.csv  35.417378   41.46708  \n",
      "tl.csv  44.228468  38.944093  , (2, True):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  20.897779  31.469961  27.982361   29.28984  31.027015  26.810963  \\\n",
      "es.csv   27.07923  18.784248   25.12035  28.965038  24.237032  30.308435   \n",
      "fr.csv  28.585712  28.319004  19.703018  33.973468  28.874987   30.31644   \n",
      "in.csv  28.924859  33.147844  31.945387  20.857584  32.624991  29.551127   \n",
      "it.csv  26.927061  24.111037  25.776475  28.363825  19.153515  29.954258   \n",
      "nl.csv   27.05875  32.310611  29.025613  29.288145  32.508412  20.315714   \n",
      "pt.csv  29.501319  23.932835  26.986815  31.655019  25.779086  32.238053   \n",
      "tl.csv  28.454305  33.063345  33.712831  26.072283  32.076746  30.991691   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  32.798601  27.153291  \n",
      "es.csv  23.319751  28.253442  \n",
      "fr.csv  29.122319  33.539102  \n",
      "in.csv  34.845109  25.171219  \n",
      "it.csv  25.530354  27.124208  \n",
      "nl.csv  34.036014  30.440644  \n",
      "pt.csv  19.563727  31.095087  \n",
      "tl.csv  34.666478  20.957046  , (2, False):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  17.724468  25.479376  23.155726  23.863433  25.414703  22.317264  \\\n",
      "es.csv  20.591872  15.792758  19.247003  22.051549  18.861363  23.190483   \n",
      "fr.csv  21.120547  21.517122  16.677813  23.708614  22.196511   23.42763   \n",
      "in.csv  24.090071  26.654781  26.084358   17.38079  26.290832  24.539876   \n",
      "it.csv  21.900883  19.352324  21.301684  22.321339  16.059454  24.314415   \n",
      "nl.csv  22.689507    26.3329  24.146429  24.000524  26.679015  17.273125   \n",
      "pt.csv  21.780276  18.936763  20.922373  23.159647  19.544934  24.288718   \n",
      "tl.csv  23.292391  26.143783  26.884188  21.020966  25.462246  25.039221   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  25.631342  22.169069  \n",
      "es.csv  18.565013  21.225123  \n",
      "fr.csv  22.152307  23.490045  \n",
      "in.csv  27.324301  20.711899  \n",
      "it.csv  19.787052  21.391132  \n",
      "nl.csv  26.996438  24.810157  \n",
      "pt.csv  15.877696  21.886864  \n",
      "tl.csv  26.493854  17.157309  , (3, True):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv  27.967872  73.518715  60.029151  65.333592  72.059091  55.636322  \\\n",
      "es.csv   55.67061  25.673341  46.864248  64.443086  47.031094  67.864058   \n",
      "fr.csv  54.921113  58.992193  25.843536  73.131563  60.135874  64.845253   \n",
      "in.csv  79.126081  91.740646  87.404077  32.355814  94.873931  80.169983   \n",
      "it.csv  60.284464  49.146468  55.165184   68.51715  26.363162  73.956387   \n",
      "nl.csv  62.477532  85.282565  71.306632   74.50332  89.532867  28.990114   \n",
      "pt.csv  62.531663  45.360171   56.72219  70.496443   51.01465  75.351945   \n",
      "tl.csv  71.737453    89.7066  90.092593  58.876245  82.857666  78.833844   \n",
      "\n",
      "            pt.csv     tl.csv  \n",
      "en.csv   82.627873  56.590311  \n",
      "es.csv   46.688049  62.943661  \n",
      "fr.csv   68.208476  72.409393  \n",
      "in.csv  105.255939  60.655877  \n",
      "it.csv   58.256481  64.555193  \n",
      "nl.csv  100.478647  79.754772  \n",
      "pt.csv   27.099177  68.899993  \n",
      "tl.csv   95.569098  30.784426  , (3, False):            en.csv     es.csv     fr.csv     in.csv     it.csv     nl.csv   \n",
      "en.csv   8.815849  14.732945  13.745476  13.700723  14.580597  13.024648  \\\n",
      "es.csv  13.874963   8.457296  12.539529  14.491958  11.842342  15.129452   \n",
      "fr.csv  12.530608  12.746902   8.443971  14.282556  13.051073  13.217303   \n",
      "in.csv  17.499104  16.709219   17.69347   9.683005   16.91803  17.352945   \n",
      "it.csv  14.804006  12.258879  13.824322  14.594945   8.418997  16.373166   \n",
      "nl.csv    13.9609  15.749206  14.970512  15.304789  16.796273   9.052996   \n",
      "pt.csv  14.888521  11.309526  14.351053   14.95347  12.307066  16.330253   \n",
      "tl.csv  15.258254  15.737283  16.484133   12.58356  14.681736  15.551648   \n",
      "\n",
      "           pt.csv     tl.csv  \n",
      "en.csv  14.416649  12.092542  \n",
      "es.csv   10.74643  13.565382  \n",
      "fr.csv   13.21246  13.528851  \n",
      "in.csv  16.984261  13.811721  \n",
      "it.csv  12.571588  13.938484  \n",
      "nl.csv  16.807288  14.994388  \n",
      "pt.csv    7.94614  13.996119  \n",
      "tl.csv  14.574841   8.438492  , (4, True):             en.csv      es.csv      fr.csv      in.csv      it.csv   \n",
      "en.csv   67.326731  247.701788  201.603947   226.56826  238.555499  \\\n",
      "es.csv  227.448631   61.444735  165.433134  272.709577  146.858402   \n",
      "fr.csv  179.766417  176.237233   59.575658  267.795817  194.388278   \n",
      "in.csv  330.419859  332.984737  352.142225   85.885451   336.09163   \n",
      "it.csv  243.467214  162.844355  207.745983  276.447967    63.23571   \n",
      "nl.csv  204.908679  261.810975  230.615687  274.635636  285.707777   \n",
      "pt.csv  252.943146  129.050481  207.525364  290.446593  160.779345   \n",
      "tl.csv  255.102314  304.952271  324.558698  193.122462  268.592968   \n",
      "\n",
      "            nl.csv      pt.csv      tl.csv  \n",
      "en.csv  185.803822  277.388394  177.273367  \n",
      "es.csv  259.182262  135.339057  243.901972  \n",
      "fr.csv  203.845598  221.884099  257.603911  \n",
      "in.csv  330.716119  383.409708  228.020113  \n",
      "it.csv  291.697852   203.25427  250.607147  \n",
      "nl.csv   69.987817  332.359819  282.138796  \n",
      "pt.csv  302.353189   64.330886  260.300703  \n",
      "tl.csv  290.458347  315.556439   74.746015  , (4, False):           en.csv    es.csv    fr.csv    in.csv    it.csv    nl.csv    pt.csv   \n",
      "en.csv  4.397238  7.860662   7.68184   7.44575  7.367346  7.485805  7.305019  \\\n",
      "es.csv  9.363457  4.627229  8.374669  9.531666  7.835745  8.763932  6.765076   \n",
      "fr.csv  8.168455  7.641715  4.399359  8.606523  7.751697   7.87421  7.947284   \n",
      "in.csv  9.285309  8.800856  9.162864  4.954691  8.682352  9.252592  8.453984   \n",
      "it.csv  8.694342  7.742432  8.625255  9.113131  4.510851  8.775903  7.637443   \n",
      "nl.csv  7.969774  7.694922  7.904729  8.551229  7.705656   4.50526  7.756327   \n",
      "pt.csv  8.780353  6.756064  8.595699  9.037032  7.602203  8.808735  4.256249   \n",
      "tl.csv  7.882867  7.736567   7.78078  7.718827  7.356612   8.09563  7.153405   \n",
      "\n",
      "          tl.csv  \n",
      "en.csv  6.504069  \n",
      "es.csv  8.656826  \n",
      "fr.csv  8.202438  \n",
      "in.csv  8.634713  \n",
      "it.csv  8.460744  \n",
      "nl.csv  8.165355  \n",
      "pt.csv  8.143537  \n",
      "tl.csv  4.219909  }\n"
     ]
    }
   ],
   "source": [
    "print(\"dataframes: \")\n",
    "print(run_match_dataframes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:34:27.162825Z",
     "end_time": "2023-04-28T13:34:27.191762Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 123456789_327156998_n1_part5.csv\n",
      "saved 123456789_327156998_n1_wo_addone_part5.csv\n",
      "saved 123456789_327156998_n2_part5.csv\n",
      "saved 123456789_327156998_n2_wo_addone_part5.csv\n",
      "saved 123456789_327156998_n3_part5.csv\n",
      "saved 123456789_327156998_n3_wo_addone_part5.csv\n",
      "saved 123456789_327156998_n4_part5.csv\n",
      "saved 123456789_327156998_n4_wo_addone_part5.csv\n"
     ]
    }
   ],
   "source": [
    "# save the dataframes to CSVs of format:\n",
    "# with add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_part5.csv\n",
    "# without add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_wo\\_addone\\_part5.csv\n",
    "\n",
    "n_values = [1, 2, 3, 4]\n",
    "add_one_values = [True, False]\n",
    "# iterate over all the n values and add_one values and save the dataframes to CSVs\n",
    "for n in n_values:\n",
    "    for add_one in add_one_values:\n",
    "        if add_one:\n",
    "            run_match_dataframes[(n, add_one)].to_csv(student_id_1 + \"_\" + student_id_2 + \"_n\" + str(n) + \"_part5.csv\", index=False)\n",
    "            print(\"saved \" + student_id_1 + \"_\" + student_id_2 + \"_n\" + str(n) + \"_part5.csv\")\n",
    "        else:\n",
    "            run_match_dataframes[(n, add_one)].to_csv(student_id_1 + \"_\" + student_id_2 + \"_n\" + str(n) + \"_wo_addone_part5.csv\", index=False)\n",
    "            print(\"saved \" + student_id_1 + \"_\" + student_id_2 + \"_n\" + str(n) + \"_wo_addone_part5.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:34:27.191762Z",
     "end_time": "2023-04-28T13:34:27.207105Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_models_to_pickle(language_models_dict=run_match_language_models, filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Saves the language models to a pickle file\n",
    "    Args:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # save run_match_language_models to a pickle file, so that we can use it later\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(language_models_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:34:27.210499Z",
     "end_time": "2023-04-28T13:34:27.266223Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_models_from_pickle(filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Loads the language models from a pickle file\n",
    "    Returns:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # load run_match_language_models from a pickle file\n",
    "    with open('run_match_language_models.pickle', 'rb') as handle:\n",
    "        language_models_dict = pickle.load(handle)\n",
    "    return language_models_dict\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:34:27.225643Z",
     "end_time": "2023-04-28T13:34:27.272223Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved run_match_language_models.pickle\n"
     ]
    }
   ],
   "source": [
    "# save the language models to a pickle file\n",
    "# (remove/put # in front of the next lines to save the language models)\n",
    "save_models_to_pickle(run_match_language_models, \"run_match_language_models.pickle\")\n",
    "print(\"saved run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:34:27.239037Z",
     "end_time": "2023-04-28T13:34:28.792952Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded run_match_language_models.pickle\n"
     ]
    }
   ],
   "source": [
    "# load the language models from a pickle file\n",
    "# (remove/put # in front of the next lines to load the language models)\n",
    "run_match_language_models = load_models_from_pickle(\"run_match_language_models.pickle\")\n",
    "print(\"loaded run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:34:28.794776Z",
     "end_time": "2023-04-28T13:34:31.884067Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg4h5Cl0q2nR"
   },
   "source": [
    "**Part 6**\n",
    "\n",
    "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
    "\n",
    "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be accepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qD6IRIQLrlZF",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:34:31.888487Z",
     "end_time": "2023-04-28T13:34:31.928613Z"
    }
   },
   "source": [
    "def classify(n=3, add_one=False):\n",
    "    \"\"\" Classifies the sentences in the test file, using the language models\n",
    "    Args:\n",
    "        n: the n-gram model to use\n",
    "        add_one: whether to use add_one smoothing or not\n",
    "    Returns:\n",
    "        classification_result: a list of tuples of the form (tweet_id, sentence, true_language, predicted_language)\n",
    "    \"\"\"\n",
    "    # we will use the language models from part 5, with n = 3 and add_one = True\n",
    "    language_models = run_match_language_models[(n, add_one)]\n",
    "\n",
    "    # read the test file, tweet_id, tweet_text, label\n",
    "    test_data = pd.read_csv(path + \"/test.csv\",  encoding=\"utf-8\")\n",
    "\n",
    "    # classify the sentences\n",
    "    classification_result = []\n",
    "\n",
    "    # iterate over the rows in the test data\n",
    "    for index, row in test_data.iterrows():\n",
    "        tweet_id = row['tweet_id']\n",
    "        sentence = row['tweet_text']\n",
    "        true_language = row['label']\n",
    "        predicted_language = ''\n",
    "\n",
    "        predicted_language = single_classification(sentence, language_models, n)\n",
    "\n",
    "        # add the result to the classification_result list\n",
    "        classification_result.append((tweet_id, sentence, true_language, predicted_language))\n",
    "\n",
    "    return classification_result\n",
    "\n",
    "\n",
    "def single_classification(sentence, language_models = run_match_language_models[(3, True)], n=3):\n",
    "    \"\"\" Classifies a single sentence, using the language models\n",
    "    Args:\n",
    "        sentence: the sentence to classify\n",
    "        language_models: the language models to use\n",
    "        n: the n-gram model to use\n",
    "    Returns:\n",
    "        predicted_language: the predicted language of the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    predicted_language = ''\n",
    "    min_perplexity = float('inf')\n",
    "    # iterate over the language models\n",
    "    for data_file in data_files:\n",
    "        current_model = language_models[data_file]\n",
    "        # create a temporary DataFrame, with the sentence as the only row\n",
    "        temp_df = pd.DataFrame([sentence], columns=['tweet_text'])\n",
    "        # calculate the perplexity using the temporary DataFrame\n",
    "        current_perplexity = calculate_perplexity(temp_df, n, current_model)\n",
    "\n",
    "        if current_perplexity < min_perplexity:\n",
    "            min_perplexity = current_perplexity\n",
    "            predicted_language = data_file[:-4] # remove the .csv from the end of the file name\n",
    "    return predicted_language\n",
    "\n"
   ],
   "execution_count": 26,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed classification for n = 1, add_one = False\n",
      "completed classification for n = 1, add_one = True\n",
      "completed classification for n = 2, add_one = False\n",
      "completed classification for n = 2, add_one = True\n",
      "completed classification for n = 3, add_one = False\n",
      "completed classification for n = 3, add_one = True\n",
      "completed classification for n = 4, add_one = False\n",
      "completed classification for n = 4, add_one = True\n"
     ]
    }
   ],
   "source": [
    "# classify the test sentences\n",
    "classification_result_n1 = classify(n=1, add_one=False)\n",
    "print(\"completed classification for n = 1, add_one = False\")\n",
    "\n",
    "classification_result_n1_add_one = classify(n=1, add_one=True)\n",
    "print(\"completed classification for n = 1, add_one = True\")\n",
    "\n",
    "classification_result_n2 = classify(n=2, add_one=False)\n",
    "print(\"completed classification for n = 2, add_one = False\")\n",
    "\n",
    "classification_result_n2_add_one = classify(n=2, add_one=True)\n",
    "print(\"completed classification for n = 2, add_one = True\")\n",
    "\n",
    "classification_result_n3 = classify(n=3, add_one=False)\n",
    "print(\"completed classification for n = 3, add_one = False\")\n",
    "\n",
    "classification_result_n3_add_one = classify(n=3, add_one=True)\n",
    "print(\"completed classification for n = 3, add_one = True\")\n",
    "\n",
    "classification_result_n4 = classify(n=4, add_one=False)\n",
    "print(\"completed classification for n = 4, add_one = False\")\n",
    "\n",
    "classification_result_n4_add_one = classify(n=4, add_one=True)\n",
    "print(\"completed classification for n = 4, add_one = True\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:34:31.901991Z",
     "end_time": "2023-04-28T13:36:23.732342Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "def get_accuracy(classification_result):\n",
    "    count_correct = 0\n",
    "    for result in classification_result:\n",
    "        predicted_language = result[3]\n",
    "        true_language = result[2]\n",
    "        if predicted_language == true_language:\n",
    "            count_correct += 1\n",
    "    return count_correct / len(classification_result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:23.735504Z",
     "end_time": "2023-04-28T13:36:23.762607Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for n = 1, add_one = False: 0.5639454931866483\n",
      "accuracy for n = 2, add_one = False: 0.8538567320915115\n",
      "accuracy for n = 3, add_one = False: 0.8821102637829729\n",
      "accuracy for n = 4, add_one = False: 0.794474309288661\n",
      "accuracy for n = 1, add_one = True: 0.6663332916614577\n",
      "accuracy for n = 2, add_one = True: 0.8744843105388174\n",
      "accuracy for n = 3, add_one = True: 0.9236154519314914\n",
      "accuracy for n = 4, add_one = True: 0.9179897487185898\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy of the classification\n",
    "print(\"accuracy for n = 1, add_one = False: \" + str(get_accuracy(classification_result_n1)))\n",
    "print(\"accuracy for n = 2, add_one = False: \" + str(get_accuracy(classification_result_n2)))\n",
    "print(\"accuracy for n = 3, add_one = False: \" + str(get_accuracy(classification_result_n3)))\n",
    "print(\"accuracy for n = 4, add_one = False: \" + str(get_accuracy(classification_result_n4)))\n",
    "\n",
    "print(\"accuracy for n = 1, add_one = True: \" + str(get_accuracy(classification_result_n1_add_one)))\n",
    "print(\"accuracy for n = 2, add_one = True: \" + str(get_accuracy(classification_result_n2_add_one)))\n",
    "print(\"accuracy for n = 3, add_one = True: \" + str(get_accuracy(classification_result_n3_add_one)))\n",
    "print(\"accuracy for n = 4, add_one = True: \" + str(get_accuracy(classification_result_n4_add_one)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:23.751286Z",
     "end_time": "2023-04-28T13:36:23.763622Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ECmLd3rktZ"
   },
   "source": [
    "**Part 7**\n",
    "\n",
    "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n",
    "\n",
    "Load the results to a CSV (using a DataFrame), with a model_name and f1_score Name it {student_id_1}\\_...\\_{student_id_n}\\_part7.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "  model_name  f1_score\n",
    "0    Model A      0.85\n",
    "1    Model B      0.92\n",
    "2    Model C      0.87\n",
    "3    Model D      0.90\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "KF3ImVdPgAGC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "def map_language_number(classification_result):\n",
    "    #we will use the following dictionary to convert the strings to numbers\n",
    "    language_to_number = {}\n",
    "    # we will use the following dictionary to convert the numbers back to strings\n",
    "    number_to_language = {}\n",
    "\n",
    "    number = 0\n",
    "    # iterate over the classification results\n",
    "    for result in classification_result:\n",
    "        if result[2] not in language_to_number:\n",
    "            language_to_number[result[2]] = number\n",
    "            number_to_language[number] = result[2]\n",
    "            number += 1\n",
    "        if result[3] not in language_to_number:\n",
    "            language_to_number[result[3]] = number\n",
    "            number_to_language[number] = result[3]\n",
    "            number += 1\n",
    "    return language_to_number, number_to_language"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:23.766746Z",
     "end_time": "2023-04-28T13:36:23.779907Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VOBO3YQls66r",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:23.783622Z",
     "end_time": "2023-04-28T13:36:24.440750Z"
    }
   },
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def calc_f1(result):\n",
    "    \"\"\" Calculates the f1 score of the classification result\n",
    "    Args:\n",
    "        result: a list of tuples, where each tuple contains the tweet_id, the sentence, the true language, and the predicted language\n",
    "    Returns:\n",
    "        f1_score: the f1 score of the classification result\n",
    "    \"\"\"\n",
    "    # create mappings from language to number and number to language\n",
    "    language_to_number, number_to_language = map_language_number(result)\n",
    "\n",
    "    # create a DataFrame with the results\n",
    "    df = pd.DataFrame(result, columns=['tweet_id', 'tweet_text', 'true_language', 'predicted_language'])\n",
    "    # drop the tweet_id and tweet_text columns\n",
    "    df = df.drop(columns=['tweet_id', 'tweet_text'])\n",
    "    # convert the true_language and predicted_language columns to numbers\n",
    "    df['true_language'] = df['true_language'].apply(lambda x: language_to_number[x])\n",
    "    df['predicted_language'] = df['predicted_language'].apply(lambda x: language_to_number[x])\n",
    "    # calculate the f1 score\n",
    "    f1_score = metrics.f1_score(df['true_language'], df['predicted_language'], average='weighted')\n",
    "    return f1_score"
   ],
   "execution_count": 31,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n = 1, add_one = False, f1_score = 0.559659649986144\n",
      "n = 2, add_one = False, f1_score = 0.8540032043148125\n",
      "n = 3, add_one = False, f1_score = 0.8825410316647341\n",
      "n = 4, add_one = False, f1_score = 0.7949199120142474\n",
      "n = 1, add_one = True, f1_score = 0.667513094316079\n",
      "n = 2, add_one = True, f1_score = 0.8750839401494929\n",
      "n = 3, add_one = True, f1_score = 0.9238768379120521\n",
      "n = 4, add_one = True, f1_score = 0.9183128996924542\n"
     ]
    }
   ],
   "source": [
    "# calculate the f1 score for each n\n",
    "f1_score_n1 = calc_f1(classification_result_n1)\n",
    "print(\"n = 1, add_one = False, f1_score = \" + str(f1_score_n1))\n",
    "\n",
    "f1_score_n2 = calc_f1(classification_result_n2)\n",
    "print(\"n = 2, add_one = False, f1_score = \" + str(f1_score_n2))\n",
    "\n",
    "f1_score_n3 = calc_f1(classification_result_n3)\n",
    "print(\"n = 3, add_one = False, f1_score = \" + str(f1_score_n3))\n",
    "\n",
    "f1_score_n4 = calc_f1(classification_result_n4)\n",
    "print(\"n = 4, add_one = False, f1_score = \" + str(f1_score_n4))\n",
    "\n",
    "f1_score_n1_add_one = calc_f1(classification_result_n1_add_one)\n",
    "print(\"n = 1, add_one = True, f1_score = \" + str(f1_score_n1_add_one))\n",
    "\n",
    "f1_score_n2_add_one = calc_f1(classification_result_n2_add_one)\n",
    "print(\"n = 2, add_one = True, f1_score = \" + str(f1_score_n2_add_one))\n",
    "\n",
    "f1_score_n3_add_one = calc_f1(classification_result_n3_add_one)\n",
    "print(\"n = 3, add_one = True, f1_score = \" + str(f1_score_n3_add_one))\n",
    "\n",
    "f1_score_n4_add_one = calc_f1(classification_result_n4_add_one)\n",
    "print(\"n = 4, add_one = True, f1_score = \" + str(f1_score_n4_add_one))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:24.442775Z",
     "end_time": "2023-04-28T13:36:24.549193Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               model_name  f1_score\n",
      "0  n = 1, add_one = False  0.559660\n",
      "1  n = 2, add_one = False  0.854003\n",
      "2  n = 3, add_one = False  0.882541\n",
      "3  n = 4, add_one = False  0.794920\n",
      "4   n = 1, add_one = True  0.667513\n",
      "5   n = 2, add_one = True  0.875084\n",
      "6   n = 3, add_one = True  0.923877\n",
      "7   n = 4, add_one = True  0.918313\n"
     ]
    }
   ],
   "source": [
    "# Load the results to a CSV (using a DataFrame), with a model_name and f1_score Name it {student_id_1}\\_...\\_{student_id_n}\\_part7.csv\n",
    "\n",
    "# create a DataFrame with the results\n",
    "df_part7 = pd.DataFrame(columns=['model_name', 'f1_score'])\n",
    "\n",
    "# add the results to the pd.DataFrame (without appending)\n",
    "df_part7.loc[0] = ['n = 1, add_one = False', f1_score_n1]\n",
    "df_part7.loc[1] = ['n = 2, add_one = False', f1_score_n2]\n",
    "df_part7.loc[2] = ['n = 3, add_one = False', f1_score_n3]\n",
    "df_part7.loc[3] = ['n = 4, add_one = False', f1_score_n4]\n",
    "df_part7.loc[4] = ['n = 1, add_one = True', f1_score_n1_add_one]\n",
    "df_part7.loc[5] = ['n = 2, add_one = True', f1_score_n2_add_one]\n",
    "df_part7.loc[6] = ['n = 3, add_one = True', f1_score_n3_add_one]\n",
    "df_part7.loc[7] = ['n = 4, add_one = True', f1_score_n4_add_one]\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "df_part7.to_csv(student_id_1 + \"_\" + student_id_2 + \"_part7.csv\", index=False)\n",
    "# print the DataFrame\n",
    "print(df_part7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:24.551708Z",
     "end_time": "2023-04-28T13:36:24.595083Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br><br><br>\n",
    "**Part 8**  \n",
    "Let's use your Language model (dictionary) for generation (NLG).\n",
    "\n",
    "When it comes to sampling from a language model decoder during text generation, there are several different methods that can be used to control the randomness and diversity of the generated text. \n",
    "\n",
    "Some of the most commonly used methods include:\n",
    "\n",
    "> `Greedy sampling`\n",
    "In this method, the model simply selects the word with the highest probability as the next word at each time step. This method can produce fluent text, but it can also lead to repetitive or predictable output.\n",
    "\n",
    "> `Temperature scaling`  \n",
    "Temperature scaling involves scaling the logits output of the language model by a temperature parameter before softmax normalization. This has the effect of smoothing the distribution of probabilities and increasing the probability of lower-probability words, which can lead to more diverse and creative output.\n",
    "\n",
    "> `Top-K sampling`  \n",
    "In this method, the model restricts the sampling to the top-K most likely words at each time step, where K is a predefined hyperparameter. This can generate more diverse output than greedy sampling, while limiting the number of low-probability words that are sampled.\n",
    "\n",
    "> `Nucleus sampling` (also known as top-p sampling)  \n",
    "This method restricts the sampling to the smallest possible set of words whose cumulative probability exceeds a certain threshold, defined by a hyperparameter p. Like top-K sampling, this can generate more diverse output than greedy sampling, while avoiding sampling extremely low probability words.\n",
    "\n",
    "> `Beam search`  \n",
    "Beam search involves maintaining a fixed number k of candidate output sequences at each time step, and then selecting the k most likely sequences based on their probabilities. This can improve the fluency and coherence of the output, but may not produce as much diversity as sampling methods.\n",
    "\n",
    "The choice of sampling method depends on the specific application and desired balance between fluency, diversity, and randomness. Hyperparameters such as temperature, K, p, and beam size can also be tuned to adjust the behavior of the language model during sampling.\n",
    "\n",
    "\n",
    "You may read more about this concept in <a href='https://huggingface.co/blog/how-to-generate#:~:text=pad_token_id%3Dtokenizer.eos_token_id)-,Greedy%20Search,-Greedy%20search%20simply'>this</a> blog post.\n"
   ],
   "metadata": {
    "id": "NfBYgfjADNPL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Please added the needed code for each sampeling method:**"
   ],
   "metadata": {
    "id": "GbReeHtwNWKS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def softmax(probabilities):\n",
    "    \"\"\" Applies the softmax function to the probabilities\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities (not yet probabilities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a dictionary of probabilities (normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    # convert the dictionary to a numpy array\n",
    "    np_probabilities = np.array(list(probabilities.values()))\n",
    "    # apply the softmax function\n",
    "    np_probabilities = np.exp(np_probabilities)\n",
    "    np_probabilities = np_probabilities / np.sum(np_probabilities)\n",
    "    # convert the numpy array back to a dictionary\n",
    "    probabilities = {key: value for key, value in zip(probabilities.keys(), np_probabilities)}\n",
    "    return probabilities\n",
    "\n",
    "def make_prob_1(probabilities):\n",
    "    \"\"\" Makes the sum of the probabilities equal to 1\n",
    "    Args:\n",
    "        probabilities: a list of probabilities (not yet probabilities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a list of probabilities (normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    # by dividing each probability by the sum of all probabilities\n",
    "    sum_prob = sum(probabilities)\n",
    "    probabilities = [prob / sum_prob for prob in probabilities]\n",
    "    return probabilities\n",
    "\n",
    "def get_correct_model(all_models=run_match_language_models, prefix=\"<start>\", language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Gets the correct model for the prefix and language\n",
    "    Args:\n",
    "        all_models: a dictionary of all the language models\n",
    "        prefix: the prefix of the tweet\n",
    "        language: the language of the tweet\n",
    "        add_one: whether to use add_one smoothing or not\n",
    "        max_n: the maximum n-gram to use\n",
    "    Returns:\n",
    "        correct_model: the correct language model\n",
    "        tuple_key_prefix: the tuple key of the prefix\n",
    "        next_token_probabilities: the probabilities of the next token\n",
    "    \"\"\"\n",
    "    prefix_tokens = tweet_to_token_tuples(prefix)\n",
    "\n",
    "    # we want to use maximum n-gram we can, but not more than max_n\n",
    "    n = min(max_n, len(prefix_tokens) + 1)\n",
    "\n",
    "    # get the n-gram model\n",
    "    correct_model = all_models[(n, add_one)][language]\n",
    "\n",
    "    # get the n-1 tokens of the prefix, i.e. the key for the language model\n",
    "    tuple_key_prefix = tuple(prefix_tokens[-(n - 1):])\n",
    "\n",
    "    # if the prefix is not in the language model, sample a random key from the language model\n",
    "    if tuple_key_prefix not in correct_model:\n",
    "        tuple_key_prefix = random.choice(list(correct_model.keys()))\n",
    "\n",
    "\n",
    "    # get the probabilities of the next token\n",
    "    next_token_probabilities = correct_model[tuple_key_prefix]\n",
    "    return correct_model, tuple_key_prefix, next_token_probabilities\n",
    "\n",
    "def get_notInModel_probabilities(probabilities, notInModel_count, vocabulary=vocabulary):\n",
    "    \"\"\" Removes <notInModel> from the probabilities dictionary and adds the probabilities of the tokens not in the model\n",
    "    Args:\n",
    "        vocabulary: the vocabulary of the language model\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        notInModel_count: the number of tokens not in the model\n",
    "    Returns:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "    \"\"\"\n",
    "    # add all tokens not in the model to the probabilities dictionary\n",
    "    if '<notInModel>' in probabilities:\n",
    "        single_notInModel_prob = probabilities['<notInModel>'] / notInModel_count\n",
    "        tokens_not_in_model_vocab = set(vocabulary) - set(probabilities.keys())\n",
    "        for token in tokens_not_in_model_vocab:\n",
    "            probabilities[token] = single_notInModel_prob\n",
    "        del probabilities['<notInModel>']\n",
    "    return probabilities\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:24.569914Z",
     "end_time": "2023-04-28T13:36:24.596783Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_greedy(probabilities, k=1):\n",
    "    \"\"\" Samples the next token greedily, i.e. the token with the highest probability\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "    Returns:\n",
    "        max_token: the token with the highest probability\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # reduce the probability of the token <notInModel> by the number of times it was sampled\n",
    "    if '<notInModel>' in probabilities_copy:\n",
    "        probabilities_copy['<notInModel>'] /= notInModel_count\n",
    "\n",
    "    # sort the probabilities by value\n",
    "    sorted_probabilities = sorted(probabilities_copy.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # if k is larger than the number of probabilities, set k to the number of probabilities\n",
    "    k = k if len(sorted_probabilities) >= k else len(sorted_probabilities)\n",
    "\n",
    "    # sample the token with the k highest probability\n",
    "    next_token = sorted_probabilities[k - 1][0]\n",
    "\n",
    "    # if the token is <notInModel>, sample a random token from the tokens not in the model\n",
    "    if next_token == '<notInModel>':\n",
    "        # get tokens not in the model\n",
    "        tokens_not_in_model_vocab = set(vocabulary) - set(probabilities_copy.keys())\n",
    "        # sample a random token from the tokens not in the model\n",
    "        next_token = random.choice(list(tokens_not_in_model_vocab))\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_temperature(probabilities, temperature=1.0, k=1):\n",
    "    \"\"\" Samples the next token using temperature sampling\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        temperature: the temperature\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # remove <notInModel> from the probabilities dictionary and add the probabilities of the tokens not in the model\n",
    "    probabilities_copy = get_notInModel_probabilities(probabilities=probabilities_copy, notInModel_count=notInModel_count)\n",
    "\n",
    "    # scale the probabilities by the temperature\n",
    "    probabilities_copy = {key: value ** (1 / temperature) for key, value in probabilities_copy.items()}\n",
    "\n",
    "    # softmax the probabilities\n",
    "    probabilities_copy = softmax(probabilities_copy)\n",
    "\n",
    "    # sample from the probabilities dictionary, use the np.random.choice function\n",
    "    np_probabilities = np.array(list(probabilities_copy.values()))\n",
    "    np_tokens = np.array(list(probabilities_copy.keys()))\n",
    "\n",
    "    # convert the tokens (tuple) to a list of strings\n",
    "    np_tokens = [\"\".join(token) for token in np_tokens]\n",
    "\n",
    "    # sample the next token\n",
    "    next_token = np.random.choice(np_tokens, p=np_probabilities)\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def sample_topK(probabilities, k=1):\n",
    "    \"\"\" Samples the next token using top-k sampling, i.e. only the top k tokens are considered\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        k: the number of tokens to consider\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # add all tokens not in the model to the probabilities dictionary\n",
    "    probabilities_copy = get_notInModel_probabilities(probabilities=probabilities_copy, notInModel_count=notInModel_count)\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities_copy.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # take the top k\n",
    "    top_k = sorted_probabilities[:k]\n",
    "\n",
    "    # split the top k into tokens and probabilities\n",
    "    top_k_probs = [prob for (token, prob) in top_k]\n",
    "    top_k_tokens = [token for (token, prob) in top_k]\n",
    "\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_k_probs = make_prob_1(top_k_probs)\n",
    "\n",
    "    # sample from the top k tokens\n",
    "    next_token = np.random.choice(top_k_tokens, p=top_k_probs)\n",
    "\n",
    "    return next_token\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_topP(probabilities, p=0.9):\n",
    "    \"\"\" Samples the next token using top-p sampling,\n",
    "    i.e. only the tokens with the highest probabilities are considered, until the sum of the probabilities is greater than p\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        p: the threshold\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "     # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # remove <notInModel> from the probabilities dictionary and add the probabilities of the tokens not in the model\n",
    "    probabilities_copy = get_notInModel_probabilities(probabilities=probabilities_copy, notInModel_count=notInModel_count)\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities_copy.items(), key=lambda x: x[1], reverse=True)\n",
    "    current_sum = 0\n",
    "    top_p_tokens = []\n",
    "    top_p_probs = []\n",
    "    current_index = 0\n",
    "    # add the top tokens until the sum of the probabilities is greater than p\n",
    "    while current_sum < p:\n",
    "        top_p_tokens.append(sorted_probabilities[current_index][0])\n",
    "        top_p_probs.append(sorted_probabilities[current_index][1])\n",
    "        current_sum += sorted_probabilities[current_index][1]\n",
    "        current_index += 1\n",
    "\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_p_probs = make_prob_1(top_p_probs)\n",
    "    # convert the tokens (tuple) to a list of strings\n",
    "    top_p_tokens = [\"\".join(token) for token in top_p_tokens]\n",
    "    # sample from the top p tokens\n",
    "    next_token = np.random.choice(top_p_tokens, p=top_p_probs)\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def sample_beam(probabilities, num_beams = 3):\n",
    "    \"\"\" Samples the next tokens using beam search, i.e., keeps the top num_beams hypotheses at each step.\n",
    "        Helper function for beam_search\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        num_beams: the number of beams to keep, i.e. the number of hypotheses to keep at each step\n",
    "    Returns:\n",
    "        beam_tokens: a list of top num_beams tokens\n",
    "        beam_probs: a list of the corresponding probabilities of the top num_beams tokens\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # remove <notInModel> from the probabilities dictionary and add the probabilities of the tokens not in the model\n",
    "    probabilities_copy = get_notInModel_probabilities(probabilities=probabilities_copy, notInModel_count=notInModel_count)\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities_copy.items(), key=lambda x: x[1], reverse=True)\n",
    "    # take the top num_beams\n",
    "    top_beams = sorted_probabilities[:num_beams]\n",
    "\n",
    "    beam_tokens = [token for (token, prob) in top_beams]\n",
    "    beam_probs = [prob for (token, prob) in top_beams]\n",
    "\n",
    "    return beam_tokens, beam_probs\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "v4TrLs1kI3fW",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:24.590644Z",
     "end_time": "2023-04-28T13:36:24.611168Z"
    }
   },
   "execution_count": 35,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your Language Model to generate each one out of the following examples with the coresponding params.    \n",
    "Notice the 4 core issues: \n",
    "- Starting tokens\n",
    "- Length of the generation\n",
    "- Sampling methond (use all)\n",
    "- Stop Token (if this token is sampled, stop generating)"
   ],
   "metadata": {
    "id": "Giylo6-lI21t"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your LM to generate a string based on the parametes of each examples, and store the generation sequance at the generation list."
   ],
   "metadata": {
    "id": "YTbF-9zKVchQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_string(all_models, prefix='<start>', sampling_method='beam', gen_length=10, stop_token='<end>', num_beams=5, add_one=False):\n",
    "    \"\"\" Generates a string using the specified sampling method\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = (prefix, sampling_method, gen_length, stop_token, num_beams), value = generation}\n",
    "        prefix: the prefix of the generation\n",
    "        sampling_method: the sampling method to use\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token to stop the generation\n",
    "        num_beams: the number of beams to keep, i.e. the number of hypotheses to keep at each step\n",
    "        add_one: whether to add one to the count of each token\n",
    "    Returns:\n",
    "        generation: the generated string\n",
    "    \"\"\"\n",
    "    if sampling_method == 'beam':\n",
    "        return beam_search(all_models= all_models, prefix= prefix, gen_length= gen_length, stop_token= stop_token, num_beams= num_beams, add_one= add_one)\n",
    "    else:\n",
    "        return generate_string_not_beam(all_models= all_models, prefix= prefix, sampling_method= sampling_method, gen_length= gen_length, stop_token= stop_token, add_one= add_one)\n",
    "\n",
    "def beam_search(all_models, prefix=\"<start>\", gen_length=10, stop_token=\"<end>\", num_beams=5, language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Generates a string using beam search\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        num_beams: the number of beams to keep\n",
    "        language: the language of the model\n",
    "        add_one: whether to use add one smoothing or not\n",
    "        max_n: the maximum n-gram to use\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    # initialize the beams\n",
    "    beams = [(prefix, 0)]  # (prefix, log_prob)\n",
    "\n",
    "    # generate the string token by token\n",
    "    for _ in range(gen_length):\n",
    "        new_beams = []\n",
    "\n",
    "        # sample the next token for each beam\n",
    "        for beam_prefix, beam_log_prob in beams:\n",
    "            # get the correct language model\n",
    "            current_lm, current_tuple_key_prefix, next_token_probabilities = get_correct_model(all_models=all_models,\n",
    "                                                                                               prefix=beam_prefix, language=language,\n",
    "                                                                                               add_one=add_one, max_n=max_n)\n",
    "\n",
    "            # sample the top num_beams tokens and probabilities\n",
    "            beam_tokens, beam_probs = sample_beam(next_token_probabilities, num_beams)\n",
    "\n",
    "            # update the beams\n",
    "            for token, prob in zip(beam_tokens, beam_probs):\n",
    "                if not (beam_prefix.endswith(stop_token) or beam_prefix.endswith(\"<end>\")):\n",
    "                    new_prefix, new_log_prob = update_beam(beam_prefix, beam_log_prob, token, prob)\n",
    "                    new_beams.append((new_prefix, new_log_prob))\n",
    "                else:\n",
    "                    new_beams.append((beam_prefix, beam_log_prob))\n",
    "\n",
    "\n",
    "        # keep the top num_beams beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:num_beams]\n",
    "\n",
    "    # get the best beam\n",
    "    best_beam = beams[0][0]\n",
    "\n",
    "    return best_beam\n",
    "\n",
    "def generate_string_not_beam(all_models, prefix='<start>', gen_length=10, stop_token='<end>', sampling_method='topK', language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Generates a string using the given language model\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "        language: the language of the model\n",
    "        add_one: whether to use add one smoothing or not\n",
    "        max_n: the maximum n-gram to use\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    current_prefix = prefix\n",
    "\n",
    "    # generate the string token by token\n",
    "    for _ in range(gen_length):\n",
    "        # get the correct language model\n",
    "        current_lm, current_tuple_key_prefix, next_token_probabilities = get_correct_model(all_models=all_models,\n",
    "                                                                                           prefix=current_prefix, language=language,\n",
    "                                                                                           add_one=add_one, max_n=max_n)\n",
    "\n",
    "        # sample the next token\n",
    "        next_token = select_next_token(next_token_probabilities, sampling_method)\n",
    "\n",
    "        # if next_token == \"<notInModel>\": uniform sample from the group (vocabulary - next_token_probabilities.keys())\n",
    "        if next_token == \"<notInModel>\":\n",
    "            tokens_not_in_model = list(set(vocabulary) - set(next_token_probabilities.keys()))\n",
    "            next_token = random.choice(tokens_not_in_model)\n",
    "\n",
    "        # update the current prefix\n",
    "        current_prefix += next_token\n",
    "\n",
    "        # stop if the stop token was sampled\n",
    "        if current_prefix.endswith(stop_token) or next_token == \"<end>\":\n",
    "            break\n",
    "\n",
    "\n",
    "    return current_prefix\n",
    "\n",
    "def update_beam(beam_prefix, beam_log_prob, token, prob):\n",
    "    \"\"\" Updates the beam\n",
    "    Args:\n",
    "        beam_prefix: the current beam prefix\n",
    "        beam_log_prob: the current beam log probability\n",
    "        token: the token to add to the beam\n",
    "        prob: the probability of the token\n",
    "    Returns:\n",
    "        new_prefix: the new beam prefix\n",
    "        new_log_prob: the new beam log probability\n",
    "    \"\"\"\n",
    "    # update the prefix and the log probability\n",
    "    new_prefix = beam_prefix + token\n",
    "    new_log_prob = beam_log_prob + np.log(prob)\n",
    "\n",
    "    return new_prefix, new_log_prob\n",
    "\n",
    "def select_next_token(next_token_probabilities, sampling_method='topK', k_greedy=1, temperature=0.5, top_k=5, p=0.5):\n",
    "    \"\"\" Selects the next token, (greedy, temperature, topK, topP)\n",
    "    Args:\n",
    "        next_token_probabilities: the probabilities of the next token\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "        k_greedy: the number of top tokens to consider for greedy sampling\n",
    "        temperature: the temperature for temperature sampling\n",
    "        top_k: the number of top tokens to consider for topK sampling\n",
    "        p: the probability mass for topP sampling\n",
    "    Returns:\n",
    "        next_token: the next token\n",
    "    \"\"\"\n",
    "    if sampling_method == 'greedy':\n",
    "        return sample_greedy(next_token_probabilities, k_greedy)\n",
    "    elif sampling_method == 'temperature':\n",
    "        return sample_temperature(next_token_probabilities, temperature)\n",
    "    elif sampling_method == 'topK':\n",
    "        return sample_topK(next_token_probabilities, top_k)\n",
    "    elif sampling_method == 'topP':\n",
    "        return sample_topP(next_token_probabilities, p)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown sampling method: {sampling_method}')\n",
    "\n"
   ],
   "metadata": {
    "id": "3zf-omUXQezz",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:24.621814Z",
     "end_time": "2023-04-28T13:36:24.654718Z"
    }
   },
   "execution_count": 36,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "test_ = {\n",
    "    'example1' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['greedy','beam'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example2' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['temperature','topK','topP'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example3' : {\n",
    "        'start_tokens' : \"He\",\n",
    "        'sampling_method' : ['greedy','beam','temperature','topK','topP'],\n",
    "        'gen_length' : \"20\",\n",
    "        'stop_token' : \"me\",\n",
    "        'generation' : []\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:24.628666Z",
     "end_time": "2023-04-28T13:36:24.665203Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "all_models = run_match_language_models\n",
    "language = \"en.csv\"\n",
    "add_one = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:24.642475Z",
     "end_time": "2023-04-28T13:36:24.665203Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "# clear the generations, useful if you want to run the code multiple times\n",
    "for example in test_:\n",
    "    test_[example]['generation'] = []\n",
    "\n",
    "# generate the strings for each example\n",
    "for example in test_:\n",
    "    # iterate over the sampling methods\n",
    "    for i in range(len(test_[example]['sampling_method'])):\n",
    "        # get the parameters\n",
    "        sampling_method = test_[example]['sampling_method'][i]\n",
    "        gen_length = int(test_[example]['gen_length'])\n",
    "        stop_token = test_[example]['stop_token']\n",
    "        prefix = test_[example]['start_tokens']\n",
    "\n",
    "        # generate the string\n",
    "        generated_string = generate_string(all_models=all_models, prefix=prefix, sampling_method=sampling_method, gen_length=gen_length, stop_token=stop_token, add_one=add_one)\n",
    "\n",
    "        # cut the start_token from the generated string\n",
    "        generated_string = generated_string[len(prefix):]\n",
    "\n",
    "        # store the string\n",
    "        test_[example]['generation'].append(generated_string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:36:24.660209Z",
     "end_time": "2023-04-28T13:36:24.672610Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example example1:\n",
      "Start tokens: H\n",
      "full string using greedy : House the s\n",
      "generated string using greedy : ouse the s\n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "full string using beam : Healthcare \n",
      "generated string using beam : ealthcare \n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "\n",
      "Example example2:\n",
      "Start tokens: H\n",
      "full string using temperature : HJ htâ€¦ so! \n",
      "generated string using temperature : J htâ€¦ so! \n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "full string using topK : Have you wh\n",
      "generated string using topK : ave you wh\n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "full string using topP : How https:/\n",
      "generated string using topP : ow https:/\n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "\n",
      "Example example3:\n",
      "Start tokens: He\n",
      "full string using greedy : Health the so much 24t\n",
      "generated string using greedy : alth the so much 24t\n",
      "generation char length: 20\n",
      "generation token length: 20\n",
      "full string using beam : Healthcare in that the\n",
      "generated string using beam : althcare in that the\n",
      "generation char length: 20\n",
      "generation token length: 20\n",
      "full string using temperature : Helie: nigrin.: pagail\n",
      "generated string using temperature : lie: nigrin.: pagail\n",
      "generation char length: 20\n",
      "generation token length: 20\n",
      "full string using topK : Herenting a chalized t\n",
      "generated string using topK : renting a chalized t\n",
      "generation char length: 20\n",
      "generation token length: 20\n",
      "full string using topP : Here have a be @Calum5\n",
      "generated string using topP : re have a be @Calum5\n",
      "generation char length: 20\n",
      "generation token length: 20\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the generations and the number of tokens\n",
    "\n",
    "# iterate over the examples\n",
    "for example in test_:\n",
    "    print(f\"Example {example}:\")\n",
    "    start_token = test_[example]['start_tokens']\n",
    "    print(\"Start tokens:\", start_token)\n",
    "    # iterate over the sampling methods\n",
    "    for i in range(len(test_[example]['generation'])):\n",
    "        sampling_method = test_[example]['sampling_method'][i]\n",
    "        generated_string = test_[example]['generation'][i]\n",
    "        print(\"full string using\", sampling_method, \":\", start_token + generated_string)\n",
    "        print(\"generated string using\", sampling_method, \":\", generated_string)\n",
    "        print(\"generation char length:\", len(generated_string))\n",
    "        print(\"generation token length:\", len(tweet_to_token_tuples(generated_string)))\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T13:45:19.694210Z",
     "end_time": "2023-04-28T13:45:19.720947Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### do not change ###\n",
    "print('-------- NLG --------')\n",
    "\n",
    "for k,v in test_.items():\n",
    "  l = ''.join([f'\\t{sm} >> {v[\"start_tokens\"]}{g}\\n' for sm,g in zip(v['sampling_method'],v['generation'])])\n",
    "  print(f'{k}:')\n",
    "  print(l)"
   ],
   "metadata": {
    "id": "bvla30-lVw8n",
    "ExecuteTime": {
     "start_time": "2023-04-28T13:45:19.711432Z",
     "end_time": "2023-04-28T13:45:19.752680Z"
    }
   },
   "execution_count": 43,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- NLG --------\n",
      "example1:\n",
      "\tgreedy >> House the s\n",
      "\tbeam >> Healthcare \n",
      "\n",
      "example2:\n",
      "\ttemperature >> HJ htâ€¦ so! \n",
      "\ttopK >> Have you wh\n",
      "\ttopP >> How https:/\n",
      "\n",
      "example3:\n",
      "\tgreedy >> Health the so much 24t\n",
      "\tbeam >> Healthcare in that the\n",
      "\ttemperature >> Helie: nigrin.: pagail\n",
      "\ttopK >> Herenting a chalized t\n",
      "\ttopP >> Here have a be @Calum5\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEtckSWNANqW"
   },
   "source": [
    "<br><br><br>\n",
    "# **Good luck!**"
   ]
  }
 ]
}
