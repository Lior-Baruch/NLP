{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3 (ipykernel)"
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7ce5pQK3bFn_"
   },
   "source": [
    "# Assignment 1\n",
    "In this assignment you will be creating tools for learning and testing language models.\n",
    "The corpora that you will be working with are lists of tweets in 8 different languages that use the Latin script. The data is provided either formatted as CSV or as JSON, for your convenience. The end goal is to write a set of tools that can detect the language of a given tweet.\n",
    "\n",
    "Do make sure all results are uploaded to CSVs (as well as printed to console) for your assignment to be fully graded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vwG8v-Ll49KM"
   },
   "source": [
    "*As a preparation for this task, download the data files from the course git repository.\n",
    "\n",
    "The relevant files are under **lm-languages-data-new**:\n",
    "\n",
    "\n",
    "*   en.csv (or the equivalent JSON file)\n",
    "*   es.csv (or the equivalent JSON file)\n",
    "*   fr.csv (or the equivalent JSON file)\n",
    "*   in.csv (or the equivalent JSON file)\n",
    "*   it.csv (or the equivalent JSON file)\n",
    "*   nl.csv (or the equivalent JSON file)\n",
    "*   pt.csv (or the equivalent JSON file)\n",
    "*   tl.csv (or the equivalent JSON file)\n",
    "*   test.csv (or the equivalent JSON file)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# !pip install numpy pandas emoji\n",
    "import emoji\n",
    "import pandas as pd\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:12.096969Z",
     "end_time": "2023-04-28T21:28:12.825945Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7xC-87z2GWMq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "7b7f40be-bf9b-4d6c-da81-25d60d710a75",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:12.820315Z",
     "end_time": "2023-04-28T21:28:12.845435Z"
    }
   },
   "source": [
    "#!git clone https://github.com/kfirbar/nlp-course.git"
   ],
   "execution_count": 2,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOVb4IhsqimJ"
   },
   "source": [
    "\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    "**Important note: please use only the files under lm-languages-data-new and NOT under lm-languages-data**\n",
    "\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QYdhPfbAGkip",
    "outputId": "af6566c6-e6e6-409a-c569-8fab9bdf400e",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:12.836697Z",
     "end_time": "2023-04-28T21:28:12.858367Z"
    }
   },
   "source": [
    "\n",
    "#!ls nlp-course/lm-languages-data-new\n"
   ],
   "execution_count": 3,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "student_id_1= \"311451660\"\n",
    "student_id_2= \"327156998\"\n",
    "\n",
    "path = \"nlp-course/lm-languages-data-new\"\n",
    "data_files = [\"en.csv\", \"es.csv\", \"fr.csv\", \"in.csv\", \"it.csv\", \"nl.csv\", \"pt.csv\", \"tl.csv\"]\n",
    "test_file = \"test.csv\""
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:12.853151Z",
     "end_time": "2023-04-28T21:28:12.879460Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ashyu_mT28o6"
   },
   "source": [
    "**Part 1**\n",
    "\n",
    "Write a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data. **Our token definition is a single UTF-8 encoded character**. So, the vocabulary list is a simple Python list of all the characters that you see at least once in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "\n",
    "def tweet_to_token_tuples(tweet, start_token=\"<start>\", end_token=\"<end>\"):\n",
    "    \"\"\" Converts a tweet to a list of tokens (characters, emojis, and start and end tokens)\n",
    "    Args:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    Returns:\n",
    "        token_tuple: a tuple of tokens (characters, emojis, and start and end tokens)\n",
    "    \"\"\"\n",
    "    token_tuple = []\n",
    "\n",
    "    # remove the start and end tokens from the tweet (if they exist) and return the tweet and the start and end tokens\n",
    "    tweet, start_token, end_token = get_start_end_tokens_and_remove_from_tweet(tweet, start_token, end_token)\n",
    "\n",
    "    # get the start and end index of all the emojis in the tweet\n",
    "    emojis_info = emoji.emoji_list(tweet) # a list of dictionaries, each dictionary contains the start and end index of an emoji in the tweet\n",
    "    emojis_info = {info[\"match_start\"]: info for info in emojis_info}\n",
    "\n",
    "    # add the start token to the token list\n",
    "    if start_token is not None:\n",
    "        token_tuple.append(start_token)\n",
    "\n",
    "    # iterate over all the characters in the tweet\n",
    "    char_index = 0\n",
    "    while char_index < len(tweet):\n",
    "        # if the current character is the start of an emoji, add the emoji to the token list and move the char_index to the end of the emoji\n",
    "        if char_index in emojis_info:\n",
    "            token_tuple.append(emojis_info[char_index][\"emoji\"])\n",
    "            char_index = emojis_info[char_index][\"match_end\"]\n",
    "        else:\n",
    "            token_tuple.append(tweet[char_index])\n",
    "            char_index += 1\n",
    "\n",
    "    # add the end token to the token list\n",
    "    if end_token is not None:\n",
    "        token_tuple.append(end_token)\n",
    "\n",
    "    # convert the token list to a tuple (so it will be hashable)\n",
    "    token_tuple = tuple(token_tuple)\n",
    "    return token_tuple\n",
    "\n",
    "def get_start_end_tokens_and_remove_from_tweet(tweet, start_token, end_token):\n",
    "    \"\"\" Removes the start and end tokens from the tweet (if they exist) and returns the tweet and the start and end tokens\n",
    "    Args:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    Returns:\n",
    "        tweet: a string representing a tweet (in UTF-8)\n",
    "        start_token: a string representing the start token\n",
    "        end_token: a string representing the end token\n",
    "    \"\"\"\n",
    "    if tweet.startswith(start_token):\n",
    "        tweet = tweet[len(start_token):]\n",
    "        #print(tweet)\n",
    "    else:\n",
    "        start_token = None\n",
    "\n",
    "    if tweet.endswith(end_token):\n",
    "        tweet = tweet[:-len(end_token)]\n",
    "        #print(tweet)\n",
    "    else:\n",
    "        end_token = None\n",
    "\n",
    "    return tweet, start_token, end_token"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:12.870510Z",
     "end_time": "2023-04-28T21:28:12.879460Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "xCfzsITW8Yaj",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:12.882905Z",
     "end_time": "2023-04-28T21:28:12.896722Z"
    }
   },
   "source": [
    "# a function *preprocess* that iterates over all the data files and creates a single vocabulary, containing all the tokens in the data.\n",
    "# the data in the files are in the form: tweet_id,tweet_text\n",
    "\n",
    "def preprocess():\n",
    "    \"\"\" Creates a vocabulary from the data files\n",
    "    Returns:\n",
    "        vocabulary: a list of all the characters that appear in the data files\n",
    "    \"\"\"\n",
    "    vocabulary = set()\n",
    "    # iterate over all the data files\n",
    "    for file in data_files:\n",
    "        # read the data file\n",
    "        current_data = pd.read_csv(path + \"/\" + file, encoding=\"utf-8\")\n",
    "        # iterate over all the tweets in the data file\n",
    "        for tweet in current_data[\"tweet_text\"]:\n",
    "            # convert the tweet to a list of tokens\n",
    "            tweet = tweet_to_token_tuples(tweet)\n",
    "            # iterate over all the tokens in the tweet\n",
    "            for token in tweet:\n",
    "                # add the token to the vocabulary\n",
    "                vocabulary.add(token)\n",
    "    # sort the vocabulary\n",
    "    vocabulary = sorted(list(vocabulary))\n",
    "    return vocabulary"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size:  2057\n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "vocabulary = preprocess()\n",
    "# add <start> and <end> to the vocabulary\n",
    "vocabulary = [\"<start>\"] + [\"<end>\"] + vocabulary\n",
    "\n",
    "# print(\"vocabulary: \", vocabulary)\n",
    "print(\"vocab size: \", len(vocabulary))\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:12.895728Z",
     "end_time": "2023-04-28T21:28:15.463491Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb2PGj0Yc2TY"
   },
   "source": [
    "**Part 2**\n",
    "\n",
    "Write a function `lm` that generates a language model from a textual corpus. The function should return a dictionary (representing a model) where the keys are all the relevant n-1 sequences, and the values are dictionaries with the n_th tokens and their corresponding probabilities to occur. For example, for a trigram model (tokens are characters), it should look something like:\n",
    "\n",
    "{\n",
    "  \"ab\":{\"c\":0.5, \"b\":0.25, \"d\":0.25},\n",
    "  \"ca\":{\"a\":0.2, \"b\":0.7, \"d\":0.1}\n",
    "}\n",
    "\n",
    "which means for example that after the sequence \"ab\", there is a 0.5 chance that \"c\" will appear, 0.25 for \"b\" to appear and 0.25 for \"d\" to appear.\n",
    "\n",
    "Note - You should think how to add the add_one smoothing information to the dictionary and implement it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "# helper functions for lm\n",
    "\n",
    "def build_ngram_model(current_data, n):\n",
    "    \"\"\" Builds an n-gram model from the data\n",
    "    Args:\n",
    "        current_data: a pandas dataframe with a column named \"tweet_text\"\n",
    "        n: the n in n-gram\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}\n",
    "    \"\"\"\n",
    "    model = {}\n",
    "\n",
    "    if n == 1: # if n is 1, build a 1-gram model\n",
    "        model = build_1gram_model(current_data, model)\n",
    "    else: # if n is not 1, build an n-gram model\n",
    "        # iterate over all the tweets in the data file\n",
    "        for tweet in current_data[\"tweet_text\"]:\n",
    "            # add the start and end tokens to the tweet\n",
    "            tweet = \"<start> \" + tweet + \" <end>\"\n",
    "            # convert the tweet to a tuple of tokens\n",
    "            tweet = tweet_to_token_tuples(tweet)\n",
    "\n",
    "            # iterate over all the n-grams in the tweet\n",
    "            for i in range(len(tweet) - n + 1):\n",
    "                # define n_gram, n_minus_1_gram and n_th_token\n",
    "                n_gram = tweet[i:i+n]\n",
    "                n_minus_1_gram = n_gram[:-1]\n",
    "                n_th_token = n_gram[-1]\n",
    "\n",
    "                # add the n-gram to the model\n",
    "                model = add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_1gram_model(current_data, model):\n",
    "    \"\"\" Builds a 1-gram model from the data\n",
    "    Args:\n",
    "        current_data: a pandas dataframe with a column named \"tweet_text\"\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}\n",
    "    \"\"\"\n",
    "    token_counter = {}\n",
    "    # iterate over all the tweets in the data file\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        tweet = \"<start> \" + tweet + \" <end>\"\n",
    "        # convert the tweet to a tuple of tokens\n",
    "        tweet_tokens = tweet_to_token_tuples(tweet)\n",
    "        # iterate over all the tokens in the tweet\n",
    "        for token in tweet_tokens:\n",
    "            if token not in token_counter: # if the token is not in the counter, add it\n",
    "                token_counter[token] = 0\n",
    "            # count the token\n",
    "            token_counter[token] += 1\n",
    "\n",
    "    # add the n-gram to the model\n",
    "    model[()] = token_counter\n",
    "    return model\n",
    "\n",
    "def add_and_count_gram_to_model(model, n_minus_1_gram, n_th_token):\n",
    "    \"\"\" Adds and counts an n-gram to the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "        n_minus_1_gram: a tuple of n-1 tokens\n",
    "        n_th_token: the n_th token\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "    # add the n-gram to the model\n",
    "    if n_minus_1_gram not in model:\n",
    "        model[n_minus_1_gram] = {}\n",
    "    # add the n_th token to the model\n",
    "    if n_th_token not in model[n_minus_1_gram]:\n",
    "        model[n_minus_1_gram][n_th_token] = 0\n",
    "    # count the n_th token, i.e. add 1 to its count\n",
    "    model[n_minus_1_gram][n_th_token] += 1\n",
    "    return model\n",
    "\n",
    "\n",
    "def add_one_smoothing(model, vocabulary):\n",
    "    \"\"\" Adds add_one smoothing to the model\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    \"\"\"\n",
    "\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # add spacial key <not in model>\n",
    "        model[n_minus_1_gram][\"<notInModel>\"] = 0\n",
    "        # iterate over all the tokens in the vocabulary\n",
    "        for token in vocabulary:\n",
    "            # if the token is not in the model replace it with <notInModel>\n",
    "            if token not in model[n_minus_1_gram]:\n",
    "                token = \"<notInModel>\"\n",
    "\n",
    "            # count the token, i.e. add 1 to its count (add one smoothing)\n",
    "            model[n_minus_1_gram][token] += 1\n",
    "    return model\n",
    "\n",
    "def calculate_probabilities(model):\n",
    "    \"\"\" Calculates the probabilities of the model,\n",
    "        also adds meta_data to the model (total_count=total number of tokens, <notInModel>_count=number of tokens that are not in the model)\n",
    "    Args:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: count}}\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # iterate over all the n-1 grams in the model\n",
    "    for n_minus_1_gram in model:\n",
    "        # get the counts of all the tokens\n",
    "        token_counts = model[n_minus_1_gram].values()\n",
    "        # if model[n_minus_1_gram] has the key <notInModel> get its count\n",
    "        if \"<notInModel>\" in model[n_minus_1_gram]:\n",
    "            token_notInModel_count = model[n_minus_1_gram][\"<notInModel>\"]\n",
    "        else:\n",
    "            token_notInModel_count = 0\n",
    "        # calculate the total count\n",
    "        total_count = sum(token_counts)\n",
    "        # iterate over all the tokens in the model\n",
    "        for token in model[n_minus_1_gram]:\n",
    "            # calculate the probability, i.e. divide the count by the total count\n",
    "            model[n_minus_1_gram][token] /= total_count\n",
    "\n",
    "        model[n_minus_1_gram][\"meta_data\"] = {\"total_count\": total_count, \"<notInModel>_count\": token_notInModel_count}\n",
    "    return model\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:15.471008Z",
     "end_time": "2023-04-28T21:28:15.479613Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kMC_u8eQbVvZ",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:15.480764Z",
     "end_time": "2023-04-28T21:28:15.539619Z"
    }
   },
   "source": [
    "def lm(n, vocabulary, data_file_path, add_one):\n",
    "    \"\"\" Builds an n-gram model from the given data\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        vocabulary: a list of all the tokens in the data\n",
    "        data_file_path: the data_file from which we record probabilities for our model\n",
    "        add_one: True/False (use add_one smoothing or not)\n",
    "    Returns:\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file_path,  encoding=\"utf-8\")\n",
    "    # build the n-gram model\n",
    "    model = build_ngram_model(current_data, n)\n",
    "    if add_one:\n",
    "        # add one smoothing\n",
    "        model = add_one_smoothing(model, vocabulary)\n",
    "    # calculate the probabilities\n",
    "    model = calculate_probabilities(model)\n",
    "    return model"
   ],
   "execution_count": 9,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# call the function for the first data file\n",
    "lm_model_True = lm(2, vocabulary, path + \"/\" + data_files[0], True)\n",
    "\n",
    "lm_model_False = lm(2, vocabulary, path + \"/\" + data_files[0], False)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:15.497170Z",
     "end_time": "2023-04-28T21:28:16.978500Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7M8TchtI22I3"
   },
   "source": [
    "**Part 3**\n",
    "\n",
    "Write a function *eval* that returns the perplexity of a model (dictionary) running over a given data file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "from math import log2\n",
    "\n",
    "def calculate_perplexity(current_data, n, model):\n",
    "    \"\"\" Calculates the perplexity of a model running over a given data file\n",
    "    Args:\n",
    "        current_data: a data frame representing the data\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {tuple of n-1 tokens: {n_th_token: probability}}\n",
    "    Returns:\n",
    "        perplexity: the perplexity of the model\n",
    "    \"\"\"\n",
    "    log_prob_sum = 0\n",
    "    n_gram_count = 0\n",
    "\n",
    "    # iterate over all the tweets in the data\n",
    "    for tweet in current_data[\"tweet_text\"]:\n",
    "        # add start and end tokens\n",
    "        tweet = \"<start> \" + tweet + \" <end>\"\n",
    "        # convert the tweet to a list of tokens\n",
    "        tweet = tweet_to_token_tuples(tweet)\n",
    "\n",
    "        # iterate over all the n-grams in the tweet\n",
    "        for i in range(len(tweet) - n + 1):\n",
    "            n_gram = tweet[i:i+n]\n",
    "            n_minus_1_gram = n_gram[:-1]\n",
    "            n_th_token = n_gram[-1]\n",
    "\n",
    "            # if the n-gram is in the model, add its log probability to the sum\n",
    "            if n_minus_1_gram in model and n_th_token in model[n_minus_1_gram]:\n",
    "                log_prob_sum += -log2(model[n_minus_1_gram][n_th_token])\n",
    "                n_gram_count += 1\n",
    "            # <notInModel> is for add_one smoothing case\n",
    "            elif n_minus_1_gram in model and \"<notInModel>\" in model[n_minus_1_gram]:\n",
    "                # get the meta_data of the n-1 gram\n",
    "                meta_data = model[n_minus_1_gram][\"meta_data\"]\n",
    "                total_count = meta_data[\"total_count\"]\n",
    "                notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "                # calculate the probability of <notInModel>\n",
    "                prob_notInModel = model[n_minus_1_gram][\"<notInModel>\"]\n",
    "                # calculate the probability of single unseen token\n",
    "                prob = prob_notInModel / notInModel_count\n",
    "                # calculate the log probability of the n-gram and add it to the sum\n",
    "                log_prob_sum += -log2(prob)\n",
    "                n_gram_count += 1\n",
    "\n",
    "    # return infinite perplexity if no n-grams found\n",
    "    if n_gram_count == 0:\n",
    "        return float('inf')\n",
    "    # calculate the entropy and the perplexity\n",
    "    entropy = log_prob_sum / n_gram_count\n",
    "    perplexity = 2 ** entropy\n",
    "    return perplexity\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:16.981589Z",
     "end_time": "2023-04-28T21:28:16.994490Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "F0kkMn328-lJ",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:16.996001Z",
     "end_time": "2023-04-28T21:28:17.030507Z"
    }
   },
   "source": [
    "def eval(n, model, data_file):\n",
    "    \"\"\" Evaluates the perplexity of a model\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        model: a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "        data_file: the data file path for which we want to calculate the perplexity\n",
    "    Returns:\n",
    "        the perplexity of the model\n",
    "    \"\"\"\n",
    "    # read the data file\n",
    "    current_data = pd.read_csv(data_file, encoding=\"utf-8\")\n",
    "    # calculate the perplexity\n",
    "    perplexity = calculate_perplexity(current_data, n, model)\n",
    "\n",
    "    return perplexity"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity lm_model_True:  20.89777888632233\n",
      "perplexity lm_model_False:  17.724467632988453\n"
     ]
    }
   ],
   "source": [
    "# call the function\n",
    "perplexity = eval(2, lm_model_True, path + \"/\" + data_files[0])\n",
    "print(\"perplexity lm_model_True: \", perplexity)\n",
    "perplexity = eval(2, lm_model_False, path + \"/\" + data_files[0])\n",
    "print(\"perplexity lm_model_False: \", perplexity)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:17.012334Z",
     "end_time": "2023-04-28T21:28:18.375502Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "enGmtLE3921p"
   },
   "source": [
    "**Part 4**\n",
    "\n",
    "Write a function *match* that creates a model for every relevant language, using a specific value of *n* and *add_one*. Then, calculate the perplexity of all possible pairs (e.g., en model applied on the data files en ,es, fr, in, it, nl, pt, tl; es model applied on the data files en, es...). This function should return a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages. Then, the values are the relevant perplexity values.\n",
    "\n",
    "Save the dataframe to a CSV with the name format: {student_id_1}\\_...\\_{student_id_n}\\_part4.csv"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "caAxLE9s_fvn",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:18.381204Z",
     "end_time": "2023-04-28T21:28:18.405273Z"
    }
   },
   "source": [
    "def match(n, add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Then, calculate the perplexity of all possible pairs.\n",
    "    Args:\n",
    "        n: the n in n-gram\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        df: a pandas DataFrame with columns [en ,es, fr, in, it, nl, pt, tl] and every row should be labeled with one of the languages.\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # create a dataframe\n",
    "    df = pd.DataFrame(columns=data_files, index=data_files)\n",
    "\n",
    "    # create models for every language\n",
    "    models = compute_data_files_models(data_files, n, vocabulary, path, add_one)\n",
    "\n",
    "    # calculate the perplexity of all possible pairs\n",
    "    for lang1 in data_files: # will be the model\n",
    "        # define the model\n",
    "        current_model = models[lang1]\n",
    "        for lang2 in data_files: # will be the data file\n",
    "            # define the data file\n",
    "            current_data_file = path + \"/\" + lang2\n",
    "            # evaluate the model\n",
    "            perplexity = eval(n, current_model, current_data_file)\n",
    "            # save the perplexity to the dataframe\n",
    "            df[lang1][lang2] = perplexity\n",
    "    return df, models # return the dataframe and the models, so that we can use them later\n",
    "\n",
    "def compute_data_files_models(data_files, n, vocabulary, path , add_one):\n",
    "    \"\"\" Creates a model for every relevant language, using a specific value of n and add_one.\n",
    "    Args:\n",
    "        data_files: the data files to create models for\n",
    "        n: the n in n-gram\n",
    "        vocabulary: the vocabulary\n",
    "        path: the path to the data files\n",
    "        add_one: whether to use add one smoothing or not\n",
    "    Returns:\n",
    "        models: a dictionary of the models, so that we can use them later,\n",
    "                i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    models = {}\n",
    "    for data_file in data_files:\n",
    "        models[data_file] = lm(n, vocabulary, path + \"/\" + data_file, add_one)\n",
    "    return models\n"
   ],
   "execution_count": 14,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# call the function\n",
    "df_part4, models_part4 = match(2, True)\n",
    "\n",
    "# save the dataframe to a CSV of format {student_id_1}\\_...\\_{student_id_n}\\_part4.csv\n",
    "df_part4.to_csv(student_id_1 + \"_\" + student_id_2 + \"_part4.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:28:18.390187Z",
     "end_time": "2023-04-28T21:29:04.341025Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [],
   "source": [
    "# print(\"dataframe: \")\n",
    "# print(df_part4)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:29:04.335766Z",
     "end_time": "2023-04-28T21:29:04.355815Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "waGMwA8H_n17"
   },
   "source": [
    "**Part 5**\n",
    "\n",
    "Run match with *n* values 1-4, once with add_one and once without, and print the 8 tables to this notebook, one after another.\n",
    "\n",
    "Load each result to a dataframe and save to a CSV with the name format: \n",
    "\n",
    "for cases with add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_part5.csv\n",
    "\n",
    "For cases without add_one:\n",
    "{student_id_1}\\_...\\_{student_id_n}\\_n1\\_wo\\_addone\\_part5.csv\n",
    "\n",
    "Follow the same format for n2,n3, and n4\n"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "nk32naXyAMdl",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:29:04.352818Z",
     "end_time": "2023-04-28T21:29:04.369098Z"
    }
   },
   "source": [
    "def run_match(n_values = [1, 2, 3, 4],add_one_values = [True, False] ):\n",
    "    \"\"\" Runs match with n values 1-4, once with add_one and once without, and print the 8 tables to this notebook.\n",
    "    Args:\n",
    "        n_values: the n values to run match with\n",
    "        add_one_values: the add_one values to run match with\n",
    "    Returns:\n",
    "        dataframes: a dictionary of the dataframes, so that we can use them later,\n",
    "                i.e {n, add_one: dataframe}\n",
    "        language_models_dict: a dictionary of the language models, so that we can use them later,\n",
    "                i.e {n, add_one: {language: model}}\n",
    "    \"\"\"\n",
    "    # create dictionaries for the dataframes, key = (n, add_one), value = dataframe\n",
    "    dataframes = {}\n",
    "    # create a dictionary for the language models, key = (n, add_one), value = language models = {language: model}\n",
    "    language_models_dict = {}\n",
    "    # iterate over all the n values\n",
    "    for n in n_values:\n",
    "        # iterate over all the add_one values\n",
    "        for add_one in add_one_values:\n",
    "            # create the dataframe and the language models, using the match function\n",
    "            current_df, current_language_models = match(n, add_one)\n",
    "            print(\"completed n = \" + str(n) + \", add_one = \" + str(add_one) + \"!\")\n",
    "            # add the dataframe to the dataframes dictionary\n",
    "            dataframes[(n, add_one)] = current_df\n",
    "            # add the language models to the language_models_dict dictionary\n",
    "            language_models_dict[(n, add_one)] = current_language_models\n",
    "\n",
    "    return dataframes, language_models_dict\n"
   ],
   "execution_count": 17,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed n = 1, add_one = True!\n",
      "completed n = 1, add_one = False!\n",
      "completed n = 2, add_one = True!\n",
      "completed n = 2, add_one = False!\n",
      "completed n = 3, add_one = True!\n",
      "completed n = 3, add_one = False!\n",
      "completed n = 4, add_one = True!\n",
      "completed n = 4, add_one = False!\n"
     ]
    }
   ],
   "source": [
    "# call the function and save the results\n",
    "run_match_dataframes, run_match_language_models = run_match()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:29:04.366040Z",
     "end_time": "2023-04-28T21:37:59.526836Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "# print(\"dataframes: \")\n",
    "# print(run_match_dataframes)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:37:59.529371Z",
     "end_time": "2023-04-28T21:37:59.558654Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved 311451660_327156998_n1_part5.csv\n",
      "saved 311451660_327156998_n1_wo_addone_part5.csv\n",
      "saved 311451660_327156998_n2_part5.csv\n",
      "saved 311451660_327156998_n2_wo_addone_part5.csv\n",
      "saved 311451660_327156998_n3_part5.csv\n",
      "saved 311451660_327156998_n3_wo_addone_part5.csv\n",
      "saved 311451660_327156998_n4_part5.csv\n",
      "saved 311451660_327156998_n4_wo_addone_part5.csv\n"
     ]
    }
   ],
   "source": [
    "# save the dataframes to CSVs, format:\n",
    "# with add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_part5.csv\n",
    "# without add_one: {student_id_1}\\_...\\_{student_id_n}\\_n1\\_wo\\_addone\\_part5.csv\n",
    "\n",
    "n_values = [1, 2, 3, 4]\n",
    "add_one_values = [True, False]\n",
    "# iterate over all the n values and add_one values and save the dataframes to CSVs\n",
    "for n in n_values:\n",
    "    for add_one in add_one_values:\n",
    "        if add_one:\n",
    "            run_match_dataframes[(n, add_one)].to_csv(student_id_1 + \"_\" + student_id_2 + \"_n\" + str(n) + \"_part5.csv\", index=False)\n",
    "            print(\"saved \" + student_id_1 + \"_\" + student_id_2 + \"_n\" + str(n) + \"_part5.csv\")\n",
    "        else:\n",
    "            run_match_dataframes[(n, add_one)].to_csv(student_id_1 + \"_\" + student_id_2 + \"_n\" + str(n) + \"_wo_addone_part5.csv\", index=False)\n",
    "            print(\"saved \" + student_id_1 + \"_\" + student_id_2 + \"_n\" + str(n) + \"_wo_addone_part5.csv\")\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:37:59.547132Z",
     "end_time": "2023-04-28T21:37:59.558654Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def save_models_to_pickle(language_models_dict=run_match_language_models, filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Saves the language models to a pickle file\n",
    "    Args:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "        filename: the name of the pickle file\n",
    "    \"\"\"\n",
    "    # save run_match_language_models to a pickle file, so that we can use it later\n",
    "    with open(filename, 'wb') as handle:\n",
    "        pickle.dump(language_models_dict, handle, protocol=pickle.HIGHEST_PROTOCOL)\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:37:59.559652Z",
     "end_time": "2023-04-28T21:37:59.575178Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "def load_models_from_pickle(filename=\"run_match_language_models.pickle\"):\n",
    "    \"\"\" Loads the language models from a pickle file\n",
    "    Args:\n",
    "        filename: the name of the pickle file\n",
    "    Returns:\n",
    "        language_models_dict: a dictionary of the models, i.e {n, add_one: language_models} where language_models is a dictionary of the models, i.e {language: model} where model is a dictionary representing the n-gram model, i.e {n-1_gram: {n_th_token: probability}}\n",
    "    \"\"\"\n",
    "    # load run_match_language_models from a pickle file\n",
    "    with open(filename, 'rb') as handle:\n",
    "        language_models_dict = pickle.load(handle)\n",
    "\n",
    "    return language_models_dict\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:37:59.577176Z",
     "end_time": "2023-04-28T21:37:59.617789Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "saved run_match_language_models.pickle\n"
     ]
    }
   ],
   "source": [
    "# save the language models to a pickle file\n",
    "# (remove/put # in front of the next lines to save the language models)\n",
    "\n",
    "save_models_to_pickle(run_match_language_models, \"run_match_language_models.pickle\")\n",
    "print(\"saved run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:37:59.590314Z",
     "end_time": "2023-04-28T21:38:01.187187Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded run_match_language_models.pickle\n"
     ]
    }
   ],
   "source": [
    "# load the language models from a pickle file\n",
    "# (remove/put # in front of the next lines to load the language models)\n",
    "\n",
    "run_match_language_models = load_models_from_pickle(\"run_match_language_models.pickle\")\n",
    "print(\"loaded run_match_language_models.pickle\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:38:01.189300Z",
     "end_time": "2023-04-28T21:38:03.685545Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cg4h5Cl0q2nR"
   },
   "source": [
    "**Part 6**\n",
    "\n",
    "Each line in the file test.csv contains a sentence and the language it belongs to. Write a function that uses your language models to classify the correct language of each sentence.\n",
    "\n",
    "Important note regarding the grading of this section: this is an open question, where a different solution will yield different accuracy scores. any solution that is not trivial (e.g. returning 'en' in all cases) will be accepted. We do reserve the right to give bonus points to exceptionally good/creative solutions."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qD6IRIQLrlZF",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:38:03.689914Z",
     "end_time": "2023-04-28T21:38:03.728723Z"
    }
   },
   "source": [
    "def classify(n=3, add_one=False):\n",
    "    \"\"\" Classifies the sentences in the test file, using the language models\n",
    "    Args:\n",
    "        n: the n-gram model to use\n",
    "        add_one: whether to use add_one smoothing or not\n",
    "    Returns:\n",
    "        classification_result: a list of tuples of the form (tweet_id, sentence, true_language, predicted_language)\n",
    "    \"\"\"\n",
    "    # we will use the language models from part 5, with n = 3 and add_one = True\n",
    "    language_models = run_match_language_models[(n, add_one)]\n",
    "\n",
    "    # read the test file, tweet_id, tweet_text, label\n",
    "    test_data = pd.read_csv(path + \"/test.csv\",  encoding=\"utf-8\")\n",
    "\n",
    "    # classify the sentences\n",
    "    classification_result = []\n",
    "\n",
    "    # iterate over the rows in the test data\n",
    "    for index, row in test_data.iterrows():\n",
    "        tweet_id = row['tweet_id']\n",
    "        sentence = row['tweet_text']\n",
    "        true_language = row['label']\n",
    "        # classify the sentence\n",
    "        predicted_language = single_classification(sentence, language_models, n)\n",
    "\n",
    "        # add the result to the classification_result list\n",
    "        classification_result.append((tweet_id, sentence, true_language, predicted_language))\n",
    "\n",
    "    return classification_result\n",
    "\n",
    "\n",
    "def single_classification(sentence, language_models = run_match_language_models[(3, True)], n=3):\n",
    "    \"\"\" Classifies a single sentence, using the language models\n",
    "    Args:\n",
    "        sentence: the sentence to classify\n",
    "        language_models: the language models to use\n",
    "        n: the n-gram model to use\n",
    "    Returns:\n",
    "        predicted_language: the predicted language of the sentence\n",
    "    \"\"\"\n",
    "\n",
    "    predicted_language = ''\n",
    "    min_perplexity = float('inf')\n",
    "    # iterate over the language models\n",
    "    for data_file in data_files:\n",
    "        current_model = language_models[data_file]\n",
    "        # create a temporary DataFrame, with the sentence as the only row\n",
    "        temp_df = pd.DataFrame([sentence], columns=['tweet_text'])\n",
    "        # calculate the perplexity using the temporary DataFrame\n",
    "        current_perplexity = calculate_perplexity(temp_df, n, current_model)\n",
    "\n",
    "        if current_perplexity < min_perplexity:\n",
    "            min_perplexity = current_perplexity\n",
    "            predicted_language = data_file[:-4] # remove the .csv from the end of the file name\n",
    "\n",
    "    return predicted_language\n",
    "\n"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed classification for n = 1, add_one = False\n",
      "completed classification for n = 1, add_one = True\n",
      "completed classification for n = 2, add_one = False\n",
      "completed classification for n = 2, add_one = True\n",
      "completed classification for n = 3, add_one = False\n",
      "completed classification for n = 3, add_one = True\n",
      "completed classification for n = 4, add_one = False\n",
      "completed classification for n = 4, add_one = True\n"
     ]
    }
   ],
   "source": [
    "# classify the test sentences\n",
    "classification_result_n1 = classify(n=1, add_one=False)\n",
    "print(\"completed classification for n = 1, add_one = False\")\n",
    "\n",
    "classification_result_n1_add_one = classify(n=1, add_one=True)\n",
    "print(\"completed classification for n = 1, add_one = True\")\n",
    "\n",
    "classification_result_n2 = classify(n=2, add_one=False)\n",
    "print(\"completed classification for n = 2, add_one = False\")\n",
    "\n",
    "classification_result_n2_add_one = classify(n=2, add_one=True)\n",
    "print(\"completed classification for n = 2, add_one = True\")\n",
    "\n",
    "classification_result_n3 = classify(n=3, add_one=False)\n",
    "print(\"completed classification for n = 3, add_one = False\")\n",
    "\n",
    "classification_result_n3_add_one = classify(n=3, add_one=True)\n",
    "print(\"completed classification for n = 3, add_one = True\")\n",
    "\n",
    "classification_result_n4 = classify(n=4, add_one=False)\n",
    "print(\"completed classification for n = 4, add_one = False\")\n",
    "\n",
    "classification_result_n4_add_one = classify(n=4, add_one=True)\n",
    "print(\"completed classification for n = 4, add_one = True\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:38:03.703896Z",
     "end_time": "2023-04-28T21:39:56.473221Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "def get_accuracy(classification_result):\n",
    "    count_correct = 0\n",
    "    for result in classification_result:\n",
    "        predicted_language = result[3]\n",
    "        true_language = result[2]\n",
    "        if predicted_language == true_language:\n",
    "            count_correct += 1\n",
    "    return count_correct / len(classification_result)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:39:56.474926Z",
     "end_time": "2023-04-28T21:39:56.505421Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "accuracy for n = 1, add_one = False: 0.5639454931866483\n",
      "accuracy for n = 2, add_one = False: 0.8538567320915115\n",
      "accuracy for n = 3, add_one = False: 0.8821102637829729\n",
      "accuracy for n = 4, add_one = False: 0.794474309288661\n",
      "accuracy for n = 1, add_one = True: 0.6663332916614577\n",
      "accuracy for n = 2, add_one = True: 0.8744843105388174\n",
      "accuracy for n = 3, add_one = True: 0.9236154519314914\n",
      "accuracy for n = 4, add_one = True: 0.9179897487185898\n"
     ]
    }
   ],
   "source": [
    "# print the accuracy of the classification\n",
    "\n",
    "print(\"accuracy for n = 1, add_one = False: \" + str(get_accuracy(classification_result_n1)))\n",
    "print(\"accuracy for n = 2, add_one = False: \" + str(get_accuracy(classification_result_n2)))\n",
    "print(\"accuracy for n = 3, add_one = False: \" + str(get_accuracy(classification_result_n3)))\n",
    "print(\"accuracy for n = 4, add_one = False: \" + str(get_accuracy(classification_result_n4)))\n",
    "\n",
    "print(\"accuracy for n = 1, add_one = True: \" + str(get_accuracy(classification_result_n1_add_one)))\n",
    "print(\"accuracy for n = 2, add_one = True: \" + str(get_accuracy(classification_result_n2_add_one)))\n",
    "print(\"accuracy for n = 3, add_one = True: \" + str(get_accuracy(classification_result_n3_add_one)))\n",
    "print(\"accuracy for n = 4, add_one = True: \" + str(get_accuracy(classification_result_n4_add_one)))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:39:56.492116Z",
     "end_time": "2023-04-28T21:39:56.506426Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q5ECmLd3rktZ"
   },
   "source": [
    "**Part 7**\n",
    "\n",
    "Calculate the F1 score of your output from part 6. (hint: you can use https://scikit-learn.org/stable/modules/generated/sklearn.metrics.f1_score.html). \n",
    "\n",
    "Load the results to a CSV (using a DataFrame), with a model_name and f1_score Name it {student_id_1}\\_...\\_{student_id_n}\\_part7.csv\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "```\n",
    "  model_name  f1_score\n",
    "0    Model A      0.85\n",
    "1    Model B      0.92\n",
    "2    Model C      0.87\n",
    "3    Model D      0.90\n",
    "```\n",
    "\n"
   ],
   "metadata": {
    "id": "KF3ImVdPgAGC"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "def map_language_number(classification_result):\n",
    "    #we will use the following dictionary to convert the strings to numbers\n",
    "    language_to_number = {}\n",
    "    # we will use the following dictionary to convert the numbers back to strings\n",
    "    number_to_language = {}\n",
    "\n",
    "    number = 0\n",
    "    # iterate over the classification results\n",
    "    for result in classification_result:\n",
    "        if result[2] not in language_to_number:\n",
    "            language_to_number[result[2]] = number\n",
    "            number_to_language[number] = result[2]\n",
    "            number += 1\n",
    "        if result[3] not in language_to_number:\n",
    "            language_to_number[result[3]] = number\n",
    "            number_to_language[number] = result[3]\n",
    "            number += 1\n",
    "    return language_to_number, number_to_language"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:39:56.503421Z",
     "end_time": "2023-04-28T21:39:56.519419Z"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "VOBO3YQls66r",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:39:56.523421Z",
     "end_time": "2023-04-28T21:39:57.487284Z"
    }
   },
   "source": [
    "import sklearn.metrics as metrics\n",
    "\n",
    "def calc_f1(result):\n",
    "    \"\"\" Calculates the f1 score of the classification result\n",
    "    Args:\n",
    "        result: a list of tuples, where each tuple contains the tweet_id, the sentence, the true language, and the predicted language\n",
    "    Returns:\n",
    "        f1_score: the f1 score of the classification result\n",
    "    \"\"\"\n",
    "    # create mappings from language to number and number to language\n",
    "    language_to_number, number_to_language = map_language_number(result)\n",
    "\n",
    "    # create a DataFrame with the results\n",
    "    df = pd.DataFrame(result, columns=['tweet_id', 'tweet_text', 'true_language', 'predicted_language'])\n",
    "    # drop the tweet_id and tweet_text columns\n",
    "    df = df.drop(columns=['tweet_id', 'tweet_text'])\n",
    "    # convert the true_language and predicted_language columns to numbers\n",
    "    df['true_language'] = df['true_language'].apply(lambda x: language_to_number[x])\n",
    "    df['predicted_language'] = df['predicted_language'].apply(lambda x: language_to_number[x])\n",
    "    # calculate the f1 score\n",
    "    f1_score = metrics.f1_score(df['true_language'], df['predicted_language'], average='weighted')\n",
    "\n",
    "    return f1_score"
   ],
   "execution_count": 30,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "completed f1 score for n = 1, add_one = False\n",
      "completed f1 score for n = 2, add_one = False\n",
      "completed f1 score for n = 3, add_one = False\n",
      "completed f1 score for n = 4, add_one = False\n",
      "completed f1 score for n = 1, add_one = True\n",
      "completed f1 score for n = 2, add_one = True\n",
      "completed f1 score for n = 3, add_one = True\n",
      "completed f1 score for n = 4, add_one = True\n"
     ]
    }
   ],
   "source": [
    "# calculate the f1 score for each n\n",
    "f1_score_n1 = calc_f1(classification_result_n1)\n",
    "print(\"completed f1 score for n = 1, add_one = False\")\n",
    "\n",
    "f1_score_n2 = calc_f1(classification_result_n2)\n",
    "print(\"completed f1 score for n = 2, add_one = False\")\n",
    "\n",
    "f1_score_n3 = calc_f1(classification_result_n3)\n",
    "print(\"completed f1 score for n = 3, add_one = False\")\n",
    "\n",
    "f1_score_n4 = calc_f1(classification_result_n4)\n",
    "print(\"completed f1 score for n = 4, add_one = False\")\n",
    "\n",
    "f1_score_n1_add_one = calc_f1(classification_result_n1_add_one)\n",
    "print(\"completed f1 score for n = 1, add_one = True\")\n",
    "\n",
    "f1_score_n2_add_one = calc_f1(classification_result_n2_add_one)\n",
    "print(\"completed f1 score for n = 2, add_one = True\")\n",
    "\n",
    "f1_score_n3_add_one = calc_f1(classification_result_n3_add_one)\n",
    "print(\"completed f1 score for n = 3, add_one = True\")\n",
    "\n",
    "f1_score_n4_add_one = calc_f1(classification_result_n4_add_one)\n",
    "print(\"completed f1 score for n = 4, add_one = True\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:39:57.489323Z",
     "end_time": "2023-04-28T21:39:57.596946Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               model_name  f1_score\n",
      "0  n = 1, add_one = False  0.559660\n",
      "1  n = 2, add_one = False  0.854003\n",
      "2  n = 3, add_one = False  0.882541\n",
      "3  n = 4, add_one = False  0.794920\n",
      "4   n = 1, add_one = True  0.667513\n",
      "5   n = 2, add_one = True  0.875084\n",
      "6   n = 3, add_one = True  0.923877\n",
      "7   n = 4, add_one = True  0.918313\n"
     ]
    }
   ],
   "source": [
    "# Load the results to a CSV (using a DataFrame), with a model_name and f1_score Name it {student_id_1}\\_...\\_{student_id_n}\\_part7.csv\n",
    "\n",
    "# create a DataFrame with the results\n",
    "df_part7 = pd.DataFrame(columns=['model_name', 'f1_score'])\n",
    "\n",
    "# add the results to the pd.DataFrame\n",
    "df_part7.loc[0] = ['n = 1, add_one = False', f1_score_n1]\n",
    "df_part7.loc[1] = ['n = 2, add_one = False', f1_score_n2]\n",
    "df_part7.loc[2] = ['n = 3, add_one = False', f1_score_n3]\n",
    "df_part7.loc[3] = ['n = 4, add_one = False', f1_score_n4]\n",
    "df_part7.loc[4] = ['n = 1, add_one = True', f1_score_n1_add_one]\n",
    "df_part7.loc[5] = ['n = 2, add_one = True', f1_score_n2_add_one]\n",
    "df_part7.loc[6] = ['n = 3, add_one = True', f1_score_n3_add_one]\n",
    "df_part7.loc[7] = ['n = 4, add_one = True', f1_score_n4_add_one]\n",
    "\n",
    "# save the DataFrame to a CSV file\n",
    "df_part7.to_csv(student_id_1 + \"_\" + student_id_2 + \"_part7.csv\", index=False)\n",
    "# print the DataFrame\n",
    "print(df_part7)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:39:57.600162Z",
     "end_time": "2023-04-28T21:39:57.647137Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "<br><br><br><br>\n",
    "**Part 8**  \n",
    "Let's use your Language model (dictionary) for generation (NLG).\n",
    "\n",
    "When it comes to sampling from a language model decoder during text generation, there are several different methods that can be used to control the randomness and diversity of the generated text. \n",
    "\n",
    "Some of the most commonly used methods include:\n",
    "\n",
    "> `Greedy sampling`\n",
    "In this method, the model simply selects the word with the highest probability as the next word at each time step. This method can produce fluent text, but it can also lead to repetitive or predictable output.\n",
    "\n",
    "> `Temperature scaling`  \n",
    "Temperature scaling involves scaling the logits output of the language model by a temperature parameter before softmax normalization. This has the effect of smoothing the distribution of probabilities and increasing the probability of lower-probability words, which can lead to more diverse and creative output.\n",
    "\n",
    "> `Top-K sampling`  \n",
    "In this method, the model restricts the sampling to the top-K most likely words at each time step, where K is a predefined hyperparameter. This can generate more diverse output than greedy sampling, while limiting the number of low-probability words that are sampled.\n",
    "\n",
    "> `Nucleus sampling` (also known as top-p sampling)  \n",
    "This method restricts the sampling to the smallest possible set of words whose cumulative probability exceeds a certain threshold, defined by a hyperparameter p. Like top-K sampling, this can generate more diverse output than greedy sampling, while avoiding sampling extremely low probability words.\n",
    "\n",
    "> `Beam search`  \n",
    "Beam search involves maintaining a fixed number k of candidate output sequences at each time step, and then selecting the k most likely sequences based on their probabilities. This can improve the fluency and coherence of the output, but may not produce as much diversity as sampling methods.\n",
    "\n",
    "The choice of sampling method depends on the specific application and desired balance between fluency, diversity, and randomness. Hyperparameters such as temperature, K, p, and beam size can also be tuned to adjust the behavior of the language model during sampling.\n",
    "\n",
    "\n",
    "You may read more about this concept in <a href='https://huggingface.co/blog/how-to-generate#:~:text=pad_token_id%3Dtokenizer.eos_token_id)-,Greedy%20Search,-Greedy%20search%20simply'>this</a> blog post.\n"
   ],
   "metadata": {
    "id": "NfBYgfjADNPL"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Please added the needed code for each sampeling method:**"
   ],
   "metadata": {
    "id": "GbReeHtwNWKS"
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def softmax(probabilities):\n",
    "    \"\"\" Applies the softmax function to the probabilities\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities (not yet probabilities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a dictionary of probabilities (normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    # convert the dictionary to a numpy array\n",
    "    np_probabilities = np.array(list(probabilities.values()))\n",
    "    # apply the softmax function\n",
    "    np_probabilities = np.exp(np_probabilities)\n",
    "    np_probabilities = np_probabilities / np.sum(np_probabilities)\n",
    "    # convert the numpy array back to a dictionary\n",
    "    probabilities = {key: value for key, value in zip(probabilities.keys(), np_probabilities)}\n",
    "    return probabilities\n",
    "\n",
    "def make_prob_1(probabilities):\n",
    "    \"\"\" Makes the sum of the probabilities equal to 1\n",
    "    Args:\n",
    "        probabilities: a list of probabilities (not yet probabilities, just numbers)\n",
    "    Returns:\n",
    "        probabilities: a list of probabilities (normalized)\n",
    "    \"\"\"\n",
    "\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    # by dividing each probability by the sum of all probabilities\n",
    "    sum_prob = sum(probabilities)\n",
    "    probabilities = [prob / sum_prob for prob in probabilities]\n",
    "    return probabilities\n",
    "\n",
    "def get_correct_model(all_models, prefix=\"<start>\", language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Gets the correct model for the prefix and language\n",
    "    Args:\n",
    "        all_models: a dictionary of all the language models\n",
    "        prefix: the prefix of the tweet\n",
    "        language: the language of the tweet\n",
    "        add_one: whether to use add_one smoothing or not\n",
    "        max_n: the maximum n-gram to use\n",
    "    Returns:\n",
    "        correct_model: the correct language model\n",
    "        tuple_key_prefix: the tuple key of the prefix\n",
    "        next_token_probabilities: the probabilities of the next token\n",
    "    \"\"\"\n",
    "    prefix_tokens = tweet_to_token_tuples(prefix)\n",
    "\n",
    "    # we want to use maximum n-gram we can, but not more than max_n\n",
    "    n = min(max_n, len(prefix_tokens) + 1)\n",
    "\n",
    "    # get the n-gram model\n",
    "    correct_model = all_models[(n, add_one)][language]\n",
    "\n",
    "    # get the n-1 tokens of the prefix, i.e. the key for the language model\n",
    "    tuple_key_prefix = tuple(prefix_tokens[-(n - 1):])\n",
    "\n",
    "    # if the prefix is not in the language model, sample a random key from the language model\n",
    "    if tuple_key_prefix not in correct_model:\n",
    "        tuple_key_prefix = random.choice(list(correct_model.keys()))\n",
    "\n",
    "\n",
    "    # get the probabilities of the next token\n",
    "    next_token_probabilities = correct_model[tuple_key_prefix]\n",
    "    return correct_model, tuple_key_prefix, next_token_probabilities\n",
    "\n",
    "def get_notInModel_probabilities(probabilities, notInModel_count=1, vocabulary=vocabulary):\n",
    "    \"\"\" Removes <notInModel> from the probabilities dictionary and adds the probabilities of the tokens not in the model\n",
    "    Args:\n",
    "        vocabulary: the vocabulary of the language model\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        notInModel_count: the number of tokens not in the model\n",
    "    Returns:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "    \"\"\"\n",
    "    # add all tokens not in the model to the probabilities dictionary\n",
    "    if '<notInModel>' in probabilities:\n",
    "        single_notInModel_prob = probabilities['<notInModel>'] / notInModel_count\n",
    "        tokens_not_in_model_vocab = set(vocabulary) - set(probabilities.keys())\n",
    "        for token in tokens_not_in_model_vocab:\n",
    "            probabilities[token] = single_notInModel_prob\n",
    "        del probabilities['<notInModel>']\n",
    "\n",
    "    return probabilities\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:41:14.778517Z",
     "end_time": "2023-04-28T21:41:14.814881Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def sample_greedy(probabilities, k=1):\n",
    "    \"\"\" Samples the next token greedily, i.e. the token with the highest probability\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        k: the number of tokens to sample\n",
    "    Returns:\n",
    "        max_token: the token with the highest probability\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # reduce the probability of the token <notInModel> by the number of times it was sampled\n",
    "    if '<notInModel>' in probabilities_copy:\n",
    "        probabilities_copy['<notInModel>'] /= notInModel_count\n",
    "\n",
    "    # sort the probabilities by value\n",
    "    sorted_probabilities = sorted(probabilities_copy.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # if k is larger than the number of probabilities, set k to the number of probabilities\n",
    "    k = k if len(sorted_probabilities) >= k else len(sorted_probabilities)\n",
    "\n",
    "    # sample the token with the k highest probability\n",
    "    next_token = sorted_probabilities[k - 1][0]\n",
    "\n",
    "    # if the token is <notInModel>, sample a random token from the tokens not in the model\n",
    "    if next_token == '<notInModel>':\n",
    "        # get tokens not in the model\n",
    "        tokens_not_in_model_vocab = set(vocabulary) - set(probabilities_copy.keys())\n",
    "        # sample a random token from the tokens not in the model\n",
    "        next_token = random.choice(list(tokens_not_in_model_vocab))\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def sample_temperature(probabilities, temperature=1.0, k=1):\n",
    "    \"\"\" Samples the next token using temperature sampling\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        temperature: the temperature\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # remove <notInModel> from the probabilities dictionary and add the probabilities of the tokens not in the model\n",
    "    probabilities_copy = get_notInModel_probabilities(probabilities=probabilities_copy, notInModel_count=notInModel_count)\n",
    "\n",
    "    # scale the probabilities by the temperature\n",
    "    probabilities_copy = {key: value ** (1 / temperature) for key, value in probabilities_copy.items()}\n",
    "\n",
    "    # softmax the probabilities\n",
    "    probabilities_copy = softmax(probabilities_copy)\n",
    "\n",
    "    # sample from the probabilities dictionary, use the np.random.choice function\n",
    "    np_probabilities = np.array(list(probabilities_copy.values()))\n",
    "    np_tokens = np.array(list(probabilities_copy.keys()))\n",
    "\n",
    "    # convert the tokens (tuple) to a list of strings\n",
    "    np_tokens = [\"\".join(token) for token in np_tokens]\n",
    "\n",
    "    # sample the next token\n",
    "    next_token = np.random.choice(np_tokens, p=np_probabilities)\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def sample_topK(probabilities, k=1):\n",
    "    \"\"\" Samples the next token using top-k sampling, i.e. only the top k tokens are considered\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        k: the number of tokens to consider\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # add all tokens not in the model to the probabilities dictionary\n",
    "    probabilities_copy = get_notInModel_probabilities(probabilities=probabilities_copy, notInModel_count=notInModel_count)\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities_copy.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "    # take the top k\n",
    "    top_k = sorted_probabilities[:k]\n",
    "\n",
    "    # split the top k into tokens and probabilities\n",
    "    top_k_probs = [prob for (token, prob) in top_k]\n",
    "    top_k_tokens = [token for (token, prob) in top_k]\n",
    "\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_k_probs = make_prob_1(top_k_probs)\n",
    "\n",
    "    # sample from the top k tokens\n",
    "    next_token = np.random.choice(top_k_tokens, p=top_k_probs)\n",
    "\n",
    "    return next_token\n",
    "\n",
    "# probabilities = {key = next_token, value = probability}\n",
    "def sample_topP(probabilities, p=0.9):\n",
    "    \"\"\" Samples the next token using top-p sampling,\n",
    "    i.e. only the tokens with the highest probabilities are considered, until the sum of the probabilities is greater than p\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        p: the threshold\n",
    "    Returns:\n",
    "        next_token: the sampled token\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "     # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # remove <notInModel> from the probabilities dictionary and add the probabilities of the tokens not in the model\n",
    "    probabilities_copy = get_notInModel_probabilities(probabilities=probabilities_copy, notInModel_count=notInModel_count)\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities_copy.items(), key=lambda x: x[1], reverse=True)\n",
    "    current_sum = 0\n",
    "    top_p_tokens = []\n",
    "    top_p_probs = []\n",
    "    current_index = 0\n",
    "    # add the top tokens until the sum of the probabilities is greater than p\n",
    "    while current_sum < p:\n",
    "        top_p_tokens.append(sorted_probabilities[current_index][0])\n",
    "        top_p_probs.append(sorted_probabilities[current_index][1])\n",
    "        current_sum += sorted_probabilities[current_index][1]\n",
    "        current_index += 1\n",
    "\n",
    "    # make the sum of the probabilities equal to 1\n",
    "    top_p_probs = make_prob_1(top_p_probs)\n",
    "    # convert the tokens (tuple) to a list of strings\n",
    "    top_p_tokens = [\"\".join(token) for token in top_p_tokens]\n",
    "    # sample from the top p tokens\n",
    "    next_token = np.random.choice(top_p_tokens, p=top_p_probs)\n",
    "\n",
    "    return next_token\n",
    "\n",
    "\n",
    "def sample_beam(probabilities, num_beams = 3):\n",
    "    \"\"\" Samples the next tokens using beam search, i.e., keeps the top num_beams hypotheses at each step.\n",
    "        Helper function for beam_search\n",
    "    Args:\n",
    "        probabilities: a dictionary of probabilities, i.e. {key = next_token, value = probability}\n",
    "        num_beams: the number of beams to keep, i.e. the number of hypotheses to keep at each step\n",
    "    Returns:\n",
    "        beam_tokens: a list of top num_beams tokens\n",
    "        beam_probs: a list of the corresponding probabilities of the top num_beams tokens\n",
    "    \"\"\"\n",
    "    # copy the dictionary\n",
    "    probabilities_copy = probabilities.copy()\n",
    "    # remember meta_data and remove\n",
    "    meta_data = probabilities_copy['meta_data']\n",
    "    del probabilities_copy['meta_data']\n",
    "    total_count = meta_data[\"total_count\"]\n",
    "    notInModel_count = meta_data[\"<notInModel>_count\"]\n",
    "\n",
    "    # remove <notInModel> from the probabilities dictionary and add the probabilities of the tokens not in the model\n",
    "    probabilities_copy = get_notInModel_probabilities(probabilities=probabilities_copy, notInModel_count=notInModel_count)\n",
    "\n",
    "    # sort the probabilities dictionary by the values\n",
    "    sorted_probabilities = sorted(probabilities_copy.items(), key=lambda x: x[1], reverse=True)\n",
    "    # take the top num_beams\n",
    "    top_beams = sorted_probabilities[:num_beams]\n",
    "\n",
    "    beam_tokens = [token for (token, prob) in top_beams]\n",
    "    beam_probs = [prob for (token, prob) in top_beams]\n",
    "\n",
    "    return beam_tokens, beam_probs\n",
    "\n",
    "\n"
   ],
   "metadata": {
    "id": "v4TrLs1kI3fW",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:41:14.795160Z",
     "end_time": "2023-04-28T21:41:14.853822Z"
    }
   },
   "execution_count": 42,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your Language Model to generate each one out of the following examples with the coresponding params.    \n",
    "Notice the 4 core issues: \n",
    "- Starting tokens\n",
    "- Length of the generation\n",
    "- Sampling methond (use all)\n",
    "- Stop Token (if this token is sampled, stop generating)"
   ],
   "metadata": {
    "id": "Giylo6-lI21t"
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Use your LM to generate a string based on the parametes of each examples, and store the generation sequance at the generation list."
   ],
   "metadata": {
    "id": "YTbF-9zKVchQ"
   }
  },
  {
   "cell_type": "code",
   "source": [
    "def generate_string(all_models, prefix='<start>', sampling_method='beam', gen_length=10, stop_token='<end>', num_beams=5, add_one=False):\n",
    "    \"\"\" Generates a string using the specified sampling method\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = (prefix, sampling_method, gen_length, stop_token, num_beams), value = generation}\n",
    "        prefix: the prefix of the generation\n",
    "        sampling_method: the sampling method to use\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token to stop the generation\n",
    "        num_beams: the number of beams to keep, i.e. the number of hypotheses to keep at each step\n",
    "        add_one: whether to add one to the count of each token\n",
    "    Returns:\n",
    "        generation: the generated string\n",
    "    \"\"\"\n",
    "    if sampling_method == 'beam':\n",
    "        return beam_search(all_models= all_models, prefix= prefix, gen_length= gen_length, stop_token= stop_token, num_beams= num_beams, add_one= add_one)\n",
    "    else:\n",
    "        return generate_string_not_beam(all_models= all_models, prefix= prefix, sampling_method= sampling_method, gen_length= gen_length, stop_token= stop_token, add_one= add_one)\n",
    "\n",
    "def beam_search(all_models, prefix=\"<start>\", gen_length=10, stop_token=\"<end>\", num_beams=5, language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Generates a string using beam search\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        num_beams: the number of beams to keep\n",
    "        language: the language of the model\n",
    "        add_one: whether to use add one smoothing or not\n",
    "        max_n: the maximum n-gram to use\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    # initialize the beams\n",
    "    beams = [(prefix, 0)]  # (prefix, log_prob)\n",
    "\n",
    "    # generate the string token by token\n",
    "    for _ in range(gen_length):\n",
    "        new_beams = []\n",
    "\n",
    "        # sample the next token for each beam\n",
    "        for beam_prefix, beam_log_prob in beams:\n",
    "            # get the correct language model\n",
    "            current_lm, current_tuple_key_prefix, next_token_probabilities = get_correct_model(all_models=all_models,\n",
    "                                                                                               prefix=beam_prefix, language=language,\n",
    "                                                                                               add_one=add_one, max_n=max_n)\n",
    "\n",
    "            # sample the top num_beams tokens and probabilities\n",
    "            beam_tokens, beam_probs = sample_beam(next_token_probabilities, num_beams)\n",
    "\n",
    "            # update the beams\n",
    "            for token, prob in zip(beam_tokens, beam_probs):\n",
    "                if not (beam_prefix.endswith(stop_token) or beam_prefix.endswith(\"<end>\")):\n",
    "                    new_prefix, new_log_prob = update_beam(beam_prefix, beam_log_prob, token, prob)\n",
    "                    new_beams.append((new_prefix, new_log_prob))\n",
    "                else:\n",
    "                    new_beams.append((beam_prefix, beam_log_prob))\n",
    "\n",
    "\n",
    "        # keep the top num_beams beams\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:num_beams]\n",
    "\n",
    "    # get the best beam\n",
    "    best_beam = beams[0][0]\n",
    "\n",
    "    return best_beam\n",
    "\n",
    "def generate_string_not_beam(all_models, prefix='<start>', gen_length=10, stop_token='<end>', sampling_method='topK', language=\"en.csv\", add_one=False, max_n=4):\n",
    "    \"\"\" Generates a string using the given language model\n",
    "    Args:\n",
    "        all_models: where lm_dict = {key = (n, add_one), value = language_model},\n",
    "                where language_model = {key = language, value = model},\n",
    "                where model = {key = prefix, value = probabilities}\n",
    "        prefix: the prefix\n",
    "        gen_length: the length of the generation\n",
    "        stop_token: the token that stops the generation\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "        language: the language of the model\n",
    "        add_one: whether to use add one smoothing or not\n",
    "        max_n: the maximum n-gram to use\n",
    "    Returns:\n",
    "        generated_string: the generated string\n",
    "    \"\"\"\n",
    "    current_prefix = prefix\n",
    "\n",
    "    # generate the string token by token\n",
    "    for _ in range(gen_length):\n",
    "        # get the correct language model\n",
    "        current_lm, current_tuple_key_prefix, next_token_probabilities = get_correct_model(all_models=all_models,\n",
    "                                                                                           prefix=current_prefix, language=language,\n",
    "                                                                                           add_one=add_one, max_n=max_n)\n",
    "\n",
    "        # sample the next token\n",
    "        next_token = select_next_token(next_token_probabilities, sampling_method)\n",
    "\n",
    "        # if next_token == \"<notInModel>\": uniform sample from the group (vocabulary - next_token_probabilities.keys())\n",
    "        if next_token == \"<notInModel>\":\n",
    "            tokens_not_in_model = list(set(vocabulary) - set(next_token_probabilities.keys()))\n",
    "            next_token = random.choice(tokens_not_in_model)\n",
    "\n",
    "        # update the current prefix\n",
    "        current_prefix += next_token\n",
    "\n",
    "        # stop if the stop token was sampled\n",
    "        if current_prefix.endswith(stop_token) or next_token == \"<end>\":\n",
    "            break\n",
    "\n",
    "\n",
    "    return current_prefix\n",
    "\n",
    "def update_beam(beam_prefix, beam_log_prob, token, prob):\n",
    "    \"\"\" Updates the beam\n",
    "    Args:\n",
    "        beam_prefix: the current beam prefix\n",
    "        beam_log_prob: the current beam log probability\n",
    "        token: the token to add to the beam\n",
    "        prob: the probability of the token\n",
    "    Returns:\n",
    "        new_prefix: the new beam prefix\n",
    "        new_log_prob: the new beam log probability\n",
    "    \"\"\"\n",
    "    # update the prefix and the log probability\n",
    "    new_prefix = beam_prefix + token\n",
    "    new_log_prob = beam_log_prob + np.log(prob)\n",
    "\n",
    "    return new_prefix, new_log_prob\n",
    "\n",
    "def select_next_token(next_token_probabilities, sampling_method='topK', k_greedy=1, temperature=0.5, top_k=5, p=0.5):\n",
    "    \"\"\" Selects the next token, (greedy, temperature, topK, topP)\n",
    "    Args:\n",
    "        next_token_probabilities: the probabilities of the next token\n",
    "        sampling_method: the sampling method, can be 'greedy', 'temperature', 'topK', 'topP'\n",
    "        k_greedy: the number of top tokens to consider for greedy sampling\n",
    "        temperature: the temperature for temperature sampling\n",
    "        top_k: the number of top tokens to consider for topK sampling\n",
    "        p: the probability mass for topP sampling\n",
    "    Returns:\n",
    "        next_token: the next token\n",
    "    \"\"\"\n",
    "    if sampling_method == 'greedy':\n",
    "        return sample_greedy(next_token_probabilities, k_greedy)\n",
    "    elif sampling_method == 'temperature':\n",
    "        return sample_temperature(next_token_probabilities, temperature)\n",
    "    elif sampling_method == 'topK':\n",
    "        return sample_topK(next_token_probabilities, top_k)\n",
    "    elif sampling_method == 'topP':\n",
    "        return sample_topP(next_token_probabilities, p)\n",
    "    else:\n",
    "        raise ValueError(f'Unknown sampling method: {sampling_method}')\n",
    "\n"
   ],
   "metadata": {
    "id": "3zf-omUXQezz",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:41:14.808885Z",
     "end_time": "2023-04-28T21:41:14.855817Z"
    }
   },
   "execution_count": 43,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "test_ = {\n",
    "    'example1' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['greedy','beam'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example2' : {\n",
    "        'start_tokens' : \"H\",\n",
    "        'sampling_method' : ['temperature','topK','topP'],\n",
    "        'gen_length' : \"10\",\n",
    "        'stop_token' : \"\\n\",\n",
    "        'generation' : []\n",
    "    },\n",
    "    'example3' : {\n",
    "        'start_tokens' : \"He\",\n",
    "        'sampling_method' : ['greedy','beam','temperature','topK','topP'],\n",
    "        'gen_length' : \"20\",\n",
    "        'stop_token' : \"me\",\n",
    "        'generation' : []\n",
    "    }\n",
    "}"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:41:14.817881Z",
     "end_time": "2023-04-28T21:41:14.856817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "# Define the parameters\n",
    "all_models = run_match_language_models\n",
    "language = \"en.csv\"\n",
    "add_one = False"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:41:14.835279Z",
     "end_time": "2023-04-28T21:41:14.856817Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# clear the generations, useful if you want to run the code multiple times\n",
    "for example in test_:\n",
    "    test_[example]['generation'] = []\n",
    "\n",
    "# generate the strings for each example\n",
    "for example in test_:\n",
    "    # iterate over the sampling methods\n",
    "    for i in range(len(test_[example]['sampling_method'])):\n",
    "        # get the parameters\n",
    "        sampling_method = test_[example]['sampling_method'][i]\n",
    "        gen_length = int(test_[example]['gen_length'])\n",
    "        stop_token = test_[example]['stop_token']\n",
    "        prefix = test_[example]['start_tokens']\n",
    "\n",
    "        # generate the string\n",
    "        generated_string = generate_string(all_models=all_models, prefix=prefix, sampling_method=sampling_method, gen_length=gen_length, stop_token=stop_token, add_one=add_one)\n",
    "\n",
    "        # cut the start_token from the generated string\n",
    "        generated_string = generated_string[len(prefix):]\n",
    "\n",
    "        # store the string\n",
    "        test_[example]['generation'].append(generated_string)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:41:14.850822Z",
     "end_time": "2023-04-28T21:41:14.877821Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example example1:\n",
      "Start tokens: H\n",
      "full string using greedy : House the s\n",
      "generated string using greedy : ouse the s\n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "full string using beam : Healthcare \n",
      "generated string using beam : ealthcare \n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "\n",
      "Example example2:\n",
      "Start tokens: H\n",
      "full string using temperature : HSUND 1 ink\n",
      "generated string using temperature : SUND 1 ink\n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "full string using topK : Here's non \n",
      "generated string using topK : ere's non \n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "full string using topP : How me the \n",
      "generated string using topP : ow me the \n",
      "generation char length: 10\n",
      "generation token length: 10\n",
      "\n",
      "Example example3:\n",
      "Start tokens: He\n",
      "full string using greedy : Health the so much 24t\n",
      "generated string using greedy : alth the so much 24t\n",
      "generation char length: 20\n",
      "generation token length: 20\n",
      "full string using beam : Healthcare in that the\n",
      "generated string using beam : althcare in that the\n",
      "generation char length: 20\n",
      "generation token length: 20\n",
      "full string using temperature : HeavyweiseyUpday-Z BLA\n",
      "generated string using temperature : avyweiseyUpday-Z BLA\n",
      "generation char length: 20\n",
      "generation token length: 20\n",
      "full string using topK : He whene of the crime\n",
      "generated string using topK :  whene of the crime\n",
      "generation char length: 19\n",
      "generation token length: 19\n",
      "full string using topP : Hered me\n",
      "generated string using topP : red me\n",
      "generation char length: 6\n",
      "generation token length: 6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# print the generations and the number of tokens\n",
    "\n",
    "# iterate over the examples\n",
    "for example in test_:\n",
    "    print(f\"Example {example}:\")\n",
    "    start_token = test_[example]['start_tokens']\n",
    "    print(\"Start tokens:\", start_token)\n",
    "    # iterate over the sampling methods\n",
    "    for i in range(len(test_[example]['generation'])):\n",
    "        sampling_method = test_[example]['sampling_method'][i]\n",
    "        generated_string = test_[example]['generation'][i]\n",
    "        print(\"full string using\", sampling_method, \":\", start_token + generated_string)\n",
    "        print(\"generated string using\", sampling_method, \":\", generated_string)\n",
    "        print(\"generation char length:\", len(generated_string))\n",
    "        print(\"generation token length:\", len(tweet_to_token_tuples(generated_string)))\n",
    "    print(\"\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-28T21:41:14.867830Z",
     "end_time": "2023-04-28T21:41:14.925283Z"
    }
   }
  },
  {
   "cell_type": "code",
   "source": [
    "### do not change ###\n",
    "print('-------- NLG --------')\n",
    "\n",
    "for k,v in test_.items():\n",
    "  l = ''.join([f'\\t{sm} >> {v[\"start_tokens\"]}{g}\\n' for sm,g in zip(v['sampling_method'],v['generation'])])\n",
    "  print(f'{k}:')\n",
    "  print(l)"
   ],
   "metadata": {
    "id": "bvla30-lVw8n",
    "ExecuteTime": {
     "start_time": "2023-04-28T21:41:14.882739Z",
     "end_time": "2023-04-28T21:41:14.956019Z"
    }
   },
   "execution_count": 48,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------- NLG --------\n",
      "example1:\n",
      "\tgreedy >> House the s\n",
      "\tbeam >> Healthcare \n",
      "\n",
      "example2:\n",
      "\ttemperature >> HSUND 1 ink\n",
      "\ttopK >> Here's non \n",
      "\ttopP >> How me the \n",
      "\n",
      "example3:\n",
      "\tgreedy >> Health the so much 24t\n",
      "\tbeam >> Healthcare in that the\n",
      "\ttemperature >> HeavyweiseyUpday-Z BLA\n",
      "\ttopK >> He whene of the crime\n",
      "\ttopP >> Hered me\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iEtckSWNANqW"
   },
   "source": [
    "<br><br><br>\n",
    "# **Good luck!**"
   ]
  }
 ]
}
